<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-19T11:43:07+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Gon’s Tech Jurnal</title><subtitle>The Record of Backend Development</subtitle><author><name>Yong gon Yun</name></author><entry><title type="html">ML-agents - AttentionPPO code</title><link href="http://localhost:4000/attentionPPO-code.html" rel="alternate" type="text/html" title="ML-agents - AttentionPPO code" /><published>2024-05-18T09:32:20+09:00</published><updated>2024-05-18T09:32:20+09:00</updated><id>http://localhost:4000/attentionPPO-code</id><content type="html" xml:base="http://localhost:4000/attentionPPO-code.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>해당 내용은 다음의 강의 및 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">인프런 - 유니티 머신러닝 에이전트 완전정복 (응용편) </a><br /></li>
</ul>

<h3 id="1-code-개요">1. code 개요</h3>

<p>이전에 작성된 ML-Agents PPO code <a href="https://y2gon2.github.io/PPO2_code.html">link</a> 에서 network input data (state) 의 종류의 구분 없이 넣어 주었다. 그러나 해당 코드에서는 Agent 속도 값과 ray 에 의해 관측된 일정 거리 안에 존재하는 물체 (공 or 벽) 에 대해서 다르게 처리한 이후, 이를 통합 PPO 알고리즘을 처리한다. 특히  ray 관측 정보의 경우, trasformer encoding layer 를 사용하여 주목해야 하는 물체에 집중도를 높여 attention output 을 얻는다. 이후, 핻아 정보를 합한 PPO network 를 통해 다음 행동을 결정하도록 구성되었다.</p>

<p>​<center><img src="assets\img\posts\2024-05-18-attentionPPO-code\1.png" width="600" /></center></p>

<p>아울러 기존 PPO code 에서 변경된 부분 위주로 설명을 추가하였으며, 동일한 부분은 ML-Agents PPO code <a href="https://y2gon2.github.io/PPO2_code.html">link</a> 참고</p>

<h3 id="2-추가-또는-수정-파라미터">2. (추가 또는 수정) 파라미터</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vel_state_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># agent 속도 (x, z)
</span><span class="n">ray_chan_size</span> <span class="o">=</span> <span class="mi">40</span>  <span class="c1"># ray 최대 개수 
</span><span class="n">ray_feat_size</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c1"># ray 1개당 정보 크기 (감지된 물체의 x, z 좌표, x, z 속도)
</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">RAY_OBS</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># 환경으로부터 Observation 벡터에서 Ray 정보가 담긴 index
</span><span class="n">VEL_OBS</span> <span class="o">=</span> <span class="mi">1</span>         <span class="c1"># 환경으로부터 Observation 벡터에서 Agent 의 속도 정보가 담긴 index
</span>
<span class="c1"># attention parameters
</span>
<span class="c1"># Transformer Encoder Layer 에서 사용할 입력 피쳐 (attention incodingd 입력 ebedding 크기)
# feedforward model  피쳐 크기로도 사용
</span><span class="n">embed_size</span> <span class="o">=</span> <span class="mi">32</span>     

<span class="c1"># Transformer Encoder Layer 에서 MultiHeadAttention  모델의 Head 갯수
</span><span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>
</code></pre></div></div>

<h3 id="3-attentionactorcritic-class">3. AttentionActorCritic Class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AttentionActorCritic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionActorCritic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">ray_feat_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> 
            <span class="n">nhead</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> 
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> 
            <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">embed_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">vel_state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">ray</span><span class="p">,</span> <span class="n">vel</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">ray_feat_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (1)
</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
        <span class="n">ray</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">ray_chan_size</span><span class="p">,</span> <span class="n">ray_feat_size</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">attn_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_in</span><span class="p">(</span><span class="n">ray</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ray_chan_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span> <span class="c1"># (3)
</span>        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_layer</span><span class="p">(</span><span class="n">attn_in</span><span class="p">)</span> <span class="c1"># (4)
</span>
        <span class="n">ray_embed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_out</span><span class="p">(</span><span class="n">attn_out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="c1"># (5)
</span>        <span class="n">vel_embed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">e</span><span class="p">(</span><span class="n">vel</span><span class="p">))</span> <span class="c1"># (6)
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">vel_embed</span><span class="p">,</span> <span class="n">ray_embed</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (7)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># (8)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># (9)
</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">pi</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (10)
</span></code></pre></div></div>
<p>​<center><img src="assets\img\posts\2024-05-18-attentionPPO-code\2.png" width="750" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<h3 id="4-추가-또는-수정-attentionppoagent-class">4. (추가 또는 수정) AttentionPPOAgent Class</h3>

<p>해당 클래스의 경우, network class 명칭을 제외하고 PPO 코드와 동일 <a href="https://y2gon2.github.io/PPO2_code.html">link</a></p>

<h3 id="5-추가-또는-수정-main">5. (추가 또는 수정) main</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ---------//--------
</span> <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">preprocess</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">ray</span><span class="p">,</span> <span class="n">vel</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">ray</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">ray_feat_size</span><span class="p">),</span> <span class="n">vel</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span>
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># 환경으로부터 얻는 정보
</span>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_worker</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">):</span>
            <span class="n">next_term_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>

        <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">:</span>
            <span class="n">_id</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">).</span><span class="nf">index</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
            <span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_term_state</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>
            <span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>

        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># ---------//--------
</span></code></pre></div></div>

<p>(1) state, next_state 를 유니티로부터 받아왔을 때, 기존 PPO 에서는 모든 observation 요소 들을 동일하게 network 에 입력값으로 사용되었으나, Attention-PPO 에서는 ray 정보와 (agent) vel 정보를 명시적으로 받아서 network 에 입력되는 형태로 정리 해주어야 한다. 따라서, 전처리 과정으로 해당 data 를 받아 순서대로 concatenate 처리 해준다.</p>

<p>아울러 이와 관련된 부분을 추가 수정해준다.</p>

<h3 id="6-전체-코드">6. 전체 코드</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span>\
                             <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.environment_parameters_channel</span>\
                             <span class="kn">import</span> <span class="n">EnvironmentParametersChannel</span>

<span class="n">vel_state_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># agent 속도 (x, z)
</span><span class="n">ray_chan_size</span> <span class="o">=</span> <span class="mi">40</span>  <span class="c1"># ray 최대 개수 
</span><span class="n">ray_feat_size</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c1"># ray 1개당 정보 크기 (감지된 물체의 x, z 좌표, x, z 속도)
</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">RAY_OBS</span> <span class="o">=</span> <span class="mi">0</span>         <span class="c1"># 환경으로부터 Observation 벡터에서 Ray 정보가 담긴 index
</span><span class="n">VEL_OBS</span> <span class="o">=</span> <span class="mi">1</span>         <span class="c1"># 환경으로부터 Observation 벡터에서 Agent 의 속도 정보가 담긴 index
</span>
<span class="c1"># attention parameters
</span>
<span class="c1"># Transformer Encoder Layer 에서 사용할 입력 피쳐 (attention incodingd 입력 ebedding 크기)
# feedforward model  피쳐 크기로도 사용
</span><span class="n">embed_size</span> <span class="o">=</span> <span class="mi">32</span>     

<span class="c1"># Transformer Encoder Layer 에서 MultiHeadAttention  모델의 Head 갯수
</span><span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">n_step</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">n_epoch</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">run_step</span> <span class="o">=</span> <span class="mi">2000000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span>
<span class="n">test_step</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 닷지 환경 설정
</span><span class="n">env_static_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">ballSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">"</span><span class="s">ballRandom</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sh">"</span><span class="s">agentSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">env_dynamic_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">boardRadius</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">},</span>
                      <span class="sh">"</span><span class="s">ballNums</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">}}</span>

<span class="c1"># 유니티 환경 경로
</span><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Dodge_Attention</span><span class="sh">"</span>
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span>
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../Env/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span>
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># 모델 저장 및 불러오기 경로
</span><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/AttentionPPO/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span>
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/AttentionPPO/20230728125435</span><span class="sh">"</span>

<span class="c1"># 연산 장치 
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># AttentionActorCritic 클래스 -&gt; Attention을 사용하는 ActorCritic Network 정의 
</span><span class="k">class</span> <span class="nc">AttentionActorCritic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">AttentionActorCritic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_in</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">ray_feat_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> 
            <span class="n">nhead</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> 
            <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">embed_size</span><span class="p">,</span> 
            <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">embed_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">vel_state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">ray</span><span class="p">,</span> <span class="n">vel</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">ray_feat_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">b</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ray</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">ray_chan_size</span><span class="p">,</span> <span class="n">ray_feat_size</span><span class="p">)</span>
        <span class="n">attn_in</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_in</span><span class="p">(</span><span class="n">ray</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ray_chan_size</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">)</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_layer</span><span class="p">(</span><span class="n">attn_in</span><span class="p">)</span>

        <span class="n">ray_embed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_out</span><span class="p">(</span><span class="n">attn_out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">vel_embed</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">e</span><span class="p">(</span><span class="n">vel</span><span class="p">))</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">vel_embed</span><span class="p">,</span> <span class="n">ray_embed</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">pi</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># AttentionPPOAgent 클래스 -&gt; AttentionPPOAgent 알고리즘을 위한 다양한 함수 정의
</span><span class="k">class</span> <span class="nc">AttentionPPOAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">AttentionActorCritic</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># 정책을 통해 행동 결정
</span>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># 네트워크 모드 설정
</span>        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># 네트워크 연산에 따라 행동 결정
</span>        <span class="n">pi</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="c1"># 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
</span>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="c1"># 학습 수행
</span>    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># prob_old, adv, ret 계산
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">pi_old</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">prob_old</span> <span class="o">=</span> <span class="n">pi_old</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span>

            <span class="n">_</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">-</span> <span class="n">value</span>
            <span class="n">adv</span> <span class="o">=</span> <span class="n">delta</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
            <span class="n">adv</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_step</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">(),</span>
                <span class="p">[</span><span class="n">adv</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="c1"># GAE 수행
</span>            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">[:,</span> <span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">_lambda</span> <span class="o">*</span> <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            
            <span class="n">adv</span> <span class="o">=</span> <span class="n">adv</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">adv</span> <span class="o">+</span> <span class="n">value</span>

        <span class="c1"># 학습 이터레이션 시장
</span>        <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>

                <span class="n">_state</span><span class="p">,</span> <span class="n">_action</span><span class="p">,</span> <span class="n">_ret</span><span class="p">,</span> <span class="n">_adv</span><span class="p">,</span> <span class="n">_prob_old</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> 
                    <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">prob_old</span><span class="p">]</span>
                <span class="p">)</span>

                <span class="n">pi</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">_state</span><span class="p">)</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">pi</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span>

                <span class="c1"># 정책 신경망 손실 함수 계산
</span>                <span class="n">ratio</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">/</span> <span class="p">(</span><span class="n">_prob_old</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span> <span class="c1"># probability ratio
</span>                <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">_adv</span>
                <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">_adv</span>
                <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

                <span class="c1"># 가치 신경망 손실함수 계산
</span>                <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_ret</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

                <span class="n">total_loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span>

                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">total_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>
    
    <span class="c1"># 네트워크 모델 저장
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">network</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># 학습 기록 
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/actor_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/critic_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 유니티 환경 경로 설정(file_name)
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">environment_parameters_channel</span> <span class="o">=</span> <span class="nc">EnvironmentParametersChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
        <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">,</span> <span class="n">environment_parameters_channel</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 유니티 behavior 설정
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_static_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_float_parameter</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_dynamic_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_uniform_sampler_parameters</span><span class="p">(</span>
                              <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">])</span>

    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
    <span class="n">num_worker</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dec</span><span class="p">)</span>

    <span class="c1"># AttentioPPOAgent class 를 agent 로 정의
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">AttentionPPOAgent</span><span class="p">()</span>
    <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">preprocess</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">ray</span><span class="p">,</span> <span class="n">vel</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">ray</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">ray_chan_size</span> <span class="o">*</span> <span class="n">ray_feat_size</span><span class="p">),</span> <span class="n">vel</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span>
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># 환경으로부터 얻는 정보
</span>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_worker</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">):</span>
            <span class="n">next_term_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">RAY_OBS</span><span class="p">],</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">VEL_OBS</span><span class="p">])</span>

        <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">:</span>
            <span class="n">_id</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">).</span><span class="nf">index</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
            <span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_term_state</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>
            <span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>

        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_worker</span><span class="p">):</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]],</span> <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]])</span>

            <span class="c1"># 학습 수행
</span>            <span class="nf">if </span><span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_actor_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">mean_critic_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_actor_loss</span><span class="p">,</span> <span class="n">mean_critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / </span><span class="sh">"</span> <span class="o">+</span>\
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Actor loss: </span><span class="si">{</span><span class="n">mean_actor_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / Critic loss: </span><span class="si">{</span><span class="n">mean_critic_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span> <span class="p">)</span>

            <span class="c1"># 네트워크 모델 저장 
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="transformer" /><category term="attention" /><category term="embedding" /><category term="ppo" /><category term="ml-agents" /><summary type="html"><![CDATA[ML-agents - AttentionPPO code]]></summary></entry><entry><title type="html">Trasformer - Attention 모델 학습</title><link href="http://localhost:4000/transformer.html" rel="alternate" type="text/html" title="Trasformer - Attention 모델 학습" /><published>2024-05-17T09:32:20+09:00</published><updated>2024-05-17T09:32:20+09:00</updated><id>http://localhost:4000/transformer</id><content type="html" xml:base="http://localhost:4000/transformer.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<p>&lt;Transformer - Attention is All you Need&gt; 의 논문에 나온 Transformer 에 대해 학습 내용을 정리</p>

<p>해당 논문은 언어를 다른나라의 언어로 번역하는 작업을 Transformer-Attention 알고리즘을 적용하여 기존보다 효율적으로 언어를 변환할 수 있음을 보여준다. Transformer  구조도는 아래와 같으며 해당 구조에 각 요소와 process 를 분석한다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\1.png" width="400" /></center></p>

<ul>
  <li>해당 내용은 다음의 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://product.kyobobook.co.kr/detail/S000001952238">자연어 처리 바이블 -ChatGPT 핵심기술-</a></li>
</ul>

<h3 id="1--word-embedding">1.  (Word) Embedding</h3>

<p>입력된 정보(단어)들을 컴퓨터가 연산으로 처리할 수 있도록 vector 공간의 특정 값으로 매핑 시킨다. 매핑된 정보들은 임이의 vector 값을 가지거나 어떤 학습을 통해 아래와 같이, 연관성이 높은 단어들 간에 가까운 공간내 존재하도록 값을 할당 받을 수 있다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\2.png" width="600" /></center>
  <a href="https://medium.com/@hari4om/word-embedding-d816f643140">이미지 출처</a></p>

<h3 id="2-positional-embedding">2. Positional Embedding</h3>

<p>언어를 처리할 때, 문장에서 각 단어의 위치는 중요한 의미를 가진다. 그런다. word embedding 에는 해당 정보가 존재하지 않으므로, 문장 구조(각 단어의 위치와 시퀀스 내 다른 단어간의 위치 차이에 대한 정보) 를 가진 positional encoding 을 추가하여 순서의 의미를 포함한 embedding 으로 변환해준다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\3.png" width="500" /></center>
  <a href="https://product.kyobobook.co.kr/detail/S000001952238">이미지 출처</a></p>

<h3 id="3-incoder">3. Incoder</h3>

<p>입력 정보는 앞에서와 같이 Embedding 작업, Positional Encoding 추가 작업을 거친 후 Incoder 로 진입하게 된다. 여기서 Multi-Head Attention 과정을 거치게 되는데, Mulit-Head Attention 은 여러개의 독립적인  Self-Attion 으로 구성되어 있다. 따라서 우선 Self-Attion 에서 진행되는 작업을 우선 살펴본다.</p>

<h4 id="31-self-attention">3.1 Self-Attention</h4>

<h4 id="311-query-벡터-key-벡터-value-벡터를-생성">3.1.1 Query 벡터, Key 벡터, Value 벡터를 생성</h4>

<p>해당 과정에서 우선 입력된 Embedding 벡터로부터 Query 벡터, Key 벡터, Value 벡터를 생성한다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\4.png" width="500" /></center>
  <a href="https://product.kyobobook.co.kr/detail/S000001952238">이미지 출처</a></p>

<ul>
  <li>
    <p>Query 벡터 (Q) : 현재 처리하고 있는 입력 요소에 대한 벡터로, 다른 모든 요소들과의 관계를 평가하는 데 사용. 즉, 해당 단어가 다른 모든 Key 벡터들과의 상호작용을 통해 어느 정도 연관성이나 중요도를 가지는지를 평가하는 것을 의미. (이를 은유적으로 “질문 (Query)” 로 표현함)</p>
  </li>
  <li>
    <p>Key 벡터 (K) : 비교 대상 (연결될 다음 단어)이 되는 각 입력 요소에 대응하는 벡터로, Query와 비교(내적연산) 된다. Key 벡터는 Query가 어떤 요소에 집중(다음 단어로 선택)해야 할지 결정하는 데 도움을 준다.</p>
  </li>
  <li>
    <p>Value 벡터 (V) : 각 입력 요소의 실제 정보를 담고 있는 벡터로, attention 가중치가 적용된 후, 이 값들이 합쳐져 최종적으로 출력될 정보를 형성.</p>
  </li>
</ul>

<p>각각의 가중치 행렬 W<sup>Q</sup>, W<sup>K</sup>, W<sup>V</sup> 일반적으로 각각 독립젹으로 학습되며, gradient 하강법을 사용하여 최적화 된다.</p>

<h4 id="312-점수-계산">3.1.2 점수 계산</h4>

<p>현재 처리 중인 단어와 해당 단어가 포함된 문장 내 모든 다른 단어들에 대해서 점수를 계산한다. 이 점수는 현재 단어를 encode 할 때, 다른 단어들에 대해서 얼마나 집중을 해야 할지를 결정한다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\5.png" width="140" /></center>
  <a href="https://product.kyobobook.co.kr/detail/S000001952238">이미지 출처</a></p>

<p>현재 단어의 Query 벡터와 다른 단어들의 Key 벡터들 간에 각각 내적으로 계산된다. 해당 값의 크기를 조절(scale &amp; Mask) 하고, softmax 계산을 통과시켜 모든 점수들을 양수로 만들고 그 합을 1 로 만들어 준다. 이 점수는 현재 위치의 단어의 encoding 에 있어서 얼마나 각 단어들의 표현이 들어갈 것인지를 결정한다.</p>

<p>예를 들어, I am a student. (영어) 을 넣고, 현재 I 라는 단어의 위치를 encoding 하고 있다면, 현재 위치에 위치할 단어로 I (동일 단어) 가 가장 높은 점수를 가지게 되겠지만, 가끔은 현재 단어에 관련이 있는 다른 단어에 대한 정보가 들어가는 것이 필요할 수 있다(? 이해가 잘 안됨.)</p>

<h4 id="313-weighted-value-합--최종-출력-벡터-형성">3.1.3 weighted value 합 &amp; 최종 출력 벡터 형성</h4>

<ol>
  <li>
    <p>가중합 (weighted sum) : 앞의 과정에서 얻은 점수(attention weights - 가중치)를 vector 에 value 벡터를 곱한다. 이 과정에서 각단어의 value 가 가중치에 의해 그 값이 (가중치가 높다면), 그 값이 거의 그대로 유지되거나, (가중치가 낮다면) 값이 매우 작아지게된다. 가중합 과정을 거치면 value 가 매우 작아진 단어들은 최종 결과에 거의 기여하지 못하고, 중요도(결과값)가 높은 단어들에 집중할 수 있게 된다. (정보의 필터링)</p>
  </li>
  <li>
    <p>모든 weighted value 벡터들을 요소별로 합산한다. 즉, 같은 위치에 있는 요소들끼리 더한다. 이렇게 합산된 결과는 하나의 벡터가 되며, 이 벡터가 그 위치에서의 self-attention layer의 최종 출력이 된다.</p>
  </li>
</ol>

<p>여기까지 3.1.2 &amp; 3.1.3 의 과정을 Scaled Dot=Product Attention 이라고 하며, 그 작업을 정리하면 아래와 같다. 
​<center><img src="assets\img\posts\2024-05-17-transformer\6.png" width="180" /></center>
  <a href="https://product.kyobobook.co.kr/detail/S000001952238">이미지 출처</a></p>

<h4 id="32-multi-head-attention">3.2 Multi Head Attention</h4>

<p>논문에서는 성능 개선을 위해  encoder/decoder 마다 위와 같이 구조의 Attention Head 를  8개씩 갖도록 한다. 각 Head 는 다른 representation 공간을 가진다. 즉, 각 단어에 대해 scale dot product attention 을 여러번 병렬적으로 수행함을 의미한다. 해당 작동 과정을 정리하면 아래와 같다.</p>

<ol>
  <li>
    <p>분할(Divide): 입력 벡터(예: 단어 임베딩)는 먼저 여러 개의 ‘head’로 분할된다. 각 head에서는 독립적인 attention 연산이 수행되며, 이는 각기 다른 query, key, value 벡터의 세트를 사용.</p>
  </li>
  <li>
    <p>병렬 처리(Parallel Processing): 각 head는 독립적으로 scale dot product attention을 수행한다. 이는 각 head가 입력 데이터의 서로 다른 부분집합 또는 다른 관점에서의 정보를 처리하게 함으로써, 모델이 다양한 특성을 동시에 고려할 수 있도록 한다.</p>
  </li>
  <li>
    <p>Attention 연산: 각 head에서는 독립적으로 query 벡터와 key 벡터의 내적을 통해 점수를 계산하고, 이 점수에 softmax를 적용하여 attention 가중치를 얻는다. 이 가중치는 각 head의 value 벡터와 곱해져 weighted value 벡터를 생성한다.</p>
  </li>
  <li>
    <p>결합(Concatenate): 각 head에서 생성된 weighted value 벡터들은 다시 하나의 벡터로 결합된다. 이 결합 과정은 각 head가 추출한 정보를 통합하여 전체적인 문맥을 형성한다.</p>
  </li>
  <li>
    <p>선형 변환(Linear Transformation): 결합된 벡터는 추가적인 선형 변환을 거쳐 최종 출력 벡터를 형성한다. 이 변환은 모든 head에서 얻은 정보를 최적화하고 특정 작업에 맞게 조정하는 역할을 한다.
​<center><img src="assets\img\posts\2024-05-17-transformer\7.png" width="250" /></center></p>
  </li>
</ol>

<h4 id="33-잔차-연결-residual-connection">3.3 잔차 연결 (Residual connection)</h4>
<p>​<center><img src="assets\img\posts\2024-05-17-transformer\8.png" width="130" /></center>
​<center><img src="assets\img\posts\2024-05-17-transformer\9.png" width="480" /></center>
  <a href="https://velog.io/@glad415/Transformer-7.-%EC%9E%94%EC%B0%A8%EC%97%B0%EA%B2%B0%EA%B3%BC-%EC%B8%B5-%EC%A0%95%EA%B7%9C%ED%99%94-by-WikiDocs">이미지 출처</a></p>

<p>위와 같이 해당 위치의 단어 encoding 벡터 (x: 서브층 입력)에 multi haed attention 연산의 결과를 더함. 각 레이어의 입력과 출력을 직접 연결하여, 깊은 네트워크에서도 학습이 원활하게 진행되도록 돕는다. 그 결과, 네트워크가 더 깊어져도 정보가 손실되는 것을 방지할 수 있다.</p>

<h4 id="34-층-정규화-layer-normalization">3.4 층 정규화 (Layer normalization)</h4>

<p>잔차 연결로 학습 내용을 누적시킨 x 에 대해서 요소의 평균과 분산을 계산하여 데이터를 정규화한다. 이는 훈련 과정을 안정화하고, 다른 스케일의 특징들이 네트워크에 의해 고르게 학습될 수 있도록 한다. 이 정규화는 벡터의 각 차원에 걸쳐 진행된다.</p>

<h4 id="35-feed-forward-networks-전방-전달-네트워크">3.5 Feed Forward Networks (전방 전달 네트워크)</h4>

<p>이 부분은 각 위치의 인코딩된 벡터 z를 독립적으로 동일한 신경망(Feed Forward Neural Network)에 통과시킨다. 이 네트워크는 일반적으로 두 개의 선형 변환과 그 사이에 하나의 비선형 활성화 함수(예: ReLU)로 구성된다.</p>

<ul>
  <li>첫 번째 선형 변환: 입력 벡터를 더 높은 차원으로 매핑.</li>
  <li>활성화 함수: 비선형성을 도입하여 모델이 더 복잡한 패턴을 학습할 수 있도록 함.</li>
  <li>두 번째 선형 변환: 다시 원래 차원으로 매핑.</li>
</ul>

<p>Feed Forward 네트워크는 각 위치에서 독립적으로 작동하므로, 서로 다른 위치의 데이터가 서로 영향을 미치지 않는다. 이것은 Transformer가 문맥에 따라 각 단어의 표현을 개별적으로 조정할 수 있게 한다.</p>

<p>Feed Forward Networks (전방 전달 네트워크) 의 목적</p>

<ol>
  <li>
    <p>비선형 처리 능력 추가: Transformer의 self-attention 메커니즘은 기본적으로 선형적인 처리를 수행한다. 비선형 활성화 함수를 포함하는 Feed Forward Network를 통해 모델에 비선형성을 도입함으로써, 모델이 더 복잡하고 추상적인 패턴과 관계를 학습할 수 있게 된다. 이는 모델이 더 높은 수준의 언어 이해와 처리 능력을 갖추도록 돕는다.</p>
  </li>
  <li>
    <p>향상된 표현력: 각 입력 포지션에 대해 동일한 Feed Forward Network를 적용함으로써, 인코더는 각 단어 혹은 토큰의 표현을 더욱 풍부하게 만들 수 있다. 이 과정은 입력 벡터를 더 높은 차원으로 확장시키고, 다시 원래의 차원으로 줄이는 과정을 통해 각 토큰의 특징을 더욱 세밀하게 조정한다.</p>
  </li>
  <li>
    <p>독립적인 위치 처리: Transformer의 Feed Forward Network는 각 포지션에서 독립적으로 작동한다. 이는 모델이 문장의 각 부분을 독립적으로 평가하고 조정할 수 있도록 하며, 이는 특히 병렬 처리에서 큰 장점을 제공한다. 각 위치의 데이터 처리가 다른 위치의 데이터에 영향을 받지 않기 때문에, 전체적인 계산 효율성이 높아짐.</p>
  </li>
  <li>
    <p>문맥적 통합 강화: 비록 Feed Forward Network는 위치별로 독립적으로 작동하지만, self-attention 단계에서 이미 계산된 문맥적 정보를 토대로 각 토큰의 표현을 더욱 개선하고 정제합니다. 이는 전체 문장의 의미를 더 잘 파악하고, 각 단어나 토큰이 전체 문맥에서 어떻게 기능하는지 더 정확하게 반영할 수 있게 합니다.</p>
  </li>
</ol>

<p>이러한 과정을 통해, Transformer는 각 입력 데이터의 특성을 개선하고, 더 정교한 언어 처리를 수행할 수 있는 강력한 툴을 제공합니다. Feed Forward Networks는 Transformer의 구조적 핵심 요소 중 하나로, 모델의 전반적인 성능과 표현력을 크게 향상시키는 역할을 합니다.</p>

<h4 id="36-add--normalize-추가-및-정규화">3.6 Add &amp; Normalize (추가 및 정규화)</h4>

<p>Feed Forward Networks 입력과 출력에 대해 다시 잔차 연결 &amp; 정규화 과정을 진행한다.</p>

<h3 id="4-decoder">4. Decoder</h3>
<p>​<center><img src="assets\img\posts\2024-05-17-transformer\11.png" width="400" /></center></p>
<h4 id="41-masked-multi-head-attention">4.1 Masked Multi-Head Attention</h4>

<p>Masking 의 작동 방식</p>

<p>ransformer 디코더의 self-attention 계층에서 각 토큰은 자신과 그 이전의 모든 토큰에 대해서만 attention 계산을 수행할 수 있다. 이는 softmax 함수를 적용하기 전에 특정 위치의 attention 점수를 아주 작은 값(예를 들어, -무한대와 같은)으로 설정함으로써 해당 위치의 점수는 0에 가까운 값이 되어, 결국 해당 위치의 토큰은 계산에서 제외됨.
​<center><img src="assets\img\posts\2024-05-17-transformer\10.png" width="450" /></center>
  <a href="https://paul-hyun.github.io/transformer-02/">이미지 출처</a></p>

<p>Masking의 목적</p>

<p>디코더가 아직 생성하지 않은 출력 토큰들에 대한 정보를 참조하지 못하게 하는 것. 즉, 디코더가 각 단계에서 예측을 수행할 때 오직 그 단계 이전까지의 출력들만 고려하도록 보장.</p>

<h4 id="42-입력-data-랙-multi-head-attention">4.2 입력 data 랙 Multi-Head Attention</h4>

<p>Query Vector : Decoder 내 이전 단계의 Masked Mulit-Head Attention 결과를 사용
Key &amp; Value Vector : Incoder 프로세스 결과 사용</p>

<h3 id="5-linear-layer--softmax">5. Linear Layer &amp; Softmax</h3>

<p>여러 개의 decoder 를 거치고 난 후에는 확률을 요소로 가진 벡터 하나가 남게 된다. 어떻게 이 하나의 벡터를 단어로 바꿀 수 있을까?</p>

<p>Linear layer 는 fully-connected 신경망으로 decoder 가 마지막으로 출력한 벡터를 그보다 훨씬 큰 사이즈의 벡터인 logits 벡터에 투영 시킨다. logit 벡터의 크기는 모델이 선택할수 있는 어휘의 크기와 같다. 즉 Decoder 의 결과를 신경망 layer 로 넣어서 현재 위치(step)에서 전체 어휘들 중 선택 가능 점수를 계산하게 된다.</p>

<p>해당 점수는 softmax 를 적용하여 확률분포로 변환되고, 최종적으로 가장 높은 확률의 단어가 선택된다.</p>]]></content><author><name>Yong gon Yun</name></author><category term="transformer" /><category term="attention" /><category term="incoding" /><category term="decoding" /><category term="embedding" /><summary type="html"><![CDATA[Trasformer - Attention 모델 학습]]></summary></entry><entry><title type="html">PPO(Proximal Policy Optimization) Code</title><link href="http://localhost:4000/PPO2_code.html" rel="alternate" type="text/html" title="PPO(Proximal Policy Optimization) Code" /><published>2024-05-14T09:32:20+09:00</published><updated>2024-05-14T09:32:20+09:00</updated><id>http://localhost:4000/PPO2_code</id><content type="html" xml:base="http://localhost:4000/PPO2_code.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>해당 내용은 다음의 강의 및 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">인프런 - 유니티 머신러닝 에이전트 완전정복 (응용편) </a><br /></li>
</ul>

<h3 id="1-라이브러리--파라미터-설정">1. 라이브러리 &amp; 파라미터 설정</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span>\
                             <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.environment_parameters_channel</span>\
                             <span class="kn">import</span> <span class="n">EnvironmentParametersChannel</span>
<span class="c1"># 파라미터 값 세팅 
</span><span class="n">state_size</span> <span class="o">=</span> <span class="mi">122</span>
<span class="c1"># ray 당 정보 수집: 40 rays * 감지된 물체의 (거리 + x축 속도 + z축 속도) = 120
# agent 좌표 (x, z) = 2
</span>
<span class="n">action_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 상, 하, 좌, 우, 정지
</span>
<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># 미래 보상의 감가율
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>    <span class="c1"># 네트워크 학습률
</span><span class="n">n_step</span> <span class="o">=</span> <span class="mi">128</span>            <span class="c1"># 모델 학습 주기
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>        <span class="c1"># 한번 network 를 업데이트할 때 사용되는 데이터 수
</span><span class="n">n_epoch</span> <span class="o">=</span> <span class="mi">3</span>             <span class="c1"># 한번 모델을 학습할 때 시행하는 epoch 수
</span><span class="n">_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>          <span class="c1"># GAE 기법에 사용할 설정값
</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.2</span>           <span class="c1"># clipped surrogate objective 에 사용할 설정 값
</span>
<span class="n">run_step</span> <span class="o">=</span> <span class="mi">2000000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span> <span class="c1"># 학습모드에서 진행할 스텝 수
</span><span class="n">test_step</span> <span class="o">=</span> <span class="mi">100000</span>     <span class="c1"># 평가모드에서 사용할 스텝수
</span>
<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 닷지 환경 설정
</span><span class="n">env_static_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">ballSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">"</span><span class="s">ballRandom</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sh">"</span><span class="s">agentSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="c1"># 정적 리셋 파라미터들을 가진 변수 (환경 리셋시 항상 동일하게 사용되는 파라미트들)
</span>
<span class="n">env_dynamic_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">boardRadius</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">},</span>
                      <span class="sh">"</span><span class="s">ballNums</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">}}</span>
<span class="c1"># 동적 리셋 파라미터들을 가진 변수 (환경 리셋시 min ~ max 값 사이의 임의 값을 매번 다르게 생성)
</span>
<span class="c1"># 유니티 환경 경로 
</span><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Dodge</span><span class="sh">"</span>
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span>
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../Env/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span>
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># 모델 저장 및 불러오기 경로
</span><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/PPO/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span>
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/PPO/20230728125435</span><span class="sh">"</span>

<span class="c1"># 연산 장치
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="2-model-class">2. Model class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ActorCritic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">pi</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>Actor-Critic 통합 모델
​<center><img src="assets\img\posts\2024-05-14-PPO2_code\1.png" width="480" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<h3 id="3-agent-class">3. Agent class</h3>

<h4 id="31-ppoagent-class-와-초기화">3.1 PPOAgent class 와 초기화</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PPOAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">ActorCritic</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span> <span class="c1"># n_step 동안 진행한 여러 worker(학습 agent)들을 저장
</span>        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># 저장된 모델을 사용할 경우
</span>            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># cpu or gpu 메모링 모델 로드
</span>            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="32-정책을-통해-행동-결정">3.2 정책을 통해 행동 결정</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># 네트워크 모드 설정 (ex. Dropout layer 존재 시 train mode 에서 활성화, test mode 에서 비활성화)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># 네트워크 연산에 따라 행동 결정
</span>        <span class="n">pi</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="c1"># torch.multinomial() : pi ( 1 * 5 텐서이며, 모든 요소의 합이 1인 확률 분보 값) 를 사용
</span>        <span class="c1"># 해당 확률에 근거하여 하나의 index 를 선택
</span>
        <span class="k">return</span> <span class="n">action</span>
</code></pre></div></div>

<p>get_action 메서드를 사용하여 network 를 통해 action 을 선택하는 과정</p>

<p>​<center><img src="assets\img\posts\2024-05-14-PPO2_code\2.png" width="480" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<h4 id="33-리플레이-메모리에-데이터-추가-상태-행동-보상-다음-상태-게임-종료-여부">3.3 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">aciton</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">doen</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="34-학습-수행">3.4 학습 수행</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

        <span class="c1"># 롤아웃 데이터 추출 및 텐서 변환
</span>        <span class="n">state</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span>       <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

        <span class="c1"># 실수형 텐서로 변환
</span>        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                                        <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>
        <span class="c1"># prob_old, adv, ret 계산
</span>        <span class="c1"># pi_old    : state 에 대한 업데이트 전 network 의 pi  예측값
</span>        <span class="c1"># value     : state 에 대한 업데이트 전 network 의 value 예측값
</span>        <span class="c1"># prob_old  : pi_old 의 action index 에 대한 value (업데이트 전 action 의 확률값)
</span>        <span class="c1"># adv       : 정책 신경망 업데이트에 사용할 어드벤티지 값
</span>        <span class="c1"># ret       : 가치 신경망 업데이트에 사용할 티켓 값
</span>        <span class="c1"># delta     : TD error (GAE 에서 1 ~ T step 까지 TD error 의 합은 Advantage (At) 임)
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">pi_old</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>         <span class="c1"># (1)
</span>            <span class="n">prob_old</span> <span class="o">=</span> <span class="n">pi_old</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span>  <span class="c1"># (2)
</span>            <span class="c1"># gather(1, action) : 1 차원 (행) 의 상태를 유지하면서, 열의 값에서 action 에 해당하는 index 값을 추출
</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>    <span class="c1"># (3)
</span>            <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">-</span> <span class="n">value</span>
            <span class="n">adv</span> <span class="o">=</span> <span class="n">delta</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>                         <span class="c1"># (4)
</span>
            <span class="n">adv</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>                            <span class="c1"># (5)
</span>                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_step</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">(),</span> 
                <span class="p">[</span><span class="n">adv</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
            <span class="p">)</span> 
            <span class="c1"># view(n_step, -1): 만약 n_step 이 32, adv 와 done 텐서 크기가 128 이였다면, 
</span>            <span class="c1">#      adv, done 을 각각 4, 32 tensor 차원으로 변환 
</span>            <span class="c1">#      즉, 메모리에서 추출한 data 를 num_worker * n_step 차원을 변환
</span>            <span class="c1"># transpose(0, 1) : num_worker * n_step =&gt; n_step * num_worker 변환
</span>            <span class="c1"># countiguous()   : view나 transpose 같은 연산 후에 텐서의 물리적인 메모리 배열이 실제 데이터 배열과 일치하지 않을 수 있음
</span>            <span class="c1">#      contiguous()는 데이터를 메모리 상에서 연속적으로 재배치하여 텐서가 예상대로 작동하도록 함.
</span>
            <span class="c1"># GAE 연산 수행
</span>            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_step</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>         <span class="c1"># (6)
</span>                <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">[:,</span> <span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">_lambda</span> <span class="o">*</span> <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># GAE 작업을 완료된 advantage 값들은 원래 차원으로 되돌리는 변환을 진행한다.
</span>            <span class="n">adv</span> <span class="o">=</span> <span class="n">adv</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 현재 정책 조건에서의  advantage + 현재 상태 value 
</span>            <span class="c1"># =&gt; 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용
</span>            <span class="n">ret</span> <span class="o">=</span> <span class="n">adv</span> <span class="o">+</span> <span class="n">value</span>                           <span class="c1"># (7)
</span>
        <span class="c1"># 학습 이터레이션 시작
</span>        <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">))</span>   <span class="c1"># 사용되는 전체 data 의 크기의 index
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>     <span class="c1"># idxs 간 순서를 통한 연관성을 배제시키기 위해
</span>            <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span> <span class="c1"># batch 크기 만큼씩 slicing
</span>
                <span class="n">_state</span><span class="p">,</span> <span class="n">_action</span><span class="p">,</span> <span class="n">_ret</span><span class="p">,</span> <span class="n">_adv</span><span class="p">,</span> <span class="n">_prob_old</span> <span class="o">=</span>\
                    <span class="nf">map</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> 
                        <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">prob_old</span><span class="p">]</span>
                    <span class="p">)</span> <span class="c1"># slicing 크기 만큼씩 추출
</span>                
                <span class="n">pi</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">_state</span><span class="p">)</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">pi</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span> <span class="c1"># 해당 action 의 정책 확률 추출
</span>
                <span class="c1"># 정책신경망 손실함수 계산
</span>                <span class="n">ratio</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">/</span> <span class="p">(</span><span class="n">_prob_old</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>   <span class="c1"># probability ratio
</span>                <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">_adv</span>                <span class="c1"># surrogate object
</span>                <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">_adv</span> <span class="c1"># clipped surrogate object
</span>                <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

                <span class="c1"># 가치신경망 손실함수 계산
</span>                <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_ret</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>    <span class="c1"># ret - V(s) 의 제곱 평균
</span>
                <span class="n">total_loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span>           <span class="c1"># action entropy 반영되지 않음
</span>
                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">total_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>
</code></pre></div></div>

<p>​<center><img src="assets\img\posts\2024-05-14-PPO2_code\3.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<ol>
  <li>
    <p>pi_old, value = self.network(state)</p>

    <p>update 되기 전 network 에 상태를 넣어 update 되기 전 정책과 value 구함.</p>
  </li>
  <li>
    <p>prob_old = pi_old.gather(1, action.long())</p>

    <p>pi_old tensor 에서 열(column) 이 action 인 곳의 값을 취함.</p>
  </li>
  <li>
    <p>_, next_value = self.network(next_state)<br />
delta = reward + (1 - done) * discount_factor * next_value - value</p>

    <p>next_state 를 update 전 network 에 넣어 next_value 를 구하고, 이를 앞에서 계산된 요소들로 Temporal Difference Error 를 구한다.</p>
  </li>
  <li>
    <p>adv = delta.clone()</p>

    <p>TD error 복제 하여 advantage 로 사용</p>
  </li>
  <li>
    <p>adv, done = map(lambda x: x.view(n_step, -1).transpose(0, 1).contiguous(), [adv, done])</p>

    <p>GAE 진행 전, 학습 가능한 차원으로 adv를 전처리. (n_step * num_worker) 차원으로 변환.</p>
  </li>
  <li>
    <p>for t in reversed(range(n_step - 1)):<br />
adv[:, t] += (1 - done[:, t]) * discount_factor * _lambda * adv[:, t + 1]</p>

    <p>GAE 연산 수행</p>
  </li>
  <li>
    <p>ret = adv + value<br /></p>

    <p>현재 정책 조건에서의  advantage + 현재 상태 value =&gt; 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용</p>
  </li>
</ol>

<h4 id="35-네트워크-모델-저장--학습기록">3.5 네트워크 모델 저장 &amp; 학습기록</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># 네트워크 모델 저장
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">network</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># 학습기록
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wirter</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/actor_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/critic_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="4-main-함수">4. Main 함수</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 유니티 환경 경로 설정 (file_name)
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">environment_parameters_channel</span> <span class="o">=</span> <span class="nc">EnvironmentParametersChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span><span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
                           <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">,</span>
                                          <span class="n">environment_parameters_channel</span><span class="p">])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 유니티 behavior 설정 
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    
    <span class="c1"># 환경 정적 파라미터 값 설정
</span>    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_static_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_float_parameter</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="c1"># 환경 동적 파라미터 분포 설정 (환경이 reset 될 때마다 설정된 범위 내에서 임의 값을 sampling 하여 사용)
</span>    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_dynamic_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_uniform_sampler_parameters</span><span class="p">(</span>
                              <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">])</span>
    
    <span class="c1"># decision step, termination step
</span>    <span class="c1"># episode 진행 중 -&gt; dec
</span>    <span class="c1"># episode 종료 : 종료 step -&gt; term / 다음 episode 의 첫 step -&gt; dec
</span>    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
    <span class="n">num_worker</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dec</span><span class="p">)</span>

    <span class="c1"># PPO 클래스를 agent로 정의 
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">PPOAgent</span><span class="p">()</span>
    <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        
        <span class="n">state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># dec.obs : 지정한 behavior_name 가진 모든 agent 에 대한 모든 관측을 포함한 튜플
</span>        <span class="c1"># 현재 관측 정보는 단 1개 종류이므로 obs[0] 값만 가져와서 state 정보로 넣어줌.
</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span>
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>           <span class="c1"># 이산적 action 할당
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span><span class="c1"># unity 에 동작 정보 전달
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>                                  <span class="c1"># 동작 진행
</span>
        <span class="c1"># 환경으로부터 얻는 정보 (분산 학습을 위한 worker data 저장)
</span>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_worker</span> <span class="c1"># 모든 worker 들 done 을 일괄적으로 초기화
</span>        <span class="n">next_state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>     <span class="c1"># 모든 worker 들의 관측 정보 반영
</span>        <span class="n">reward</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>         <span class="c1"># 모든 worker 들의 보상 정보 반영
</span>        
        <span class="c1"># 종료 worker 정보를 업데이트 
</span>        <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">:</span>
            <span class="n">_id</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">).</span><span class="nf">index</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
            <span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">_id</span><span class="p">]</span>
            <span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="c1"># rollout memory 에 woker 들의 정보 저장
</span>            <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_worker</span><span class="p">):</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]],</span> <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]])</span>
            
            <span class="c1"># n_step 마다 학습수행 모델 업데이트 및 loss 값 들을 리스트에 추가
</span>            <span class="nf">if </span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">episode</span> <span class="o">+=</span><span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_actor_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">mean_critic_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_actor_loss</span><span class="p">,</span> <span class="n">mean_critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / </span><span class="sh">"</span> <span class="o">+</span>\
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Actor loss: </span><span class="si">{</span><span class="n">mean_actor_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / Critic loss: </span><span class="si">{</span><span class="n">mean_critic_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span> <span class="p">)</span>

            <span class="c1"># 네트워크 모델 저장 
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="5-전체-코드">5. 전체 코드</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 라이브러리 불러오기
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span>\
                             <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.environment_parameters_channel</span>\
                             <span class="kn">import</span> <span class="n">EnvironmentParametersChannel</span>
<span class="c1"># 파라미터 값 세팅 
</span><span class="n">state_size</span> <span class="o">=</span> <span class="mi">122</span>
<span class="c1"># ray 당 정보 수집: 40 rays * 감지된 물체의 (거리 + x축 속도 + z축 속도) = 120
# agent 좌표 (x, z) = 2
</span>
<span class="n">action_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 상, 하, 좌, 우, 정지
</span>
<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># 미래 보상의 감가율
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>    <span class="c1"># 네트워크 학습률
</span><span class="n">n_step</span> <span class="o">=</span> <span class="mi">128</span>            <span class="c1"># 모델 학습 주기
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>        <span class="c1"># 한번 network 를 업데이트할 때 사용되는 데이터 수
</span><span class="n">n_epoch</span> <span class="o">=</span> <span class="mi">3</span>             <span class="c1"># 한번 모델을 학습할 때 시행하는 epoch 수
</span><span class="n">_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>          <span class="c1"># GAE 기법에 사용할 설정값
</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.2</span>           <span class="c1"># clipped surrogate objective 에 사용할 설정 값
</span>
<span class="n">run_step</span> <span class="o">=</span> <span class="mi">2000000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span> <span class="c1"># 학습모드에서 진행할 스텝 수
</span><span class="n">test_step</span> <span class="o">=</span> <span class="mi">100000</span>     <span class="c1"># 평가모드에서 사용할 스텝수
</span>
<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 닷지 환경 설정
</span><span class="n">env_static_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">ballSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">"</span><span class="s">ballRandom</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sh">"</span><span class="s">agentSpeed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="c1"># 정적 리셋 파라미터들을 가진 변수 (환경 리셋시 항상 동일하게 사용되는 파라미트들)
</span>
<span class="n">env_dynamic_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">boardRadius</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">},</span>
                      <span class="sh">"</span><span class="s">ballNums</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span> <span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">:</span> <span class="mi">77</span><span class="p">}}</span>
<span class="c1"># 동적 리셋 파라미터들을 가진 변수 (환경 리셋시 min ~ max 값 사이의 임의 값을 매번 다르게 생성)
</span>
<span class="c1"># 유니티 환경 경로 
</span><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Dodge</span><span class="sh">"</span>
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span>
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../Env/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span>
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># 모델 저장 및 불러오기 경로
</span><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/PPO/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span>
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/PPO/20230728125435</span><span class="sh">"</span>

<span class="c1"># 연산 장치
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># ActorCritic 클래스 -&gt; Actor Network, Critic Network 정의 
</span><span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ActorCritic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">d2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">pi</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># PPOAgent 클래스 -&gt; PPO 알고리즘을 위한 다양한 함수 정의 
</span><span class="k">class</span> <span class="nc">PPOAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">ActorCritic</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span> <span class="c1"># n_step 동안 진행한 여러 worker(학습 agent)들을 저장
</span>        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># 저장된 모델을 사용할 경우
</span>            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># cpu or gpu 메모링 모델 로드
</span>            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># 정책을 통해 행동 결정 
</span>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># 네트워크 모드 설정 (ex. Dropout layer 존재 시 train mode 에서 활성화, test mode 에서 비활성화)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>

        <span class="c1"># 네트워크 연산에 따라 행동 결정
</span>        <span class="n">pi</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="c1"># torch.multinomial() : pi ( 1 * 5 텐서이며, 모든 요소의 합이 1인 확률 분보 값) 를 사용
</span>        <span class="c1"># 해당 확률에 근거하여 하나의 index 를 선택
</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="c1"># 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
</span>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="c1"># 학습 수행
</span>    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

        <span class="c1"># 롤아웃 데이터 추출 및 텐서 변환
</span>        <span class="n">state</span>      <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span>     <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span>       <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>

        <span class="c1"># 실수형 텐서로 변환
</span>        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                                        <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>
        <span class="c1"># prob_old, adv, ret 계산
</span>        <span class="c1"># pi_old    : state 에 대한 업데이트 전 network 의 pi  예측값
</span>        <span class="c1"># value     : state 에 대한 업데이트 전 network 의 value 예측값
</span>        <span class="c1"># prob_old  : pi_old 의 action index 에 대한 value (업데이트 전 action 의 확률값)
</span>        <span class="c1"># adv       : 정책 신경망 업데이트에 사용할 어드벤티지 값
</span>        <span class="c1"># ret       : 가치 신경망 업데이트에 사용할 티켓 값
</span>        <span class="c1"># delta     : TD error (GAE 에서 1 ~ T step 까지 TD error 의 합은 Advantage (At) 임)
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">pi_old</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>         <span class="c1"># (1)
</span>            <span class="n">prob_old</span> <span class="o">=</span> <span class="n">pi_old</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span>  <span class="c1"># (2)
</span>            <span class="c1"># gather(1, action) : 1 차원 (행) 의 상태를 유지하면서, 열의 값에서 action 에 해당하는 index 값을 추출
</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>    <span class="c1"># (3)
</span>            <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">-</span> <span class="n">value</span>
            <span class="n">adv</span> <span class="o">=</span> <span class="n">delta</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>                         <span class="c1"># (4)
</span>
            <span class="n">adv</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>                            <span class="c1"># (5)
</span>                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">n_step</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">(),</span> 
                <span class="p">[</span><span class="n">adv</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
            <span class="p">)</span> 
            <span class="c1"># view(n_step, -1): 만약 n_step 이 32, adv 와 done 텐서 크기가 128 이였다면, 
</span>            <span class="c1">#      adv, done 을 각각 4, 32 tensor 차원으로 변환 
</span>            <span class="c1">#      즉, 메모리에서 추출한 data 를 num_worker * n_step 차원을 변환
</span>            <span class="c1"># transpose(0, 1) : num_worker * n_step =&gt; n_step * num_worker 변환
</span>            <span class="c1"># countiguous()   : view나 transpose 같은 연산 후에 텐서의 물리적인 메모리 배열이 실제 데이터 배열과 일치하지 않을 수 있음
</span>            <span class="c1">#      contiguous()는 데이터를 메모리 상에서 연속적으로 재배치하여 텐서가 예상대로 작동하도록 함.
</span>
            <span class="c1"># GAE 연산 수행
</span>            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_step</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>         <span class="c1"># (6)
</span>                <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">[:,</span> <span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">_lambda</span> <span class="o">*</span> <span class="n">adv</span><span class="p">[:,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># GAE 작업을 완료된 advantage 값들은 원래 차원으로 되돌리는 변환을 진행한다.
</span>            <span class="n">adv</span> <span class="o">=</span> <span class="n">adv</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 현재 정책 조건에서의  advantage + 현재 상태 value 
</span>            <span class="c1"># =&gt; 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용
</span>            <span class="n">ret</span> <span class="o">=</span> <span class="n">adv</span> <span class="o">+</span> <span class="n">value</span>                           <span class="c1"># (7)
</span>
        <span class="c1"># 학습 이터레이션 시작
</span>        <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">))</span>   <span class="c1"># 사용되는 전체 data 의 크기의 index
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
            <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>     <span class="c1"># idxs 간 순서를 통한 연관성을 배제시키기 위해
</span>            <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span> <span class="c1"># batch 크기 만큼씩 slicing
</span>
                <span class="n">_state</span><span class="p">,</span> <span class="n">_action</span><span class="p">,</span> <span class="n">_ret</span><span class="p">,</span> <span class="n">_adv</span><span class="p">,</span> <span class="n">_prob_old</span> <span class="o">=</span>\
                    <span class="nf">map</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> 
                        <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">ret</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">prob_old</span><span class="p">]</span>
                    <span class="p">)</span> <span class="c1"># slicing 크기 만큼씩 추출
</span>                
                <span class="n">pi</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">_state</span><span class="p">)</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">pi</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_action</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span> <span class="c1"># 해당 action 의 정책 확률 추출
</span>
                <span class="c1"># 정책신경망 손실함수 계산
</span>                <span class="n">ratio</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">/</span> <span class="p">(</span><span class="n">_prob_old</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>   <span class="c1"># probability ratio
</span>                <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">_adv</span>                <span class="c1"># surrogate object
</span>                <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">_adv</span> <span class="c1"># clipped surrogate object
</span>                <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

                <span class="c1"># 가치신경망 손실함수 계산
</span>                <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_ret</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>    <span class="c1"># ret - V(s) 의 제곱 평균
</span>
                <span class="n">total_loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span>           <span class="c1"># action entropy 반영되지 않음
</span>
                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">total_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>

    <span class="c1"># 네트워크 모델 저장
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">network</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># 학습 기록 
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/actor_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/critic_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

<span class="c1"># Main 함수 -&gt; 전체적으로 PPO 알고리즘을 진행 
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 유니티 환경 경로 설정 (file_name)
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">environment_parameters_channel</span> <span class="o">=</span> <span class="nc">EnvironmentParametersChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span><span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
                           <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">,</span>
                                          <span class="n">environment_parameters_channel</span><span class="p">])</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 유니티 behavior 설정 
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    
    <span class="c1"># 환경 정적 파라미터 값 설정
</span>    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_static_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_float_parameter</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="c1"># 환경 동적 파라미터 분포 설정 (환경이 reset 될 때마다 설정된 범위 내에서 임의 값을 sampling 하여 사용)
</span>    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">env_dynamic_config</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">environment_parameters_channel</span><span class="p">.</span><span class="nf">set_uniform_sampler_parameters</span><span class="p">(</span>
                              <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">min</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">max</span><span class="sh">"</span><span class="p">],</span> <span class="n">value</span><span class="p">[</span><span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">])</span>
    
    <span class="c1"># decision step, termination step
</span>    <span class="c1"># episode 진행 중 -&gt; dec
</span>    <span class="c1"># episode 종료 : 종료 step -&gt; term / 다음 episode 의 첫 step -&gt; dec
</span>    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
    <span class="n">num_worker</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">dec</span><span class="p">)</span>

    <span class="c1"># PPO 클래스를 agent로 정의 
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">PPOAgent</span><span class="p">()</span>
    <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        
        <span class="n">state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># dec.obs : 지정한 behavior_name 가진 모든 agent 에 대한 모든 관측을 포함한 튜플
</span>        <span class="c1"># 현재 관측 정보는 단 1개 종류이므로 obs[0] 값만 가져와서 state 정보로 넣어줌.
</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span>
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>           <span class="c1"># 이산적 action 할당
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span><span class="c1"># unity 에 동작 정보 전달
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>                                  <span class="c1"># 동작 진행
</span>
        <span class="c1"># 환경으로부터 얻는 정보 (분산 학습을 위한 worker data 저장)
</span>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_worker</span> <span class="c1"># 모든 worker 들 done 을 일괄적으로 초기화
</span>        <span class="n">next_state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>     <span class="c1"># 모든 worker 들의 관측 정보 반영
</span>        <span class="n">reward</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>         <span class="c1"># 모든 worker 들의 보상 정보 반영
</span>        
        <span class="c1"># 종료 worker 정보를 업데이트 
</span>        <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">:</span>
            <span class="n">_id</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">).</span><span class="nf">index</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
            <span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">_id</span><span class="p">]</span>
            <span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span><span class="p">[</span><span class="n">_id</span><span class="p">]</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="c1"># rollout memory 에 woker 들의 정보 저장
</span>            <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_worker</span><span class="p">):</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">reward</span><span class="p">[</span><span class="nb">id</span><span class="p">]],</span> <span class="n">next_state</span><span class="p">[</span><span class="nb">id</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">[</span><span class="nb">id</span><span class="p">]])</span>
            
            <span class="c1"># n_step 마다 학습수행 모델 업데이트 및 loss 값 들을 리스트에 추가
</span>            <span class="nf">if </span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
                <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
                <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">episode</span> <span class="o">+=</span><span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_actor_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">mean_critic_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_actor_loss</span><span class="p">,</span> <span class="n">mean_critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / </span><span class="sh">"</span> <span class="o">+</span>\
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Actor loss: </span><span class="si">{</span><span class="n">mean_actor_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / Critic loss: </span><span class="si">{</span><span class="n">mean_critic_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span> <span class="p">)</span>

            <span class="c1"># 네트워크 모델 저장 
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="ppo" /><category term="proximal policy optimization" /><category term="surrogate object" /><category term="clipping" /><summary type="html"><![CDATA[PPO(Proximal Policy Optimization) Code]]></summary></entry><entry><title type="html">PPO(Proximal Policy Optimization) 이론</title><link href="http://localhost:4000/PPO1.html" rel="alternate" type="text/html" title="PPO(Proximal Policy Optimization) 이론" /><published>2024-05-12T09:32:20+09:00</published><updated>2024-05-12T09:32:20+09:00</updated><id>http://localhost:4000/PPO1</id><content type="html" xml:base="http://localhost:4000/PPO1.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>해당 내용은 다음의 강의 및 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">인프런 - 유니티 머신러닝 에이전트 완전정복 (응용편) </a><br />
<a href="https://product.kyobobook.co.kr/detail/S000001952238">단단한 심층 강화 학습</a></li>
</ul>

<h3 id="1-ppo-의-개념-및-특징">1. PPO 의 개념 및 특징</h3>

<p>PPO 의 주요 아이디어는 단조 정책 향상 (monotonic policy imporvement) 를 보장함으로써 성능 붕괴를 피하게 해주는 대리 목적(surrogate objective) 을 도입하는 것.</p>

<p>구조적 특징</p>

<ul>
  <li>정책 + 가치 기반 강화학습 알고리즘</li>
  <li>확률적 경사 상승법 (Stochastic Gradient Ascent) 사용 -&gt; “Surrogate” 목적 함수 최대화</li>
  <li>다수의 epoch 동안 미니 배치 업데이트 수행</li>
  <li>On-policy RL 알고리즘</li>
</ul>

<p>기능적 특징</p>

<ul>
  <li>Trust Region Policy Optimization (TRPO) 의 장점 + TRPO 대비 비교적 단순한 구현, 다양한 환경에서의 평균적으로 좋은 성능, 더 낮은 샘플 복잡도의 장점을 지님.</li>
</ul>

<h3 id="2-ppo-알고리즘-배경">2. PPO 알고리즘 배경</h3>

<h4 id="21-성능-붕괴">2.1 성능 붕괴</h4>

<p>REIMFORCE 알고리즘과 같이, 정책 πθ 에 대한 최대 목적 max J(π<sub>θ</sub>) 찾기 위해 gradient 를 사용한다. 다시 말해서, 정책 경사 알고리즘을 사용하는 강화 학습의 경우, ∇<sub>θ</sub>J(θ) 를 이용하여 파라미터 θ 를 조정하는 방식으로 최적화 된다.</p>

<p>그러나, 우리가 학습시키는 파라미터 θ 로 표현된 공간 (parameter space) ‘Θ’  과 실제로 수행하고자 하는 정책들 π 로 표현된 공간 (policy space) ‘Π’ 은 별개의 공간이다. 그리고 각각의 공간상의 값들이 어떤 연관 관계가 있는지 알 수 없다. 그 배치가 일정한지, 또는 서로 비례된 간격으로 배치되어 있는지 모른다. 이를 수식으로 표현하면 아래와 같다. 
​<center><img src="assets\img\posts\2024-05-12-PPO1\1.png" width="400" /></center></p>

<p>이 문제는 파라미미터 업데이트를 위한 이상적인 학습률 α 를 결정하기 어렵다는 점에서 문제가 될 수 있다. 파라미터 공간에 대응되는 정책 공간 Π 에 속하는 정책들 사이의 간격이 얼마인지를 사전에 알 수 없다.</p>

<p>만약 α 가 너무 작으면,</p>

<ul>
  <li>훈련 횟수 증가에 따른 학습 시간 지연</li>
  <li>지역 최대값 (local maxima) 빠져 전체 최대값을 찾지 못함 (성능 향상 중단)</li>
</ul>

<p>의 문제가 발생할 수 있고, α 가 너무 크면,</p>

<ul>
  <li>업데이트 간경이 너무 커서 좋은 정책을 건너뛰게 되는 성능 붕괴 발생</li>
</ul>

<p>이 나타날 수 있다.</p>

<h4 id="22-상대적-정책-성능-식별자-relative-policy-preformance-identity">2.2 상대적 정책 성능 식별자 (relative policy preformance identity)</h4>

<p>앞에서 언급된 문제를 해결하기 위해, 학습의 방향을 각각의 정책을 평가하는 것이 아닌 현재의 정책 새로운 정책이 얼마나 향상되었는가를 평가한다. 이를 아래와 같이 표기하며,  상대적 정책 성능 식별자 (relative policy performance identity) 라고 한다. (식 유도는 생략)
​<center><img src="assets\img\posts\2024-05-12-PPO1\2.png" width="300" /></center></p>

<p>π          : 기존정책 <br />
 π’         : 다음정책<br />
 A<sup>π</sup>(s , a)   : 이전 정책으로부터의 이득 A<sup>π</sup><sub>t</sub>=Q(s<sub>t</sub>,a<sub>t</sub>)−V(s<sub>t</sub>)<br /></p>

<p>상대적 정책 성능 식별자  J(π’) – J(π) 는 정책 향상을 측정하는 지표 역할 을 하며, J(π’) 를 최대화 하는 것은, 곧 상대적 정책 성능 식별가 최대가 되게 하는 것이다. 
 ​<center><img src="assets\img\posts\2024-05-12-PPO1\3.png" width="300" /></center></p>

<p>목적함수를 이렇나 방식으로 구조화하는 것은  모든 정책 반복이 음이 아닌(단조로운) 향상, 즉  J(π’) – J(π)≥ 0 을 보장할 수 있어야 한다는 뜻이다. 최악의 경우에도 π’ = π 이기 때문에, 이렇게 할 수 있다면, 훈련 과정에서 성능 붕괴는 일어나지 않을 것이다.</p>

<h4 id="23-대리-목적-surrogate-objective">2.3 대리 목적 (surrogate objective)</h4>

<p>상대적 정책 성능 식별자  J(π’) – J(π) 에 대한 최대의 기대값을 찾을 수 있다면, 성능붕괴를 막을 수 있었다. 그러나 문제는 업데이트된 정책에 대한 목적값 J(π’) 을 현재 상태에서 알 수 없다. 이러한 역설을 해결하기 위해 상대적 정책 성능 식별자에 근사된 대리 목적(surrogate objective)  J<sub>π</sub><sup>CPI</sup>(π’) 을 구하는 함수 를 사용한다. (유도과정은 생략)
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\4.png" width="400" /></center></p>

<ul>
  <li>CPI 의미 : 보수적 정책 반복 (Conservative Policy Iteration)</li>
</ul>

<h4 id="24-probability-ratio--trpo">2.4 Probability ratio &amp; TRPO</h4>

<p>surrogate objective 함수에서 ‘새로운 정책 / 기존 정책’ 을 probability ratio,
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\5.png" width="330" /></center></p>

<p>라고 하며, TRPO (Trust Region Policy Optimization) 에서는 L<sup>CPI</sup> 을 최대화 하는 것을 목표로 한다. 
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\6.png" width="340" /></center></p>

<p>그러나 probablity ratio 를 그대로 사용할 경우, 해당 값이 과도하게 큰 경우, 학습이 실패하거나 성능이 정하되는 문제가 발생한다. 이를 해결하기 위해 TRPO 의 경우, KL-Divergence 를 사용하여 penalty 를 주는 방식으로 문제를 해결하지만 이 방법은 이해가 어렵고 구현이 난해하다는 단점이 있다. PPO 의 경우 이 문제를  Clipping 기법을 사용하여, 비교적 간단하게 해결 하였다.</p>

<h3 id="3-ppo-proximal-policy-optimization">3. PPO (Proximal Policy Optimization)</h3>

<h4 id="31-clipped-surrogate-objective">3.1 Clipped Surrogate Objective</h4>

<p>계산적으로 효율적인 Penalty 를 적용하고 과도한 Policy 업데이트를 방지 <br />
​<img src="assets\img\posts\2024-05-12-PPO1\7.png" width="420" />
​<center><img src="assets\img\posts\2024-05-12-PPO1\8.png" width="500" /></center></p>

<p>A<sub>t</sub> &gt; 0 일때, <br /></p>
<ol>
  <li>r = 1 : 새 정책과 이전 정책이 같은 확률로 동일 행동을 선택. 이 경우 새 정책에서의 어드밴티지 A<sub>t</sub>의 기대값은 변하지 않음.</li>
  <li>r &lt; 1 : 새 정책이 이전 정책보다 해당 행동을 선택할 확률이 낮아졌음을 의미. 즉, 새 정책이 이전 정책보다 더 적은 이득을 기대할 때 발생.</li>
  <li>r &gt; 1 : 정책이 이전 정책보다 해당 행동을 선택할 확률이 높아졌음을 의미. 새 정책에서 더 많은 이득을 기대</li>
  <li>Clipping 조건
    <ul>
      <li><strong>r &gt; 1 + ϵ</strong> :  r의 값이 너무 높게 나타나면 학습 과정에서 성능 붕괴가 발생할 위험이 있다. 예를 들어, 에이전트가 한 행동에 지나치게 의존하게 되면, 다른 잠재적으로 유리한 행동들을 탐색하지 못하고 환경에 대한 이해가 제한될 수 있다. <strong>클리핑은 L<sup>CLIP</sup> = (1 + ϵ) * A<sub>t</sub> 로 제한</strong>함으로써, 너무 큰 정책 변동을 방지하고 보다 안정적인 학습을 촉진.</li>
      <li><strong>r &lt; 1 - ϵ</strong> : <strong>r<sub>t</sub>(θ)의 값을 1 - ϵ 으로 고정.</strong> 이렇게 하면, 에이전트가 불리한 행동을 취하는 것으로 평가되어 이전보다 훨씬 적게 선택될 경우, 이러한 선택의 영향을 완화하여 학습의 안정성을 높이는 것이다.</li>
    </ul>
  </li>
</ol>

<p>A<sub>t</sub> &lt; 0 일때도 clipping 조건은 동일 하다.</p>

<h4 id="32-generalized-advantage-estimate-gae">3.2 Generalized Advantage Estimate (GAE)</h4>
<p>​<center><img src="assets\img\posts\2024-05-12-PPO1\8_1.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<h4 id="33-분산-강화-학습-distributed-rl">3.3 분산 강화 학습 (Distributed RL)</h4>

<p>다수의 환경을 통해 얻은 데이터 사용
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\9.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<h4 id="34-network-architecture">3.4 Network Architecture</h4>

<p>A2C 에서는 가치를 평가하는 critic network 와 행동을 결정하는 Actor network 가 각각 존재 하였으나, PPO 에서는 통합 네트워크로 운영됨.</p>

<p>​<center><img src="assets\img\posts\2024-05-12-PPO1\10.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<p>네트워크 업데이트
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\11.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<ol>
  <li>Actor Network Update
    <ul>
      <li>Surrogate Functon L<sub>t</sub><sup>CLIP</sup> 을 최대화 하도록 업데이트</li>
      <li>Action Entropy S[π<sub>θ</sub>](s<sub>t</sub>) 행동 선택의 불확실성을 나타내는 척도. 최대화하여 탐색 가능성을 높임</li>
    </ul>
  </li>
  <li>
    <p>Critic Network Update
L<sub>t</sub><sup>VF</sup> 는 예측 상태 가치 V<sub>θ</sub>(S<sub>t</sub>) 와 예측 상태  가치 V<sub>t</sub><sup>targ</sup> 의 차이 (오차)를 최소화 하도록 업데이트 (제곱의 의미 : 오차의 절대값을 고려 및 큰 오차에 대한 큰 패널티 부과)</p>
  </li>
  <li>Actor-Critic Network in PPO
Actor 와 Critic Network 를 통합하여 구성. c<sub>1</sub> 은 가치 함수 오차의 영향 조절 계수. c<sub>2</sub> 는 탐색을 장려하는 정도를 조절하는 계수</li>
</ol>

<h4 id="35-ppo-알고리즘-의사-코드">3.5 PPO 알고리즘 의사 코드</h4>

<p>​<center><img src="assets\img\posts\2024-05-12-PPO1\12.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<ol>
  <li>분산학습을 통해 N 갯수의 actor 를</li>
  <li>T timesteps 까지 실행하여</li>
  <li>GAE 기법을 사용한 각 actor 의 T-step TD error A<sub>t</sub><sup>(T)</sup> 를 추정한다.</li>
  <li>이렇게 모은 advantage set NT 에서 mini batch M 을 추출하여</li>
  <li>(목표) 대리함수 L 에 대해 확률적 경사 상승 (gardient) 를 적용하여</li>
  <li>파라미터 θ 를 업데이트</li>
  <li>K epochs : 모든 데이터를 통해 mini batch 학습한 횟수</li>
</ol>

<h4 id="36-ppo-알고리즘-logic-process">3.6 PPO 알고리즘 logic process</h4>

<p>PPO 알고리즘은 Actor-Critic 통합 network 를 통해 행동 결정 및 q 값 산출을 통한 업데이트를 진행된다.</p>

<p>​<center><img src="assets\img\posts\2024-05-12-PPO1\13.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<ol>
  <li>(I) T-steps 만큼 환경을 실행하여,</li>
  <li>(I) Trajectory Memory 에 정보를 저장한다.</li>
  <li>(L) 메모리에서  M (mini batch) 를 K (epochs) 만큼 추출</li>
  <li>(L) Network 학습 정보를 통해,</li>
  <li>(L) 현재 상태 가치와 타겟 상태 가치를 사용하여 Loss VF 값을 계산</li>
  <li>(L) Action Entropy 계산</li>
  <li>(L) probability ratio - r 과 GAE 로 계산된 advantage A<sub>t</sub> 를 clipping  하여 clipped objective L<sup>CLIP</sup>(θ)를 도출</li>
  <li>(L) clipped objective + 상태 가치 loss + action entropy 를 통해 목적식을 산출</li>
  <li>(L) stochastic gradient decent 를 적용하여 actor-critic 모델 업데이트</li>
</ol>

<h4 id="37-ppo-결과">3.7 PPO 결과</h4>

<p>아래와 같이 타 알고리즘 대비 빠른 학습 결과를 얻을 수 있음을 확인
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\14.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<p>Atari 환경의 다양한 게임에서 A2C, ACER 와 성능비교</p>

<p>해당 결과에서 모든 훈련 episode 평균 보상 값이  PPO 가 가장 높은 것을 확인. (빠르게 학습되므로)<br />
다만, 학습 후반에 대한 보상 평균 점수는 ACER 가 더 높은 것으로 확인됨.
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\15.png" width="500" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>]]></content><author><name>Yong gon Yun</name></author><category term="ppo" /><category term="proximal policy optimization" /><category term="surrogate object" /><category term="clipping" /><summary type="html"><![CDATA[PPO(Proximal Policy Optimization) 이론]]></summary></entry><entry><title type="html">DDPG(Deep Deterministic Policy Gradient) 구현</title><link href="http://localhost:4000/DDPG2_code.html" rel="alternate" type="text/html" title="DDPG(Deep Deterministic Policy Gradient) 구현" /><published>2024-05-07T12:32:20+09:00</published><updated>2024-05-07T12:32:20+09:00</updated><id>http://localhost:4000/DDPG2_code</id><content type="html" xml:base="http://localhost:4000/DDPG2_code.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>해당 내용은 다음의 강의 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">인프런 - 유니티 머신러닝 에이전트 완전정복 (기초편) | 민규식</a></li>
</ul>

<h3 id="0-코드-요약">0. 코드 요약</h3>

<p>해당 code 는 DDPG 를 적용한 신경망 학습 코드이며, 그 내용은 아래와 같다.</p>

<ol>
  <li>라이브러리 &amp; 파라미터 설정</li>
  <li>OU Noise class : 연속적인 행동을 선택할 때 탐험을 위한 noise 클래스</li>
  <li>Actor class : state 입력을 통해 action 을 선택하는 신경망 클래스</li>
  <li>Critic class: state, action 입력을 통해 q 값을 반환하는 신경망 클래스</li>
  <li>Agent class</li>
  <li>Main 함수</li>
</ol>

<p>해당 포스트에서는 코드의 알고리즘 진행 프로세스와 ML-agents 와 연동하여 어떻게 프로그램 로직이 연결되어 있는지 위주로 설명할 예정임. 해당 코드에서의 PyTroch , ML-agents 등 라이브러리 사용 및 환경 설정에 대한 설명은 이전 포스트 <a href="https://y2gon2.github.io/DQN_ml_agents.html">Deep Q-Network + ML-agents 구현</a>  와 유사함으로 해당 포스트를 참조.</p>

<h3 id="1-라이브러리--파라미터-설정">1. 라이브러리 &amp; 파라미터 설정</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="n">torch.unils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span> <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span>

<span class="c1"># DDPG 파라미터 
</span><span class="n">state_size</span> <span class="o">=</span> <span class="mi">9</span> <span class="c1"># (1)
</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># 각 축방향 값
</span>
<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">mem_maxlen</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">actor_lr</span> <span class="o">=</span> <span class="mf">1e-4</span>     <span class="c1"># actor network 학습률
</span><span class="n">critic_lr</span> <span class="o">=</span> <span class="mf">5e-4</span>    <span class="c1"># critic network 학습률
</span><span class="n">tau</span>  <span class="o">=</span> <span class="mf">1e-3</span>         <span class="c1"># soft target update parameter
</span>
<span class="c1"># OU noise 파라미터
</span><span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>          <span class="c1"># 회귀할 평균값
</span><span class="n">theta</span> <span class="o">=</span> <span class="mf">1e-3</span>    <span class="c1"># 회귀 속도
</span><span class="n">sigma</span> <span class="o">=</span> <span class="mf">2e-3</span>    <span class="c1"># 랜덤 프로세스의 변동성
</span>
<span class="n">run_step</span> <span class="o">=</span> <span class="mi">50000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span>
<span class="n">test_step</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">train_start_step</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 유니티 환경 경로
</span><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Drone</span><span class="sh">"</span>
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span>
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span>
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># 모델 저장 및 불러오기 경로
</span><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DDPG/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span>
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DDPG/202405071234</span><span class="sh">"</span>

<span class="c1"># 연산장치
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cudoa</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p>(1) state_size = 9</p>

<p>현재 드론의 위치 - 골인 지점의 위치 (x, y, z) 
현재 드론의 속도 (x, y, z)                  <br />
현재 드론의 각속도 (x, y, z)
=&gt; 총 9개의 상태값 사용</p>

<h3 id="2-ou-noise-class">2. OU Noise class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OU_noise</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># noise reset
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">action_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span> <span class="c1"># (1)
</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># noise sampling
</span>        <span class="n">dx</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">))</span> <span class="c1"># (2)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">+=</span> <span class="n">dx</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">x</span>
</code></pre></div></div>
<p>(1) self.X = np.ones((1, action_size), dtype=np.floate32) * mu</p>

<ul>
  <li>모두 1.0 값을 가지는 (1 * 3) 크기의 np array * 0 =  [[0.0, 0.0, 0.0]]<br /></li>
</ul>

<p>(2) dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))</p>

<ul>
  <li>len(self.X) = 1 (1 * 3 size np array 이므로)</li>
  <li>np.random.randn(1) : 평균 0, 표준편자 1 정규분포 내 임의의 값 1개</li>
</ul>

<h3 id="3-actor-class">3. Actor class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">mu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># (1)
</span></code></pre></div></div>

<p>​<center><img src="assets\img\posts\2024-05-12-PPO1\8_1.png" width="650" /></center>
  <a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard">이미지 출처</a></p>

<p>(1) torch.tanh(self.mu(x))</p>

<p>비화성화 함수 torch.tanh() 는 해당 결과를  -1 ~ 1 사이 값을 갖는 결과를 출력한다. 그런데 일반적으로 사용하는 relu() 가 아닌 tanh() 사용한 이유는 무엇일까? (by ChatGPT)</p>

<ul>
  <li>tanh()
​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\1.png" width="200" /></center></li>
</ul>

<ol>
  <li>
    <p>출력 범위 제한:
액션 공간(action space)이 연속적이고 특정 범위 내에서 정의될 때 (예: 로봇 팔의 관절 각도나 차량의 조향 각도), 액션의 크기를 적절히 제한하는 것이 중요합니다. tanh 함수는 자연스럽게 출력을 -1에서 1 사이로 스케일링하여, 액션의 범위를 효과적으로 제어할 수 있게 합니다.</p>
  </li>
  <li>
    <p>미분 가능성과 비선형성:
tanh 함수는 연속적이며 미분 가능한 비선형 함수로서, 신경망의 학습 과정에서 그라디언트 기반 최적화 방법을 사용할 때 중요한 역할을 합니다. 비선형 함수를 사용함으로써, 네트워크는 더 복잡한 액션 선택 전략을 학습할 수 있습니다.</p>
  </li>
  <li>
    <p>Zero-Centered 출력:
tanh는 출력이 중심(0)을 기준으로 대칭이라는 장점이 있습니다. 이는 학습 과정에서 네트워크가 편향되지 않게 하는 데 도움이 될 수 있습니다. 예를 들어, 로봇이 앞으로 가거나 뒤로 가는 액션을 선택할 때, tanh는 양의 방향과 음의 방향을 모두 동일한 범위로 제공하여 균형 잡힌 학습을 촉진합니다.</p>
  </li>
  <li>
    <p>자연스러운 전환:
tanh 함수는 -1과 1 근처에서 포화(saturation)되는 성질을 가지고 있습니다. 이는 매우 높거나 낮은 입력 값에 대해 출력의 변화가 적어짐을 의미합니다. 이 포화 특성은 때때로 네트워크가 극단적인 액션을 취하는 것을 자연스럽게 제한할 수 있습니다.</p>
  </li>
</ol>

<center>Actor Network</center>
<p>​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\2.png" width="400" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="4-critic-class">4. Critic class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Critic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span> <span class="o">+</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># q 값 반환
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>(1) x = torch.cat((x, action), dim=-1)</p>

<p>critic network 에 input parameter 는 state 와 action 두 개 이다. 두 요소가 서로 관련은 있지만, 수식적으로 동등한 요소로 취급해도 되는 것일까? 두 요소를 다르게 취급하려면 어떻게 해야 할까? (by ChatGPT)</p>

<p>상태(state)와 행동(action) 정보를 같은 첫 번째 층에 동시에 넣는 것 대신 두 번째 층에서 결합하는 방식을 선택하는 이유는 신경망의 학습 동역학과 관련된 몇 가지 중요한 고려사항 때문입니다. 여기서 고려하는 접근 방식은 알고리즘의 성능과 일반화 능력을 최적화하는 데 도움이 됩니다.</p>

<ol>
  <li>
    <p>정보 처리의 단계적 접근:
상태(state) 정보는 환경에 대한 전반적인 맥락을 제공하고, 행동(action) 정보는 특정 상황에서 취할 구체적인 조치를 나타냅니다. 첫 번째 층에서 상태 정보만 처리함으로써 네트워크는 먼저 상태에 대한 특징을 추출하고 이해하는 데 집중할 수 있습니다. 그런 다음, 두 번째 층에서 이 상태 특징을 행동 정보와 결합함으로써, 특정 상태에서 특정 행동을 취했을 때의 결과(예: Q-값)를 학습하는 데 필요한 관계를 더 효과적으로 모델링할 수 있습니다.</p>
  </li>
  <li>
    <p>학습의 효율성:
상태와 행동을 각각 다른 층에서 처리함으로써 학습 과정에서 더 효율적으로 정보를 조합할 수 있습니다. 상태 정보에 기반한 특징은 행동에 독립적으로 유용할 수 있으며, 이를 통해 상태의 중요한 특성을 더 넓은 맥락에서 추출할 수 있습니다. 이러한 접근은 특히 행동 공간이 클 때 유용하며, 행동과 상태 사이의 복잡한 상호작용을 학습하는 데 도움을 줍니다.</p>
  </li>
  <li>
    <p>모듈성과 유연성:
상태와 행동을 별도의 층에서 처리하는 것은 신경망의 모듈성을 증가시키고, 각각의 입력 유형에 특화된 처리 방식을 적용할 수 있게 합니다. 이는 신경망이 더 다양한 종류의 문제에 적응하고, 상태 또는 행동의 표현이 변경되었을 때 쉽게 확장하거나 수정할 수 있는 구조를 제공합니다.</p>
  </li>
  <li>
    <p>노이즈와 불확실성 관리:
상태 데이터는 종종 노이즈가 많고 불확실할 수 있습니다. 상태 정보를 먼저 처리하고 그 결과를 행동과 결합함으로써, 네트워크는 노이즈가 결과에 미치는 영향을 최소화할 수 있으며, 더 정확한 Q-값 예측을 위한 정보 처리가 가능해집니다.</p>
  </li>
</ol>

<p>이러한 이유로, 상태와 행동을 다른 층에서 처리하는 것은 일반적으로 더 나은 결과를 제공하며, 강화 학습 알고리즘에서 널리 사용되는 접근 방식입니다.</p>

<p>위와 같은 이유로 첫번째 layer 에 state 를 넣고, 두번째 layer 에 action 을 추가하기 위해, layer 의 units 을 확장(concatenate) 시킬 수 있는 torch.cat((x, action), dim=-1) 를 사용하였다. 여기서 <code class="language-plaintext highlighter-rouge">dim=-1</code>
은 x, 와 action 텐서의 마지막 차원(열) 갯수의 합으로 units 확장하여 결합시킨다는 의미이다.</p>

<p>따라서 해당 네트워크를 도식화 하면 아래와 같다.</p>

<p>​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\3.png" width="400" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="5-agent-class">5. Agent class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DDPGAgent</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_actor</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">actor_lr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_critic</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cirtic</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">critic_lr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">OU</span> <span class="o">=</span> <span class="nc">OU_noise</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">mem_maxlen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_actor</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">actor_mitimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor_optimizer</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">critic</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">load_state_dic</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">critic_optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># OU noise 기법에 따라 행동결정
</span>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># 네트워크 모드 설정
</span>        <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span> <span class="c1"># 훈련 상태로 준비 (ex. Dropout layer 일부 노드 무작위 비활성화, 입력 정규화 (BatchNorm layer))
</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

        <span class="c1"># train_mode -&gt; action + noise 반환
</span>        <span class="c1"># test_mode  -&gt; action 반환
</span>        <span class="k">return</span> <span class="n">action</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">OU</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="n">action</span>
    
    <span class="c1"># 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
</span>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Critic update
</span>        <span class="n">next_actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_critic</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_actions</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_q</span> <span class="c1"># (3)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="c1"># (4)
</span>        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">target_q</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span> <span class="c1"># (5)
</span>
        <span class="c1"># (6)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> 
        <span class="n">critic_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># Actor update
</span>        <span class="n">action_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># (7)
</span>        <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action_pred</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="c1"># (8)
</span>
        <span class="c1"># (9)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">actor_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">actor_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">critic_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="c1"># soft target update
</span>    <span class="k">def</span> <span class="nf">soft_update_target</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">target_actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">local_param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">target_critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">local_param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># 네트워크 모델 저장
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">actor</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">actor_optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">critic</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">critic_optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># 학습 기록
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/actor_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/critic_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
</code></pre></div></div>

<p>train_model 메서드 process 진행 과정 도식도</p>

<p>​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\4.png" width="700" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="6-main-함수">6. Main 함수</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 유니티 환경 경로 설정 (file name)
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
        <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 유니티 브레인 설정
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>

    <span class="c1"># DDPGAgent 클래스를 agent 로 정의
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">DDPGAgent</span><span class="p">()</span>
    <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># (1)
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_continuous</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span> <span class="c1"># (3)
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># (4)
</span>
        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> <span class="c1"># (5)
</span>        <span class="n">done</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span> <span class="c1">#(5.1)
</span>        <span class="n">next_state</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># (5.2)
</span>        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span> <span class="c1"># (6)
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_start_step</span><span class="p">):</span>
            <span class="c1"># 학습수행
</span>            <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span> <span class="c1"># (7)
</span>            <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
            <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>

            <span class="c1"># 타겟 네트워크 소프스 업데이트
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">soft_update_target</span><span class="p">()</span> <span class="c1"># (8)
</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 게임 진행 상황 출력 및 텐서보드에 보상과 손실함수 값 기록
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_actor_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span>
                <span class="n">mean_critic_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_actor_loss</span><span class="p">,</span> <span class="n">mean_critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / </span><span class="sh">"</span><span class="o">+</span>\
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Actor loss: </span><span class="si">{</span><span class="n">mean_actor_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / Critic loss: </span><span class="si">{</span><span class="n">mean_critic_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                
            <span class="c1"># 네트워크 모델 저장
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<p>프로그램 실행 도식도</p>

<p>​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\5.png" width="700" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="7-전체-코드">7. 전체 코드</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="n">torch.unils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span>
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span> <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span>

<span class="c1"># DDPG 파라미터 
</span><span class="n">state_size</span> <span class="o">=</span> <span class="mi">9</span>
<span class="c1"># 현재 드론의 위치 - 골인 지점의 위치 (x, y, z)
# 현재 드론의 속도 (x, y, z)
# 현재 드론의 각속도 (x, y, z)
#  =&gt; 총 9개 
</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># 각 축방향 값
</span>
<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">mem_maxlen</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">actor_lr</span> <span class="o">=</span> <span class="mf">1e-4</span>     <span class="c1"># actor network 학습률
</span><span class="n">critic_lr</span> <span class="o">=</span> <span class="mf">5e-4</span>    <span class="c1"># critic network 학습률
</span><span class="n">tau</span>  <span class="o">=</span> <span class="mf">1e-3</span>         <span class="c1"># soft target update parameter
</span>
<span class="c1"># OU noise 파라미터
</span><span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>          <span class="c1"># 회귀할 평균값
</span><span class="n">theta</span> <span class="o">=</span> <span class="mf">1e-3</span>    <span class="c1"># 회귀 속도
</span><span class="n">sigma</span> <span class="o">=</span> <span class="mf">2e-3</span>    <span class="c1"># 랜덤 프로세스의 변동성
</span>
<span class="n">run_step</span> <span class="o">=</span> <span class="mi">50000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span>
<span class="n">test_step</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">train_start_step</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># 유니티 환경 경로
</span><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Drone</span><span class="sh">"</span>
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span>
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span>
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="c1"># 모델 저장 및 불러오기 경로
</span><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DDPG/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span>
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DDPG/202405071234</span><span class="sh">"</span>

<span class="c1"># 연산장치
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cudoa</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># OU noise 클래스 -&gt; ou noise 정의 및 파라미터 결정
</span><span class="k">class</span> <span class="nc">OU_noise</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># noise reset
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">action_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span> <span class="c1"># [[0.0, 0.0, 0.0]]
</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># noise sampling
</span>        <span class="n">dx</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">))</span>
        <span class="c1"># len(self.X) = 1 (1 * 3 size np array 이므로)
</span>        <span class="c1"># np.random.randn(1) : 평균 0, 표준편자 1 정규분포 내 임의의 값 1개
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">+=</span> <span class="n">dx</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">x</span>
    
<span class="c1"># Actor 클래스
</span><span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">mu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># -1 ~ 1
</span>
<span class="c1"># Critic class
</span><span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Critic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span> <span class="o">+</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># q 값 반환
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># DDPGAgent 
</span><span class="k">class</span> <span class="nc">DDPGAgent</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_actor</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">actor_lr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_critic</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cirtic</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">critic_lr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">OU</span> <span class="o">=</span> <span class="nc">OU_noise</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">mem_maxlen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_actor</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">actor_mitimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">actor_optimizer</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">critic</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">load_state_dic</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">critic_optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># OU noise 기법에 따라 행동결정
</span>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># 네트워크 모드 설정
</span>        <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span> <span class="c1"># 훈련 상태로 준비 (ex. Dropout layer 일부 노드 무작위 비활성화, 입력 정규화 (BatchNorm layer))
</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

        <span class="c1"># train_mode -&gt; action + noise 반환
</span>        <span class="c1"># test_mode  -&gt; action 반환
</span>        <span class="k">return</span> <span class="n">action</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">OU</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="n">action</span>
    
    <span class="c1"># 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
</span>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
            <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Critic update
</span>        <span class="n">next_actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_action</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_critic</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_actions</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">next_q</span> <span class="c1"># (3)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="c1"># (4)
</span>        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">target_q</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span> <span class="c1"># (5)
</span>
        <span class="c1"># (6)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> 
        <span class="n">critic_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># Actor update
</span>        <span class="n">action_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># (7)
</span>        <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action_pred</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span> <span class="c1"># (8)
</span>
        <span class="c1"># (9)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">actor_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">actor_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span> <span class="n">critic_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="c1"># soft target update
</span>    <span class="k">def</span> <span class="nf">soft_update_target</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">target_actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">local_param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">target_critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">tau</span> <span class="o">*</span> <span class="n">local_param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tau</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># 네트워크 모델 저장
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">actor</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">actor_optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">critic</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">critic_optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># 학습 기록
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/actor_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/critic_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>


<span class="c1"># Main
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 유니티 환경 경로 설정 (file name)
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span>
        <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 유니티 브레인 설정
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span>

    <span class="c1"># DDPGAgent 클래스를 agent 로 정의
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">DDPGAgent</span><span class="p">()</span>
    <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># (1)
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span>
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_continuous</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span> <span class="c1"># (3)
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># (4)
</span>
        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> <span class="c1"># (5)
</span>        <span class="n">done</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span> <span class="c1">#(5.1)
</span>        <span class="n">next_state</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># (5.2)
</span>        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span> <span class="c1"># (6)
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_start_step</span><span class="p">):</span>
            <span class="c1"># 학습수행
</span>            <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span> <span class="c1"># (7)
</span>            <span class="n">actor_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
            <span class="n">critic_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>

            <span class="c1"># 타겟 네트워크 소프스 업데이트
</span>            <span class="n">agent</span><span class="p">.</span><span class="nf">soft_update_target</span><span class="p">()</span> <span class="c1"># (8)
</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 게임 진행 상황 출력 및 텐서보드에 보상과 손실함수 값 기록
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_actor_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">actor_losses</span><span class="p">)</span>
                <span class="n">mean_critic_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">)</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_actor_loss</span><span class="p">,</span> <span class="n">mean_critic_loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">actor_losses</span><span class="p">,</span> <span class="n">critic_losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / </span><span class="sh">"</span><span class="o">+</span>\
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Actor loss: </span><span class="si">{</span><span class="n">mean_actor_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> / Critic loss: </span><span class="si">{</span><span class="n">mean_critic_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
                
            <span class="c1"># 네트워크 모델 저장
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>


</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="DDPG" /><category term="Deep Deterministic Policy Gradient" /><category term="DQN" /><category term="pytorch" /><summary type="html"><![CDATA[DDPG(Deep Deterministic Policy Gradient) 구현]]></summary></entry><entry><title type="html">DDPG(Deep Deterministic Policy Gradient) Algorithm 이해</title><link href="http://localhost:4000/DDPG1.html" rel="alternate" type="text/html" title="DDPG(Deep Deterministic Policy Gradient) Algorithm 이해" /><published>2024-05-07T09:32:20+09:00</published><updated>2024-05-07T09:32:20+09:00</updated><id>http://localhost:4000/DDPG1</id><content type="html" xml:base="http://localhost:4000/DDPG1.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>해당 내용은 다음의 강의 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">인프런 - 유니티 머신러닝 에이전트 완전정복 (기초편) | 민규식</a></li>
</ul>

<h3 id="1-기존-dqn-의-한계">1. 기존 DQN 의 한계</h3>

<p>선택하는 action 이 이산적인 행동(ex. 상, 하, 좌, 우)환경에만 적용 가능. 따라서 로봇 팔의 움직임, 로콧의 엔진 분출량 조절과 같은 연속적인 선택의 환경에서 적용 불가</p>

<h3 id="2-deep-deterministic-policy-gradient-ddpg">2. Deep Deterministic Policy Gradient (DDPG)</h3>

<ul>
  <li>Actor-Critic 기반 강화학습 알고리즘</li>
  <li>DPG(Deterministic Policy Gradient) 알고리즘을 신경망 network 에 적용</li>
  <li>연속적인 값 중에서 한가지 행동 값을 출력</li>
  <li>행동을 선택하는 actor 와 해당 행동을 실제로 수행했을 때 다음 상태에서의 q 값을 확인하는 critic 으로 강화학습을 구현</li>
</ul>

<h3 id="3-ddpg-알고리즘-기법">3. DDPG 알고리즘 기법</h3>

<h4 id="31-경험-리플레이-experience-replay">3.1 경험 리플레이 (Experience Replay)</h4>

<p>DQN 에서 사용한 바와 같이, 학습을 수행하면서 경험하는 정보를 일정랑 보관하고, 해당 데이터를 임의의 Batch 크기만큼씩 가져와서 훈련시 사용. 데이터간 상관관계 문제를 해결</p>

<h4 id="32-타겟-네트워크-target-network---soft-target-update-기법">3.2 타겟 네트워크 (Target Network) - Soft Target Update 기법</h4>

<p>기존 DQN 에서는 Target Network 의 경우, 매 step 마다 업데이트하는 경우, Target 애 매 학습마다 변화하는 문제를 발생시키기 때문에, Target Network 는 고정시킨 상태로 학습을 진행. 일정 주기마다 학습 Network 와 동기화 시켜 사용하였다.</p>

<p>그러나 연속적인 선택에서 이러한 Target Network update 는 맞지 않아 Soft Target Update 기법을 사용하여 매 step update 를 진행함.</p>

<ul>
  <li>지수이동평균(Exponential Moving Average, EMA) 과 같은 방법을 통한 업데이트</li>
</ul>

<center><img src="assets\img\posts\2024-05-07-DDPG1\1.png" width="420" /></center>

<ul>
  <li>θ  : 학습을 통해 산출된 파라미터</li>
  <li>
    <p>θ- : Target Network 파라미터</p>
  </li>
  <li>0 &lt;=  τ &lt;=  1 값을 통해 기존 Target Network Parameters 업데이트 수준을 조절.
    <ul>
      <li>τ == 0    : 학습된 파라미터로 완전 업데이트 (기존 DQN 방식)</li>
      <li>0 &lt; τ &lt; 1 : 일정 비율로 비례해서 학습된 파라미터 값을 기존 타겟 네트워크 파라미터에 반영</li>
    </ul>
  </li>
</ul>

<h4 id="33--ou-noise-를-사용한-탐험">3.3  OU Noise 를 사용한 탐험</h4>

<ul>
  <li>역시 연속된 행동 환경에서 선택 가능한 행동의 수가 무한이므로 기존의 epsilon-greedy 기법을 사용할 수 없음.</li>
  <li>실수 범위에서 행동을 선택하여 탐험할 수 있는 랜덤 평균 회귀 노이즈 생성</li>
</ul>

<center><img src="assets\img\posts\2024-05-07-DDPG1\2.png" width="530" /></center>

<ul>
  <li>1 번 식을 통해 현재 상태값 Xt 에서 다음 상태 Xt+1 로 변경시키는 그 변위 dx 는 2번 식과 같은 형태로 구성된다.</li>
  <li>2 번 식의 우변 값을 결정하는 요소들은 다음과 같다.
    <ul>
      <li>θ   : 평균 회귀 속도, 즉 변수가 평균 값 𝜇로 돌아가려는 속도</li>
      <li>μ   : 평균 값으로, 시스템이 장기적으로 안정되려는 목표 상태</li>
      <li>σ   : 노이즈의 크기 또는 강도, 시스템의 변동성을 결정</li>
      <li>dt  : 시간 증분, 일반적으로 미분 방정식을 시뮬레이션할 때 사용되는 시간의 단위</li>
      <li>dWt : 위너 과정(Wiener process) 또는 브라운 운동으로부터 파생된 임의의 충격. dt 시간 동안의 무작위 움직임을 표현</li>
    </ul>

    <p>즉,  μ − Xt 는 Xt 가 𝜇로 회귀하려는 정도 또는 현재 위치에서 목표 위치까지의 차이를 의미한다. 그리고 그 회귀가 얼마나 빠른가는 θ 에 비례한다. 다만 σdWt (노이즈 * dt 시간 동안의 무작위 움직임) 가 항상 더해지고 있으므로, 목표값에 도달하더라도 일정 수준의 연속적인 변동이 발생한다.</p>
  </li>
</ul>

<p>​<center><img src="assets\img\posts\2024-05-07-DDPG1\3.png" width="650" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h4 id="34-critic-network-update">3.4 Critic Network Update</h4>

<p>DQN 과 동일하게 벨만 방정식을 사용하여 Q(x) 가 최대인 값을 타겟값으로 업데이트 한다. 
​<center><img src="assets\img\posts\2024-05-07-DDPG1\4.png" width="580" /></center></p>

<p><br />
손실함수의 경우도 DQN 과 동일한, 차이 제곱 평균 (MSE) 적용
​<center><img src="assets\img\posts\2024-05-07-DDPG1\5.png" width="260" /></center>
<br />
actor network update 는 목표 함수값를 최대하하는 방향으로 정책을 업데이트</p>

<p>​<center><img src="assets\img\posts\2024-05-07-DDPG1\6.png" width="650" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a>
<br />
목표 함수를 최대화 하는 방향 계산을 위한 gradient (아래 유도 과정은 아직 이해 X ;;)</p>

<p>​<center><img src="assets\img\posts\2024-05-07-DDPG1\7.png" width="650" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a>
<br /></p>

<h3 id="4-ddpg-알고리즘을-사용한-network-학습-프로세스">4. DDPG 알고리즘을 사용한 network 학습 프로세스</h3>

<p>​<center><img src="assets\img\posts\2024-05-07-DDPG1\8.png" width="650" /></center>
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<ol>
  <li>Agent 가 환경과 상호작용 (상태 전이)</li>
  <li>상호작용시 Agent 의 행동을 결정하는 것은 Actor Network 
(input: 현재 상태 s -&gt; output : 행동 a)
다만 이때, 행동은 연속적며, 이런 연속적 행동 선택에서 OU noise 를 추가하여 탐험을 수행</li>
  <li>2번으로 선택된 행동을 환경에 적용, 다음 단계의 상태 (경험) 을 생성.</li>
  <li>경험 데이터를 replay memory 에 저장</li>
  <li>(경험 데이터가 일정량 이상 쌓이 이후), mini batch data 를 sampling 하여 학습을 수행</li>
  <li>critic network 학습 (input : 상태, 행동 -&gt; output : Q(s, a) 값)
    <ul>
      <li>6.1 (일반) Critic Network (input : s, a -&gt; output: q)</li>
      <li>6.2 Traget Critic Network (input : s’, a’ -&gt; output: q’)</li>
      <li>6.3 6.1 6.2 결과값의 차이를 통해 손실값을 계산</li>
      <li>6.4 손실값이 최소화 되도록 critic network 를 update</li>
      <li>6.5 매 step 마다 soft target update 로 critic network 를 통해 target critic network 를 update</li>
    </ul>
  </li>
  <li>Critic Network 의 q 값을 최대화하는 방향으로 Policy Gradient 를 통해 Actor Network 을 학습시킴.</li>
  <li>6.5 와 동일하게 매 step 마다 (일반) soft target update 로 actor network 를 통해 target actor network 를 update</li>
</ol>]]></content><author><name>Yong gon Yun</name></author><category term="DDPG" /><category term="Deep Deterministic Policy Gradient" /><category term="DQN" /><summary type="html"><![CDATA[DDPG(Deep Deterministic Policy Gradient) Algorithm 이해]]></summary></entry><entry><title type="html">Deep Q-Network + ML-agents 구현</title><link href="http://localhost:4000/DQN_ml_agents.html" rel="alternate" type="text/html" title="Deep Q-Network + ML-agents 구현" /><published>2024-05-03T09:32:20+09:00</published><updated>2024-05-03T09:32:20+09:00</updated><id>http://localhost:4000/DQN_ml_agents</id><content type="html" xml:base="http://localhost:4000/DQN_ml_agents.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<ul>
  <li>
    <p>해당 내용은 다음의 강의 내용을 개인적으로 재학습 하기 위해 작성됨. <br />
<a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">인프런 - 유니티 머신러닝 에이전트 완전정복 (기초편) | 민규식</a></p>
  </li>
  <li>
    <p>DQN 알고리즘의 전체 흐름</p>
  </li>
</ul>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\0.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="0-전체-코드-요약">0. 전체 코드 요약</h3>

<p>전체 코드는 다음의 내용으로 구성된다.</p>

<ol>
  <li>프로그램 기본 설정
    <ul>
      <li>필요한 파이썬 라이브러리 가져오기</li>
      <li>파라미터 설정</li>
      <li>유니티 연결 환경 설정</li>
      <li>학습된 모델 저장/불러오기</li>
      <li>연산장치 (CPU or GPU) 선택<br /><br /></li>
    </ul>
  </li>
  <li>Deep Q-Network class 정의
    <ul>
      <li>Layer 구현 (입/출력, Convolution layer 정의)</li>
      <li>신경망 함수<br /><br /></li>
    </ul>
  </li>
  <li>DQNAgent class 정의
    <ul>
      <li>Agent 구현 환경 정의(ex. network, optimizer, memory)</li>
      <li>network 를 통한 action 선택 함수</li>
      <li>replay memory 에 데이터 추가 함수</li>
      <li>network parameter 학습 시키는 함수</li>
      <li>target_network update 함수</li>
      <li>모델 저장 함수</li>
      <li>tensorboard 기록 함수  <br /><br /></li>
    </ul>
  </li>
  <li>프로그램 동작 구현 (main)
    <ul>
      <li>unity 와 상호 작용이 가능한  UnityEnvironment 인스턴스(env) 생성</li>
      <li>env 로 부터 관측/target 공간 정보, step 진행 후 정보 및 구동 환경(time scale) 설정</li>
      <li>반복문을 통해  run_step + test_step 동안 학습을 진행시킴
        <ul>
          <li>(run_step 마지막 단계에서 모델을 저장하고 test_mode 로 전환)</li>
          <li>전처리: 시각적 관측 정보와 목적지 관측 정보를 전처리하여 state 로 저장</li>
          <li>agent 를 통해 action 을 결정하고, 해당 action 으로 unity 에서 다음 step 을 진행시킴</li>
          <li>진행된 현재 step 정보 가져옴</li>
          <li>종료(termination) 확인 및 next_step -&gt; next_state 정보로 전처리</li>
          <li>(train mode 일 경우) next_state 를 replay memory 에 저장</li>
          <li>충분히 메모리에 state 정보가 차 있다면, 모델 학습으로 손실값을 계산하고, 일정 주기로 target_model 을 update 함.</li>
          <li>episode 종료 시, 필요한 설정값을 조정하고, tensorboard 에 보상/손실 값을 기록, 필요 조건마다 훈련된 모델 저장</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h3 id="1-프로그램-기본-설정">1. 프로그램 기본 설정</h3>
<h4 id="11-필요한-파이썬-라이브러리-가져오기">1.1 필요한 파이썬 라이브러리 가져오기</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span> <span class="c1"># system (OS) 관련
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">troch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="n">mlagetns_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span> <span class="c1"># (1)
</span><span class="kn">from</span> <span class="n">malagents_envs.side_channel.engine_configuration_channel</span> <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span> <span class="c1"># (2)
</span></code></pre></div></div>
<p>(1) 유니티 환경 클래스, 액션을 환경에 전달하기 위한 환경 객체 <br />
(2) 유니티 환경 조건을 조정하기 위한 라이브러리 (ex. 타임 스케일 조절)</p>

<h4 id="12-파라미터-설정">1.2 파라미터 설정</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">state_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">84</span><span class="p">]</span> <span class="c1"># goal-plus RGB + goal-ex RGB =&gt; 6 채널 * h * w (아래 이미지 참조)
</span><span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># 오른쪽, 왼쪽, 위, 아래
</span>
<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># 모델 불러오기 여부
</span><span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># 모델 학습 여부 (True : 학습모드, False: 평가모드)
</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">mem_maxlen</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># replay memory 최대 크기
</span><span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1"># 미래에 대한 보상 감가율
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00025</span> <span class="c1"># 네트워크 학습률
</span>
<span class="n">run_step</span> <span class="o">=</span> <span class="mi">50000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span> <span class="c1"># 학습모드에서 진행할 스텝 수 설정 (평가 모드 = 0)
</span><span class="n">test_step</span> <span class="o">=</span> <span class="mi">5000</span>         <span class="c1"># 평가 모드에서 진행할 스텝 수
</span><span class="n">train_start_step</span> <span class="o">=</span> <span class="mi">5000</span>  <span class="c1"># 학습 시작 전에 리플레이 메모리에 충분한 데이터를 모으기 위해 몇 스텝동안 임의의 행동으로 게임 진행할 것인지 설정
</span><span class="n">target_update_step</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># 타겟 네트워크를 몇 스텝 주기로 업데이트 할지 설정
</span>
<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>     <span class="c1"># 학습 진행 상황을 텐서보드에 기록할 주기
</span><span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>     <span class="c1"># 학습 모델을 저장할 에피스드 주기 설정
</span>
<span class="n">epsilon_eval</span> <span class="o">=</span> <span class="mf">0.05</span>     <span class="c1"># 평가모드의 eps 값
</span><span class="n">epsilon_init</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="n">epsilon_eval</span> <span class="c1"># eps 초기값
</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.1</span>       <span class="c1"># 학습구간에서의 eps 최소값
</span><span class="n">explore_step</span> <span class="o">=</span> <span class="n">run_step</span> <span class="o">*</span> <span class="mf">0.8</span> <span class="c1"># eps 이 감소되는 구간
</span><span class="n">eplsilon_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">epsilon_init</span> <span class="o">-</span> <span class="n">epsilon_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">explore_step</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mf">0.05</span>
                        <span class="c1"># 한스텝당 감소하는 eps 변화량
</span>
<span class="c1"># 다음의 파라미터 값들은 실제 데이터를 가리키는 인덱스, 즉 enum 과 유사한 개념으로 사용됨.
</span><span class="n">VISUAL_OBS</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 시각적 관측 데이터. 에이전트가 이미지 형태로 관측하는 정보를 가리키는 인덱스 
</span><span class="n">GOAL_OBS</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># 목적지 관측 데이터. 에이전트가 목표를 달성하는데 필요한 정포를 가리키는 인덱스
</span><span class="n">VECTOR_OBS</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 수치적 관측 인덱스. 에이전트가 벡터 형태로 관측하는 정보를 가리키는 인덱스
</span><span class="n">OBS</span> <span class="o">=</span> <span class="n">VISUAL_OBS</span> <span class="c1"># DQN 에서는 시각적 관측 인덱스를 사용
</span></code></pre></div></div>

<ul>
  <li>state_size = [3*2, 64, 84] 관련 이미지</li>
</ul>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\1.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<ul>
  <li>epsilon-greedy 를 적용한 학습에서 각 파라미터들의 사용 그래프</li>
</ul>
<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\2.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h4 id="13-유니티-연결-환경-설정">1.3 유니티 연결 환경 설정</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">GridWorld</span><span class="sh">"</span>          <span class="c1"># 환경 빌드명 
</span><span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span> <span class="c1"># 현재 사용 OS
</span><span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span> <span class="c1"># 불러올 유니티 환경 경로
</span><span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span> <span class="c1"># Mac OS
</span>    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

</code></pre></div></div>

<h4 id="14-학습된-모델-저장불러오기">1.4 학습된 모델 저장/불러오기</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DQN/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span> <span class="c1"># 모델 파일일 저장될 경로
</span><span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DQN/20240503201212</span><span class="sh">"</span> <span class="c1"># 불러올 모델 파일 경로
</span></code></pre></div></div>

<h4 id="15-연산장치-cpu-or-gpu-선택">1.5 연산장치 (CPU or GPU) 선택</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 연산 장치 (CPU or GPU)
</span></code></pre></div></div>

<h3 id="2-deep-q-network-class-정의">2. Deep Q-Network class 정의</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># 2.1 Layer 구현 (입/출력, Convolution layer 정의)
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DQN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">state_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span>
            <span class="p">)</span>
        <span class="n">dim1</span> <span class="o">=</span> <span class="p">((</span><span class="n">state_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">8</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">state_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">8</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span>
            <span class="p">)</span>
        <span class="n">dim2</span> <span class="o">=</span> <span class="p">((</span><span class="n">dim1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">dim1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="n">dim3</span> <span class="o">=</span> <span class="p">((</span><span class="n">dim2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">//</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">dim2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">//</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()</span> <span class="c1"># 전체 텐서를 1차원을 변환
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="o">*</span><span class="n">dim3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">dim3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">512</span><span class="p">)</span> <span class="c1"># 완전 연결 레이어를 만들어 주기 위함. 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>

    <span class="c1"># 2.2 신경망 함수
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 데이터 차원 순서  변환 input : unity data (H, W, Ch) -&gt; pytorch data (Ch, H, W)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">flat</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

</code></pre></div></div>
<h4 id="21-layer-구현-입출력-convolution-layer-정의">2.1 Layer 구현 (입/출력, Convolution layer 정의)</h4>

<p>ML-Agents 를 사용하여 unity 부터 상태 정보를 받을 때, agent 에 설정된 카메라를 통한 이미지를 받아 사용하거나, 해당 환경에서의 좌표값 (vector) 값을 사용할 수 있다. 해당 모델에서는 이미지를 사용하여 처리할 것이므로, 이미지 처리에 적합한 convolution layer 를 사용하여 처리하는 것으로 구현되었다.</p>

<ul>
  <li>convolution layer + flattent layer + linear layer 의 연결 이미지</li>
</ul>
<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\3.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h4 id="22-신경망-함수">2.2 신경망 함수</h4>

<p>일반적으로 PyTorch 에서 신경망 모델 구현 클래스는 torch.nn.Module 을 상속받아 사용한다. 해당 부모 class 에 <code class="language-plaintext highlighter-rouge">__call__</code> 메서드 상 정의에 의해 해당 class 명으로 요청 (ex. <code class="language-plaintext highlighter-rouge">DQN()</code>)시 <code class="language-plaintext highlighter-rouge">forward</code> 메서드가 실행 요청된다.</p>

<ul>
  <li>구현된 신경망 모델 개념도</li>
</ul>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\4.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h3 id="3-dqnagent-class-정의">3. DQNAgent class 정의</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">:</span>
    <span class="c1"># 3.1 Agent 구현 환경 정의(ex. network, optimizer, memory)
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">mem_maxlen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_init</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> 
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 
            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="c1"># 3.2 network 를 통한 action 선택 함수
</span>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span> 
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="n">epsilon_eval</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> 
        <span class="k">return</span> <span class="n">action</span>
        
    <span class="c1"># 3.3 replay memory 에 데이터 추가 함수
</span>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="c1"># 3.4 network parameter 학습 시키는 함수
</span>    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">eye</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span class="n">action</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">long</span><span class="p">()]</span> 

        <span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">*</span> <span class="n">one_hot_action</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">next_q</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">values</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span>
 
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>             
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>      
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="n">eplsilon_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="c1"># 3.5 target_network update 함수
</span>    <span class="k">def</span> <span class="nf">update_target</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

    <span class="c1"># 모델 저장 함수
</span>    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">network</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>   
            <span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># tesorboard 기록 
</span>    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/epsilon</span><span class="sh">"</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="31-agent-구현-환경-정의ex-network-optimizer-memory">3.1 Agent 구현 환경 정의(ex. network, optimizer, memory)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">mem_maxlen</span><span class="p">)</span> <span class="c1"># (3)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_init</span> <span class="c1"># (4)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># (5)
</span>            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 
            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<p>(1) self.network = DQN().to(device)<br />
    훈련에 사용할 network 를 DQN 인스턴스를 생성하여 연산 device 메모리에 넣는다.</p>

<p>(2) self.target_network = copy.deepcopy(self.network)<br />
    초기 Target_network 설정은 훈련용과 동일하게 설정되므로 그대로 깊은 복사하여 사용</p>

<p>(3) self.memory = deque(maxlen=mem_maxlen) <br />
    replay memory 로 사용될 자료 구조는 FIFO 구조인 deque 를 사용</p>

<p>(4) self.epsilon = epsilon_init<br />
    초기 설정 epsilon 값으로 사용되며, 훈련이 반복되면서 앞에서 언급된 그래프의 형태와 같이 epsilon 값을 작게 하여 무작위 요소를 점차 줄여 나간다.</p>

<p>(5) if load_model == True:<br />
    만약 기존에 저장된 model 을 사용하고자 할 경우, 해당 조건문 실행으로 기존 모델을 가져와서 실행</p>

<h4 id="32-network-를-통한-action-선택-함수">3.2 network 를 통한 action 선택 함수</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span> 
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="n">epsilon_eval</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> 
        <span class="k">return</span> <span class="n">action</span>
</code></pre></div></div>

<p>다음과 같이 epsilon-greedy 방법을 사용하여 무작위 값 또는 가장 큰 q값을 가진 idx 행동을 선택</p>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\5.png" width="400" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h4 id="33-replay-memory-에-데이터-추가-함수">3.3 replay memory 에 데이터 추가 함수</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
</code></pre></div></div>

<p>replay momory 에 각 상태 값들을 추가</p>

<h4 id="34-network-parameter-학습-시키는-함수">3.4 network parameter 학습 시키는 함수</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span> <span class="c1"># (1)
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (2)
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span> <span class="c1"># (3)
</span>
        <span class="n">eye</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># (4)
</span>        <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span class="n">action</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">long</span><span class="p">()]</span> <span class="c1"># (5) 
</span>
        <span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">*</span> <span class="n">one_hot_action</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># (6)
</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="c1"># (7)
</span>            <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span> <span class="c1"># (8)
</span>            <span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">next_q</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">values</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span><span class="p">)</span> <span class="c1">#(9)
</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span> <span class="c1"># (10)
</span>
        <span class="c1"># model update
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  <span class="c1"># 기울기 초기화
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>             <span class="c1"># 역전파를 통해 gradient 계산
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>       <span class="c1"># model parameter update
</span>        <span class="c1"># eps 감소 (훈련이 진행됨에 따라 무작위 적용 확률를 차츰 줄여나감)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="n">eplsilon_data</span><span class="p">)</span> 

        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
</code></pre></div></div>

<p>(1) batch = random.sample(self.memory, batch_size)<br />
replay memory 에 저장된 값들 중 임의의 값을 가져옴으로써, 가져오는 데이터들 간 상관 관계가 존재하지 않게되어, 과적합 또는 선형 근사가 발생하는것을 방지 한다.</p>

<p>아래 이미지는 하나의 episode 또는 근방에서 훈련된 step 간 동일 색으로 표현하였다. 그림과 같이 근방의 step 끼리 학습을 진행하면 전체 데이터에 대한 근사 함수가 아닌 각각에 근접한 step 에 대한 함수로 각각 근사되게 된다.</p>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\6.png" width="480" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<p>따라서 해당 문제를 막기 위해 한번에 학습되는 step 들을 무작위 분포에서 추출해야 아래와 같이 전체에 대한 근사함수를 얻을 수 있게 된다.</p>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\7.png" width="220" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<p>(2) state = np.stack([b[0] for b  in batch], axis=0) <br />
추출된 batch 값에서 state, action, reward, next_state, done 값들을 각각의 array 값으로 추출한다.</p>

<p>(3) state, action, reward, next_state, done = map(
            lambda x: torch.FloatTensor(x).to(device), [state, action, reward, next_state, done]
        ) <br /></p>

<p>각 상태값들의 array 값들을 실수형 tensor 로 타입 변환 후, device 메모리에 추가</p>

<p>(4) eye = torch.eye(action_size).to(device) <br /></p>

<p>주 대각선 값 1, 나머지 요소는 0인 2차원 배열 (4 * 4 (action_size))을 생성하여 device 메모리에 추가</p>

<p>(5) one_hot_action = eye[action.view(-1).long()] <br /></p>

<p>action [0, 2, 3, 2, 1, 1, …] (0~3) 의 값 * 32 np array 에서 각각의 원소를 long 형 (int64) one_hot type * 32 형 으로 변경 한다. 
즉 [0, 2, 3, 2, 1, 1, …]  -&gt; [[1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 1], ….]</p>

<p>(6) q = (self.network(state) * one_hot_action).sum(1, keepdims=True)<br /></p>

<p>현재 상태에 대한 모델 q값 * one_hot_action = 선택된 action 에 대한 Q(x) 값 이외 값은 0 으로 변환하여 각 q값 ([32.334, 0, 0, 0]) 과 같은 형태에서 ([[\32.334]]) 로변환하여 q에 저장한다. (즉, one_hot data 에서 0 인 부분은 모두 제거 한다.)</p>

<p>(7) with torch.no_grad(): <br /></p>

<p>loss 를 계산하기 위해서 target 값이 필요하지만, target 값에 대한 제어는 별도로 이루어지므로 target 값을 얻는 과정이 network 에 영향을 주어서는 안되므로 with  문 내부에서 torch.no_grad() (gradient 추적이 되지 않는) 상태에서 값을 얻는다.</p>

<p>(8) next_q = self.target_network(next_state) <br /></p>

<p>next_q : target network 을 이용하요 다음상태 s’ 에대한 q 값들을 예측함.</p>

<p>(9) target_q = reward + next_q.max(1, keepdims=True).values * ((1 - done) * discount_factor) <br /></p>

<ul>
  <li>.max(1, keepdims=True).values : (0 차원: batch , 1 차원, action) 에서 action 차원의 최대값을 가져옴</li>
  <li>(1 - done) : done == False 이면 1 True 이면 해당 값은 0 이 됨.</li>
  <li>discount_factor : 감가율을 곱함.</li>
</ul>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\10.png" width="370" /></center>

<p>즉 벨만 방정식 최적 q 를 얻는 공식  target_q = 보상값 + 감가율(다음 상태에서의 최대 Q(x)값) 을 구함 + 혹시 끝난 상태인 경우 0으로 만들어 주는 수식을 결합한 상태</p>

<p>(10) loss = F.smooth_l1_loss(q, target_q) <br /></p>

<p>Huber loss 계산</p>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\8.png" width="220" /></center>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\9.png" width="280" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<h4 id="35-target_network-update-함수">3.5 target_network update 함수</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">update_target</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
</code></pre></div></div>

<ul>
  <li>self.network.state_dict() : 일반 (훈련) 네트워크를 가져옴</li>
  <li>self.target_network.load_state_dict() : 가져운 일반 네트워크를 target 네트워크로 저장</li>
</ul>

<h3 id="4-프로그램-동작-구현-main">4. 프로그램 동작 구현 (main)</h3>

<p>아래 도식의 동작을 구현</p>

<center><img src="assets\img\posts\2024-05-03-DQN_ml_agents\11.png" width="600" /></center>
<p><a href="https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88">이미지 출처</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># 4.1 unity 와 상호 작용이 가능한 UnityEnvironment 인스턴스(env) 생성
</span>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnbironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span> <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="c1"># 4.2 env 로 부터 관측/target 공간 정보, step 진행 후 정보 및 구동 환경(time scale) 설정
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>

    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> 
    <span class="n">agent</span> <span class="o">=</span> <span class="nc">DQNAgent</span><span class="p">()</span>

    <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="c1"># 4.3 반복문을 통해 run_step + test_step 동안 학습을 진행시킴
</span>    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="c1"># 4.3.1 (run_step 마지막 단계에서 모델을 저장하고 test_mode 로 전환)
</span>        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> 

        <span class="c1"># 4.3.2 전처리: 시각적 관측 정보와 목적지 관측 정보를 전처리하여 state 로 저장
</span>        <span class="n">preprocess</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">obs</span><span class="p">,</span> <span class="n">goal</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        
        
        <span class="n">state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>

        <span class="c1"># 4.3.3 agent 를 통해 action 을 결정하고, 해당 action 으로 unity 에서 다음 step 을 진행시킴
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span> 
        <span class="n">real_action</span> <span class="o">=</span> <span class="n">action</span> <span class="o">+</span> <span class="mi">1</span> 
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span> 
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">real_action</span><span class="p">)</span> 
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span> 
        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> 

        <span class="c1"># 4.3.4 진행된 현재 step 정보 가져옴
</span>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> 

        <span class="c1"># 4.3.5 종료(termination) 확인 및 next_step -&gt; next_state 정보로 전처리
</span>        <span class="n">done</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> 
        <span class="n">reward</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span> <span class="k">if</span> <span class="n">done</span> \
            <span class="k">else</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 

        <span class="c1"># 4.3.6 (train mode 일 경우) next_state 를 replay memory 에 저장
</span>        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">])</span>

        <span class="c1"># 4.3.7 충분히 메모리에 state 정보가 차 있다면, 모델 학습으로 손실값을 계산하고, 일정 주기로 target_model 을 update 함.
</span>        <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_start_step</span><span class="p">):</span> 
            <span class="n">loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
            <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">target_update_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">update_target</span><span class="p">()</span>

        <span class="c1"># 4.3.8 episode 종료 시, 필요한 설정값을 조정하고, tensorboard 에 보상/손실 값을 기록, 필요 조건마다 훈련된 모델 저장
</span>        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span> 

            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> /</span><span class="sh">"</span> <span class="o">+</span> \
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Loss: </span><span class="si">{</span><span class="n">mean_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> / Epsilon: </span><span class="si">{</span><span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>            
            
            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="41-unity-와-상호-작용이-가능한-unityenvironment-인스턴스env-생성">4.1 unity 와 상호 작용이 가능한 UnityEnvironment 인스턴스(env) 생성</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnvironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span> <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
</code></pre></div></div>

<p>env (UnityEnvironment 인스턴스)의 역할 및 기능</p>

<ol>
  <li>
    <p>유니티와의 인터페이스: UnityEnvironment는 유니티 엔진과 파이썬 코드 간의 주요 인터페이스. 이 인스턴스를 통해 유니티 게임 환경을 시작, 중지 및 관리할 수 있다.</p>
  </li>
  <li>
    <p>데이터 교환: 유니티 게임 환경에서 생성된 데이터(에이전트의 관측값, 보상 등)를 파이썬으로 전송하고, 파이썬에서 생성한 행동 지시를 유니티로 보내는 역할</p>
  </li>
  <li>
    <p>환경 제어: side_channels을 통해 유니티 환경의 세부적인 설정을 조정할 수 있다. 이를 통해 학습 중 시뮬레이션의 속도를 조절, 테스트 중에는 보다 정밀한 테스트를 수행 가능</p>
  </li>
</ol>

<ul>
  <li>file_name=env_name : 유니티 게임 환경의  실행 파일 경로를 지정</li>
  <li>side_channels=[engine_configuration_channel] : 유니티 환경의 timescale, 해상도, 그래픽 품질 등을 수정할 때 사용</li>
</ul>

<h4 id="42-env-로-부터-관측target-공간-정보-step-진행-후-정보-및-구동-환경time-scale-설정-유니티-브레인-설정">4.2 env 로 부터 관측/target 공간 정보, step 진행 후 정보 및 구동 환경(time scale) 설정 (유니티 브레인 설정)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># 유니티 브레인 설정
</span>    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># env.behavior_specs : 모든 behavior 정보 (예: 관측 공간의 크기, 행동의 유형 및 크기 등)를 가지고 있음. 
</span>    <span class="c1"># 해당 프로젝트에서는 behavior_sepc 중  behavior_name 만 있으면 된다. 해당 spec 은 첫번쩨 요소 이므로 [0] 의 값만 가져온다.
</span>    
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>
    <span class="c1"># behavior_name 에 대한 spec 을 가져오며 관련 정보는 아래와 같다. 
</span>    <span class="c1"># 1. 관측 공간(Obervation Space) : 에이전트가 환경에서 관측할 수 있는 데이터의 형태와 크기를 설명
</span>    <span class="c1">#                                  카메라 이미지, 속도계의 값, 위치 좌표 등
</span>    <span class="c1">#
</span>    <span class="c1"># 2. 행동 공간(Action Space) : 에이전트가 취할 수 있는 행동의 유형과 범위
</span>    <span class="c1">#                             에이전트가 조종할 수 있는 방향, 속도 조절, 점프 등의 행동
</span>
    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span> <span class="c1"># 시간 12 배속 (빠르게) 설정
</span>    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> <span class="c1"># behavior_name 으로 부터  step 정보를 얻음
</span>    <span class="c1"># dec : decision step - decision request step 정보
</span>    <span class="c1"># term : termination step - 에피소드 종료 스텝 정보
</span>
    <span class="c1"># DQNAgent 클래스를 agent 객체 생성
</span>    <span class="n">agent</span> <span class="o">=</span> <span class="nc">DQNAgent</span><span class="p">()</span>

    <span class="c1"># c.0 학습을 진행하기 위해 필요한 정보 초기화
</span>    <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</code></pre></div></div>

<h4 id="43-반복문을-통해-run_step--test_step-동안-학습을-진행시킴">4.3 반복문을 통해 run_step + test_step 동안 학습을 진행시킴</h4>

<h4 id="431-run_step-마지막-단계에서-모델을-저장하고-test_mode-로-전환">4.3.1 (run_step 마지막 단계에서 모델을 저장하고 test_mode 로 전환)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
    <span class="c1"># run_step : 학습 모드 step
</span>    <span class="c1"># test_step: 테스트모드 step
</span>
        <span class="c1"># test step 진행 코드
</span>        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># test_step 은 정속으로 수행
</span>
</code></pre></div></div>

<h4 id="432-전처리-시각적-관측-정보와-목적지-관측-정보를-전처리하여-state-로-저장">4.3.2 전처리: 시각적 관측 정보와 목적지 관측 정보를 전처리하여 state 로 저장</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1">#  전처리 : 시각적 관측 정보와 목적지 관측 정보를 전처리하여 state 에 저장
</span>        <span class="n">preprocess</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">obs</span><span class="p">,</span> <span class="n">goal</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        
        <span class="c1"># a. obs*goal[0][0] : agent 관즉 이미지 * goal[0][0] (goal_plus 이면 1, goal_ex 이면 0)
</span>        <span class="c1"># b. obs*goal[0][1] : agent 관측 이미지 * goal[0][1] (goal_plus 이면 0, goal_ex 이면 1)
</span>        <span class="c1"># 위 값을 concatenate
</span>        <span class="c1">#  -&gt;  6 채널 중  goal_plus 의 경우 전반부 3 채널에 대해 값이 채워지고, 나머지 3채널에 대한 값은 모두 0 으로 처리
</span>        <span class="c1">#  -&gt;  6 채널 중  goal_ex   의 경우 후반부 3 채널에 대해 값이 채워지고, 나머지 3채널에 대한 값은 모두 0 으로 처리
</span>        
        <span class="n">state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>
        <span class="c1"># dec.obs : 지정된 behavior_name 을 가진 모든 agent 에 대한 모든 관측을 포함하는 튜플
</span>        <span class="c1"># dec.obs[0] (OBS = 0 시각적 관측 idx) : 로 시각적 관측 정보를 얻을 수 있음.
</span>        <span class="c1"># dec.obs[1] (GOAL_OBS = 1 (시각적) 목적지 관측 idx) : 로 목적지의 시각적 정보를 얻음.
</span>        <span class="c1"># dec.obs[GOAL_OBS] 에서  goal_plus : [[1., 0]] / goal_ex: [[0., 1.]]
</span>
</code></pre></div></div>

<h4 id="433-agent-를-통해-action-을-결정하고-해당-action-으로-unity-에서-다음-step-을-진행시킴">4.3.3 agent 를 통해 action 을 결정하고, 해당 action 으로 unity 에서 다음 step 을 진행시킴</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span> <span class="c1"># get_action 을 통해 현재 state 의 eps-greedy 행동 선택
</span>        <span class="n">real_action</span> <span class="o">=</span> <span class="n">action</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># unity 에서 0 은 정지 action 을 의미하게 되므로 , 0 ~3  -&gt; 1 ~ 4 로 +1
</span>        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span> 
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">real_action</span><span class="p">)</span> <span class="c1"># 신경망을 통해 결정된 action 값을 동작 값으로 저장 
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span> <span class="c1"># action 을 unity 환경에 전달
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># unity 에서 시뮬레이션 step 진행
</span></code></pre></div></div>

<h4 id="434-진행된-현재-step-정보-가져옴">4.3.4 진행된 현재 step 정보 가져옴</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> <span class="c1"># 진행한 현재 스텝 정보 가져오기
</span></code></pre></div></div>

<h4 id="435-종료termination-확인-및-next_step---next_state-정보로-전처리">4.3.5 종료(termination) 확인 및 next_step -&gt; next_state 정보로 전처리</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">done</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="c1"># 현재 시뮬레이션은 agent 가 1개 이므로 termination agent_id 가 존재하면  종료되었음을 바로 확인 가능 
</span>        <span class="n">reward</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span> <span class="k">if</span> <span class="n">done</span> \
            <span class="k">else</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># step 보상 누적
</span></code></pre></div></div>

<h4 id="436-train-mode-일-경우-next_state-를-replay-memory-에-저장">4.3.6 (train mode 일 경우) next_state 를 replay memory 에 저장</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># replay memory 에  data 저장 (학습 모드)
</span>        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="437-충분히-메모리에-state-정보가-차-있다면-모델-학습으로-손실값을-계산하고-일정-주기로-target_model-을-update-함">4.3.7 충분히 메모리에 state 정보가 차 있다면, 모델 학습으로 손실값을 계산하고, 일정 주기로 target_model 을 update 함.</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_start_step</span><span class="p">):</span> <span class="c1"># 충분한 학습 데이터가 모였다면 (최소 batch_size 이상)
</span>            <span class="c1"># 학습수행
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
            <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># 타겟 네트워크 업데이트 (특정 수의 step 타이밍 마다)
</span>            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">target_update_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">update_target</span><span class="p">()</span>

</code></pre></div></div>

<h4 id="438-episode-종료-시-필요한-설정값을-조정하고-tensorboard-에-보상손실-값을-기록-필요-조건마다-훈련된-모델-저장">4.3.8 episode 종료 시, 필요한 설정값을 조정하고, tensorboard 에 보상/손실 값을 기록, 필요 조건마다 훈련된 모델 저장</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># episode 완료 시,
</span>        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># 초기화
</span>
            <span class="c1"># 게임 진행 상황 출력 및 텐서 보드에 보상과 손실 함수 값 기록
</span>            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> /</span><span class="sh">"</span> <span class="o">+</span> \
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Loss: </span><span class="si">{</span><span class="n">mean_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> / Epsilon: </span><span class="si">{</span><span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>            
            

                
            <span class="c1"># 네트워크 모델 저장
</span>            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
                
    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="최종-전체-코드">최종 전체 코드</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="n">platform</span> <span class="c1"># system (OS) 관련
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="n">mlagents_envs.environment</span> <span class="kn">import</span> <span class="n">UnityEnvironment</span><span class="p">,</span> <span class="n">ActionTuple</span> 
<span class="kn">from</span> <span class="n">mlagents_envs.side_channel.engine_configuration_channel</span> <span class="kn">import</span> <span class="n">EngineConfigurationChannel</span> 

<span class="n">state_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">84</span><span class="p">]</span> 
<span class="n">action_size</span> <span class="o">=</span> <span class="mi">4</span> 

<span class="n">load_model</span> <span class="o">=</span> <span class="bp">False</span> 
<span class="n">train_mode</span> <span class="o">=</span> <span class="bp">True</span>  

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">mem_maxlen</span> <span class="o">=</span> <span class="mi">10000</span>  
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00025</span> 

<span class="n">run_step</span> <span class="o">=</span> <span class="mi">50000</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mi">0</span> 
<span class="n">test_step</span> <span class="o">=</span> <span class="mi">5000</span>       
<span class="n">train_start_step</span> <span class="o">=</span> <span class="mi">5000</span> 
<span class="n">target_update_step</span> <span class="o">=</span> <span class="mi">500</span> 

<span class="n">print_interval</span> <span class="o">=</span> <span class="mi">10</span>     
<span class="n">save_interval</span> <span class="o">=</span> <span class="mi">100</span>     

<span class="n">epsilon_eval</span> <span class="o">=</span> <span class="mf">0.05</span>    
<span class="n">epsilon_init</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="n">epsilon_eval</span> 
<span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.1</span>      
<span class="n">explore_step</span> <span class="o">=</span> <span class="n">run_step</span> <span class="o">*</span> <span class="mf">0.8</span> 
<span class="n">eplsilon_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">epsilon_init</span> <span class="o">-</span> <span class="n">epsilon_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">explore_step</span> <span class="k">if</span> <span class="n">train_mode</span> <span class="k">else</span> <span class="mf">0.05</span>

<span class="n">VISUAL_OBS</span> <span class="o">=</span> <span class="mi">0</span>  
<span class="n">GOAL_OBS</span> <span class="o">=</span> <span class="mi">1</span>   
<span class="n">VECTOR_OBS</span> <span class="o">=</span> <span class="mi">2</span>  
<span class="n">OBS</span> <span class="o">=</span> <span class="n">VISUAL_OBS</span> 

<span class="n">game</span> <span class="o">=</span> <span class="sh">"</span><span class="s">GridWorld</span><span class="sh">"</span>          
<span class="n">os_name</span> <span class="o">=</span> <span class="n">platform</span><span class="p">.</span><span class="nf">system</span><span class="p">()</span> 
<span class="k">if</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Windows</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="sh">"</span> 
<span class="k">elif</span> <span class="n">os_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Darwin</span><span class="sh">'</span><span class="p">:</span> 
    <span class="n">env_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">../envs/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">os_name</span><span class="si">}</span><span class="sh">"</span>

<span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y%m%d%H%M%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DQN/</span><span class="si">{</span><span class="n">date_time</span><span class="si">}</span><span class="sh">"</span> 
<span class="n">load_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./saved_models/</span><span class="si">{</span><span class="n">game</span><span class="si">}</span><span class="s">/DQN/20240503201212</span><span class="sh">"</span> 

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span> 

<span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DQN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">state_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span>
            <span class="p">)</span>
        <span class="n">dim1</span> <span class="o">=</span> <span class="p">((</span><span class="n">state_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">8</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">state_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">8</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span>
            <span class="p">)</span>
        <span class="n">dim2</span> <span class="o">=</span> <span class="p">((</span><span class="n">dim1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">dim1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="n">dim3</span> <span class="o">=</span> <span class="p">((</span><span class="n">dim2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">//</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">dim2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">//</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="o">*</span><span class="n">dim3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">dim3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">flat</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">network</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">mem_maxlen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_init</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span> <span class="o">=</span> <span class="nc">SummaryWriter</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">load_model</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span> 
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Load Model from </span><span class="si">{</span><span class="n">load_path</span><span class="si">}</span><span class="s">/ckpt</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">load_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> 
            <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">network</span><span class="sh">"</span><span class="p">])</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

        <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span> 
        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="n">epsilon_eval</span>

        <span class="k">if</span> <span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">():</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> 
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> 
        <span class="k">return</span> <span class="n">action</span>
        
    <span class="k">def</span> <span class="nf">append_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">b</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span>  <span class="ow">in</span> <span class="n">batch</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">eye</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">one_hot_action</span> <span class="o">=</span> <span class="n">eye</span><span class="p">[</span><span class="n">action</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">long</span><span class="p">()]</span> 

        <span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">*</span> <span class="n">one_hot_action</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">next_q</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">values</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">discount_factor</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span>
 
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>  
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>             
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>      
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="n">eplsilon_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">update_target</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">... Save Model to </span><span class="si">{</span><span class="n">save_path</span><span class="si">}</span><span class="s">/ckpt ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">network</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>   
            <span class="sh">"</span><span class="s">optimizer</span><span class="sh">"</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
        <span class="p">},</span> <span class="n">save_path</span><span class="o">+</span><span class="sh">'</span><span class="s">/ckpt</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">write_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">run/score</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sh">"</span><span class="s">model/epsilon</span><span class="sh">"</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">engine_configuration_channel</span> <span class="o">=</span> <span class="nc">EngineConfigurationChannel</span><span class="p">()</span>
    <span class="n">env</span> <span class="o">=</span> <span class="nc">UnityEnbironment</span><span class="p">(</span>
        <span class="n">file_name</span><span class="o">=</span><span class="n">env_name</span><span class="p">,</span> <span class="n">side_channels</span><span class="o">=</span><span class="p">[</span><span class="n">engine_configuration_channel</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>


    <span class="n">behavior_name</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">.</span><span class="nf">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">behavior_specs</span><span class="p">[</span><span class="n">behavior_name</span><span class="p">]</span>

    <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">12.0</span><span class="p">)</span>
    <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> 
    <span class="n">agent</span> <span class="o">=</span> <span class="nc">DQNAgent</span><span class="p">()</span>

    <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">episode</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">run_step</span> <span class="o">+</span> <span class="n">test_step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="n">run_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TEST START</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">train_mode</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="n">engine_configuration_channel</span><span class="p">.</span><span class="nf">set_configuration_parameters</span><span class="p">(</span><span class="n">time_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> 

        <span class="n">preprocess</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">obs</span><span class="p">,</span> <span class="n">goal</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">obs</span><span class="o">*</span><span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        
        
        <span class="n">state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">train_mode</span><span class="p">)</span> 
        <span class="n">real_action</span> <span class="o">=</span> <span class="n">action</span> <span class="o">+</span> <span class="mi">1</span> 
        <span class="n">action_tuple</span> <span class="o">=</span> <span class="nc">ActionTuple</span><span class="p">()</span> 
        <span class="n">action_tuple</span><span class="p">.</span><span class="nf">add_discrete</span><span class="p">(</span><span class="n">real_action</span><span class="p">)</span> 
        <span class="n">env</span><span class="p">.</span><span class="nf">set_actions</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">,</span> <span class="n">action_tuple</span><span class="p">)</span> 
        <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> 

        <span class="n">dec</span><span class="p">,</span> <span class="n">term</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">get_steps</span><span class="p">(</span><span class="n">behavior_name</span><span class="p">)</span> 
        <span class="n">done</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">agent_id</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> 
        <span class="n">reward</span> <span class="o">=</span> <span class="n">term</span><span class="p">.</span><span class="n">reward</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">dec</span><span class="p">.</span><span class="n">reward</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">term</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span> <span class="k">if</span> <span class="n">done</span> \
            <span class="k">else</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">OBS</span><span class="p">],</span> <span class="n">dec</span><span class="p">.</span><span class="n">obs</span><span class="p">[</span><span class="n">GOAL_OBS</span><span class="p">])</span>
        <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">append_sample</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">done</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_start_step</span><span class="p">):</span> 
            <span class="n">loss</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">train_model</span><span class="p">()</span>
            <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">target_update_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">update_target</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span> 

            <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">print_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mean_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
                <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">write_summary</span><span class="p">(</span><span class="n">mean_score</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="n">losses</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> Episode / Step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s"> / Score: </span><span class="si">{</span><span class="n">mean_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> /</span><span class="sh">"</span> <span class="o">+</span> \
                      <span class="sa">f</span><span class="sh">"</span><span class="s">Loss: </span><span class="si">{</span><span class="n">mean_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> / Epsilon: </span><span class="si">{</span><span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>            
            
            <span class="k">if</span> <span class="n">train_mode</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>

    <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="cuda" /><category term="pytorch" /><category term="unity" /><category term="dqn" /><category term="ml-agents" /><summary type="html"><![CDATA[Deep Q-Network + ML-agents 구현]]></summary></entry><entry><title type="html">(Windows) CUDA 사용 tensorflow 작업 환경 설정</title><link href="http://localhost:4000/tensorflow_gpu_setting.html" rel="alternate" type="text/html" title="(Windows) CUDA 사용 tensorflow 작업 환경 설정" /><published>2024-04-08T09:32:20+09:00</published><updated>2024-04-08T09:32:20+09:00</updated><id>http://localhost:4000/tensorflow_gpu_setting</id><content type="html" xml:base="http://localhost:4000/tensorflow_gpu_setting.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<p>tensflow 기반 딥러닝 학습을 하는데 cpu 기반 작업이 너무 느려서 GPU 를 사용하고자 하였으나 최신 자료는 없는것 같아 이를 다시 정리해 보았다. 다만 윈도우 환경에서 지원되는 CUDA toolkit 버젼은 그리 변화가 있지는 않았다.</p>

<ul>
  <li>사용환경</li>
  <li>OS: Windows 10  64bits</li>
  <li>GPU : RTX 3070</li>
  <li>GPU driver : Nvidia Graphic Driver 537.13</li>
  <li>CUDA Toolkit : 11.2</li>
  <li>Visual Studio : 2019</li>
  <li>cudnn : 8.2.1 (for cuda toolkit 11.x)</li>
  <li>python : 3.10</li>
  <li>Anacoda 가상환경 - jupyter notebook</li>
  <li>tensorflow-gpy : 2.10</li>
</ul>

<p>2024년 4월 기준, 아래 페이지에서 다음과 같이 윈도우 환경에서 Nvidia GPU 사용 환경 조건을 확인 할 수 있다. 
<a href="https://www.tensorflow.org/install/source_windows?hl=en">링크 : Tensorflow Build from source on Windows </a></p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting.png" width="600" /></center>
<p><br /></p>

<h4 id="1nvidia-graphic-driver-설치-확인-및-전체-환경-설정-조건-확인">1.Nvidia Graphic Driver 설치 확인 및 전체 환경 설정 조건 확인</h4>

<p>아래와 같이 우선 cmd 에서  ‘nvidia-smi’명령어를 실행하여 그래픽 드라이버라 정상적으로 설치되었는지 확인하며, 이 때 ‘CUDA Version’ 을 확인한다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_settingnvidiasmi.png" width="600" /></center>

<p>만약 본인 GPU CUDA 버전과 Tensorflow 가이드에 명시된 버젼이 맞다면 해당 버젼에 맞게 모든 설정을 조정하면 된다. 그러나 내 경우와 같이 아직 해당 버젼이 지원하지 않는 경우, 낮춰 모든 설정을 적용해야 한다. 내 경우는 CUDA 11.2 로 적용하였다.</p>

<h4 id="2-cuda-tookit--cudnn-설치">2. CUDA Tookit &amp; cuDNN 설치</h4>

<p>아래 링크에서  CUDA Tookit 을 받아 설치한다. 
<a href="https://developer.nvidia.com/cuda-11.2.0-download-archive">CUDA Tookit 11.2 Downlaods</a></p>

<p>상위 버전의 경우, 필요한 경우 함께  visual studio 를 설치지만, 해당 버젼은 별도로 설치 해주어야 한다. 해당 버전에 맞는 visual studio 는 2019 로 아래 링크에서 다운로드 받아 추가 설치해야 한다. 
<a href="https://visualstudio.microsoft.com/ko/vs/older-downloads/">Visual Studio 2019 Download</a></p>

<p>그리고 아래 링크에서 본인에게 맞는 버전의 cuDNN 을 다운 받는다. 
<a href="https://developer.nvidia.com/rdp/cudnn-archive">cuDNN download</a></p>

<p>내 경우, 아래 버젼으로 받아 설치하였다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting_cudnn.png" width="600" /></center>

<p>해당 파일을 받아 압축을 풀고 내부 파일들을 모두 복사하여, CUDA Toolkit 이 설치된 폴더 ( C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2)에 붙여 넣는다.</p>

<h4 id="3-환경변수-설정">3. 환경변수 설정</h4>

<p>앞에서 설치한 CUDA Toolkit 이 정상적으로 설치되었는지 확인과 함께 환경 변수 경로를 추가 해준다.
작업표시줄 검색창에서 “환경 변수” 를 검색하여 시스템 환경 변수 편집을 찾아 들어간다. 환경변수 버튼을 눌러서 들어가면 아래와 같이 창이 나오는데 여기에서 시스템 변수로 ‘CUDA_PATH’ 와 ‘CUDA_PATH_해당버전’ 이 포함되어 있어야 한다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting_env1.png" width="400" /></center>

<p>(다음 작업은 필요한지 잘 모르겠다.)
추가로 사용자 변수 - Path 에 아래와 같이 CUDA Toolkit  설치 경로에 bin, include, lib 폴더 경로를 추가한다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting_env2.png" width="500" /></center>

<h4 id="4-아나콘다-내-가상환경-설정-및-tensorflow-gpu-설치">4. 아나콘다 내 가상환경 설정 및 tensorflow-gpu 설치</h4>

<p>아나콘다를 최신 버전으로 설치하고, Anaconda Prompt 를 실행한다. 
여기서 가상환경을 추가할 때 phython 버젼을 앞에서 확인한 tensorflow-gpu 지원 버전을 설치한다. 내 경우는 3.10 으로 설치하여 진행하였다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> <span class="o">[</span>가상환경 이름] <span class="nv">python</span><span class="o">=</span>3.10
</code></pre></div></div>

<p>이후 해당 가상환경을 활성화 한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate <span class="o">[</span>가상환경이름]
</code></pre></div></div>

<p>그리고 tensorflow-gpu 를 홈페이지에서 확인한 버전으로 설치해 준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>tensorflow-gpu<span class="o">==</span>2.10
</code></pre></div></div>

<p>기타 필요한 라이브러리들을 설치하고, jupyter notebook 을 실행하기 위한 notebook 을 설치하고 jupyter notebook 을 실행한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>notebook
...
jupyter notebook
</code></pre></div></div>

<h4 id="5-jupyter-에서-tensorflow-gpu-정상-작동-확인">5. jupyter 에서 tensorflow-gpu 정상 작동 확인</h4>

<p>jupyter 커널에서 새 notebook 을 실행하여 아래 code 를 입력하여 gpu 가 정상적으로 연결되었는지를 확인한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from tensorflow.python.client import device_lib
print<span class="o">(</span>device_lib.list_local_devices<span class="o">())</span>
</code></pre></div></div>

<p>코드를 실행하면 아래와 같이 출력되는데, 만약 GPU 연결이 되지 않았다면, CPU 정보만 출력되고, GPU 관련 정보는 출력되지 않는다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting_ju.png" width="600" /></center>

<p>만약 여기에서 CPU 만 잡힌 상태라면, 앞의 과정에서 무언가 잘못되었다는 의미이므로 다시 작업해야 한다. (개인적으로도 해당 설치까지 여러번 반복하였으며, 타 블로그 글에도 수차례 실패했다는 글이 많다.)</p>

<p>최종적으로 성공했다면, 이제 GPU 실행 환경으로 설정하고 사용하면 된다. 우측 상단에 ‘Python 3(ipkernel)’ 로 되어 있다면 클릭해서 본인이 생성한 가상환겅 (ex. b2404) 로 선택하여 사용하면 된다.</p>

<center><img src="assets\img\posts\2024-04-08-tensorflow_gpu_setting_ju1.png" width="600" /></center>]]></content><author><name>Yong gon Yun</name></author><category term="cuda" /><category term="cudnn" /><category term="tensorflow" /><summary type="html"><![CDATA[(Windows 10) CUDA 사용 tensorflow 작업 환경 설정]]></summary></entry><entry><title type="html">IPC 실습 05 - Named Semaphore</title><link href="http://localhost:4000/service-daemon.html" rel="alternate" type="text/html" title="IPC 실습 05 - Named Semaphore" /><published>2024-03-24T10:32:20+09:00</published><updated>2024-03-24T10:32:20+09:00</updated><id>http://localhost:4000/service-daemon</id><content type="html" xml:base="http://localhost:4000/service-daemon.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<p>아래 내용은 개발자를 위한 시스템 반도체 SW개발 기초(디바이스 드라이버 개발) (https://comento.kr/) 강의 내용 중 일부에 해당함.</p>

<h3 id="1-systemd-란">1. Systemd 란?</h3>

<p>커널이 가장 처음 띄우는 init 프로세스 (PID : 1)</p>
<ul>
  <li>
    <p>시스템을 초기화 하여 여려가지 서비스 데몬들을 띄우고 관리함. 예를 들어, 디바이스 노드를 자동을 생성하는 udev 데몬도 systemd 서비스로 구동됨.</p>
  </li>
  <li>
    <p>journalctl 을 사용하여 각 데몬의 로그도 저장 관리함.</p>
  </li>
  <li>
    <p>각 서비스 간의 의존성을 자동으로 관리</p>
  </li>
</ul>

<h4 id="기본-systemd-서비스-관리-명령어">기본 Systemd 서비스 관리 명령어</h4>

<ul>
  <li>systemctl status <서비스 이름=""> : 해당 서비스의 상태 조회</서비스></li>
  <li>systemclt enable/disable <서비스 이름=""> : 해당 서비스 활성화/비활성화</서비스></li>
  <li>systemclt start/stop <서비스 이름=""> : 해당 서비스 시작/종료</서비스></li>
  <li>
    <p>systemclt daemon-reload : 어떤 서비스를 수정하거나 추가했을 때 해당 명령를 통해 변경사항을 적용해줘양 함.</p>
  </li>
  <li>systemctl list-unit : 모든 서비스 목록 출력</li>
  <li>systemctl list-sockets : 서비스가 사요아는 소켓의 목록을 출력</li>
  <li>
    <p>systemctl list-dependencies : 서비스를 tree 구조로 의존성을 표시하여 출력</p>
  </li>
  <li>journalctl -fn : 모든 systemd 서비스 데몬들의 로그 출력</li>
  <li>journalctl -fn -u <서비스이름>.service : 모든 systemd 서비스 데몬들의 로그 출력</서비스이름></li>
</ul>

<h3 id="2-daemon-이란">2. Daemon 이란?</h3>

<p>여러 요청에 따라서 서비스를 제공하기 위해 백그라운드로 길게 떠 있는 프로세스</p>

<p>사용자가 직접 제어하지 않으며 init 시스템 등이 데몬을 관리하게 됨.</p>
<ul>
  <li>부모의 PID 는 1 (systemd) 이며 세션 및 그룩 아이디는 본인 자신이어야 함 - daemonize 과정 필요
    <ul>
      <li>init 프로세스에 입양시킨다고 표현되기도</li>
    </ul>
  </li>
</ul>

<h4 id="daemonnochdir-noclose">daemon(nochdir, noclose)</h4>

<p>데몬 프로세스를 생성 함수.</p>

<ul>
  <li>
    <p>nochdir   : 0 이면, daemon 함수는 루트 디렉토리(/)로 현재 작업 디렉토리를 변경, 1 이면 현재 디렉토리를 변경하지 않고 진행. 일반적으로 0 의 설정값을 가지며 그 이유는 데몬 프로세스가 파일 시스템을 마운트 해제하는데 방해가 되지 않도록 하기 위함이다.</p>
  </li>
  <li>
    <p>noclose   : 0이면, daemon 함수는 표준 입력(stdin), 표준 출력(stdout), 그리고 표준 에러(stderr)를 /dev/null로 리다이렉트. 만약 1이라면, 이러한 파일 디스크립터들을 리다이렉트하지 않음. 일반적으로 1의 값으로 설정되며 그 이유는 데몬 프로세스가 터미널과의 연결을 끊고, 백그라운드에서 조용히 실행되게 하고자함이다.</p>
  </li>
</ul>

<h4 id="데몬-생성-과정">데몬 생성 과정</h4>

<ol>
  <li>
    <p>프로세스 분기: 부모 프로세스를 종료하고 자식 프로세스를 백그라운드에서 실행하여 세션 리더가 되게 한다.</p>
  </li>
  <li>
    <p>세션 생성: 새로운 세션을 생성하여 프로세스 그룹 리더가 된다.</p>
  </li>
  <li>
    <p>작업 디렉토리 변경: 파일 시스템의 마운트 해제를 방지하기 위해 작업 디렉토리를 루트(/)로 변경(nochdir 0  인 경우).</p>
  </li>
  <li>
    <p>파일 모드 마스크 초기화: 새로 생성되는 파일과 디렉토리의 권한을 제어.</p>
  </li>
  <li>
    <p>표준 입출력 리다이렉트: 데몬 프로세스가 터미널과의 입출력 연결을 끊음(noclose 1 인 경우).</p>
  </li>
</ol>

<h3 id="3-서비스-데몬-개발">3. 서비스 데몬 개발</h3>

<p>구현하고자 하는 서비스 데몬은  systemd socket 활셩화하는 것이다.</p>

<p>구현 동작</p>

<ol>
  <li>systemd 소켓을 열고 listen 상태로 둠.</li>
  <li>소켓에 접속 요청이 들어오면, systemd 는 서비스 데몬을 실행</li>
  <li>서비스 데몬은 실행되었을 때, accept 부터 처리하게 됨.</li>
  <li>서비스 데몬이 더이상 처리할 것이 없으면 종료함.</li>
  <li>요청이 들어올 때마다 2~4번 작업을 반복함.</li>
</ol>

<p>구현 서비스 데몬 관련 내용 (이해가 안감…;;)</p>
<ul>
  <li>소켓으로의 접속이 없을 경우는 서비스 데몬을 열지 않아 메모리 절약 가능</li>
  <li>Accept = yse 까지 사용하면 accpet 도 systemd 가 해주나 성능상 이유로 추천되지는 않음.</li>
</ul>

<h3 id="4-코드-작성">4. 코드 작성</h3>

<p>1.1. 루트 파일 시스템의 /usr/lib/systemd/system 밑에 <서비스>.socket 파일 생성</서비스></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>Unit]
<span class="nv">Description</span><span class="o">=</span>socket <span class="k">for </span>Comento Example
service

<span class="o">[</span>Socket]
<span class="nv">ListenStream</span><span class="o">=</span>/run/comento.sock

<span class="o">[</span>Install]
<span class="nv">WantedBy</span><span class="o">=</span>sockets.target
</code></pre></div></div>

<p>1.2. systemctl enable <서비스>.socket : 해당 소켓 켜기</서비스></p>
<ul>
  <li>소켓을 켜게 되면 ls -l /run/comento.sock 파일 생성</li>
</ul>

<p>2.1 루트파일 시스템의 /usr/lib/systemd/system 밑에 <서비스>.service 파일 생성</서비스></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>Unit]
<span class="nv">Description</span><span class="o">=</span>Comento Example service

<span class="o">[</span>Service]
<span class="nv">Type</span><span class="o">=</span>forking
<span class="nv">ExecStart</span><span class="o">=</span>/usr/bin/comento-daemon
<span class="nv">StandardOutput</span><span class="o">=</span>journal
<span class="nv">Restart</span><span class="o">=</span>on-failure
<span class="nv">StartLimitIntervalSec</span><span class="o">=</span>1s
<span class="nv">StartLimitBurst</span><span class="o">=</span>32

<span class="o">[</span>Install]
<span class="nv">WantedBy</span><span class="o">=</span>basic.target
</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="linux" /><category term="IPC" /><category term="daemon" /><category term="shared systemd" /><category term="socket" /><summary type="html"><![CDATA[개발자를 위한 반도체 SW개발 기초 (디바이스 드라이버 개발) 관련 학습 20]]></summary></entry><entry><title type="html">IPC 실습 05 - Named Semaphore</title><link href="http://localhost:4000/IPC05.html" rel="alternate" type="text/html" title="IPC 실습 05 - Named Semaphore" /><published>2024-03-22T14:32:20+09:00</published><updated>2024-03-22T14:32:20+09:00</updated><id>http://localhost:4000/IPC05</id><content type="html" xml:base="http://localhost:4000/IPC05.html"><![CDATA[<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

<p>아래 내용은 개발자를 위한 시스템 반도체 SW개발 기초(디바이스 드라이버 개발) (https://comento.kr/) 강의 내용 중 일부에 해당함.</p>

<h3 id="1-semaphore">1. Semaphore</h3>

<p>동시에 자원에 접근할 수 있는 스레드의 수를 제한하는 메커니즘. 일종의 카운터로, 특정 자원을 접근하려는 스레드의 수를 제어한다. 세마포어는 주로 두 가지 유형으로 나뉜다:</p>

<ul>
  <li>
    <p>이진 세마포어(Binary Semaphore): 값이 0 또는 1만 될 수 있으며, 이는 뮤텍스와 유사한 방식, 자원에 대한 독점적 접근을 제어하는데 사용됨.</p>
  </li>
  <li>
    <p>카운팅 세마포어(Counting Semaphore): 값이 정해진 범위 내에서 증가하거나 감소할 수 있으며, 동시에 여러 스레드가 자원에 접근할 수 있게 함.</p>
  </li>
</ul>

<p>(카운팅) 세마포어의 동작 과정</p>

<ol>
  <li>
    <p>읽기 작업 시작: 스레드가 읽기 작업을 시작하기 전, 쓰기 세마포어의 상태를 확인하여 쓰기 작업이 진행 중이지 않은지 확인한다. 그 후 읽기 세마포어의 카운트를 증가시켜 읽기 작업을 수행 중인 스레드의 수를 저장.</p>
  </li>
  <li>
    <p>읽기 작업 종료: 스레드가 읽기 작업을 마치면, 읽기 세마포어의 카운트를 감소시켜 읽기 작업을 수행 중인 스레드의 수를 감소시킨다.</p>
  </li>
  <li>
    <p>쓰기 작업 시작: 스레드가 쓰기 작업을 시작하기 전, 먼저 읽기 작업이 진행 중인지 확인하기 위해 읽기 세마포어의 카운트가 0이 될 때까지 기다린다. 이는 모든 읽기 작업이 완료됨을 의미합니다. 그 다음, 쓰기 세마포어를 잠금 상태로 전환하여 쓰기 작업을 시작한다.</p>
  </li>
  <li>
    <p>쓰기 작업 종료: 쓰기 작업을 마친 후, 쓰기 세마포어를 해제하여 다른 스레드가 읽기 또는 쓰기 작업을 시작할 수 있도록 한다.</p>
  </li>
</ol>

<p>위와 같은 방법을 통해 데이터를 쓰기 작업과 읽기 작업의 충돌을 예방할 수 있다. 다만 데이터의 일관성 (data consistency) 의 경우, 위와 같은 기본적인 세마포어로는 보장될 수 없다.</p>

<p>예를 들어 기존 데이터가 0 일때, 하나의 스레드에서  해당 값을 읽어서 +1 을 하고 나서 쓰고, 그 결과 (1)을 다른 스레드가 읽어서 -1 하여 0최종 0의 결과 값을 의도 했을 때, 단순히 읽는 과정이 마무리되었을 때 쓰기를 하는 형태로 구현한다면, 위의 과정에서 최종 결과가 0 일 수도 있고 1 이거나 -1 일 수도 있는 불확정한 상황이 발생할 수 있다.</p>

<p>이런 과정은 해당 작업 전체 (읽기-연산-쓰기) 과정에 lock 을 사용하여, 모든 작업이 작업의 절차가 분리되지 않고 하나의 덩어리르 처리되는 원자성이 보장되도록 처리하거나 하는 등의 방식을 사용하여 처리하여야 일관성을 유지할 수 있다.</p>

<h3 id="2-주요-함수">2. 주요 함수</h3>

<h4 id="sem_t-sem_openconst-char-name-int-oflag-mode_t-mode-unsigned-int-value">sem_t *sem_open(const char *name, int oflag, mode_t mode, unsigned int value);</h4>

<p>POSIX 세마포어를 생성하거나 열기 위한 함수</p>

<ul>
  <li>name  : 세마포어의 이름</li>
  <li>oflag : O_CREAT (세마포어가 존재하지 않을 경우 생성), O_EXCL (함께 O_CREAT와 사용되며, 세마포어가 이미 존재할 경우 실패), 등</li>
  <li>mode  : 세마포어에 대한 접근 권한을 설정 (e.g. 0600)</li>
  <li>valeu : 세마포어의 초기값. 이 값은 세마포어가 생성될 때만 의미가 있으며, 세마포어가 동시에 허용할 수 있는 최대 리소스 접근 수를 나타낸다. 예를 들어, 1로 설정할 경우, 해당 세마포어는 뮤텍스(mutex)와 유사하게 동작한다.</li>
</ul>

<h4 id="int-sem_waitsem_t-sem">int sem_wait(sem_t *sem);</h4>

<p>세마포어의 제어권 획득</p>

<ul>
  <li>sem   : 작업할 세마포어 객체에 대한 포인터</li>
</ul>

<h4 id="int-sem_postsem_t-sem">int sem_post(sem_t *sem);</h4>

<p>세마포어의 제어권 내려 놓음</p>

<ul>
  <li>sem   : 작업할 세마포어 객체에 대한 포인터</li>
</ul>

<h3 id="3-code">3. code</h3>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;fcntl.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdlib.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;string.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;sys/mman.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;sys/stat.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;sys/wait.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;semaphore.h&gt;</span><span class="cp">
</span>
<span class="cp">#define SEM_NAME "comento"
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">ptr</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sem_init</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> 
    <span class="n">sem_t</span> <span class="o">*</span><span class="n">sem</span><span class="p">;</span>
    <span class="n">pid_t</span> <span class="n">pid</span><span class="p">;</span>

    <span class="c1">// 동시 접근 가능 스레드 : 2 </span>
    <span class="c1">// -&gt; 두 스레드가 동시에 제어권을 가지므로써 데이터 일관성을 유지할 수 없는 상태로 만듬.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">argc</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">strcmp</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">"-no-sem"</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">sem_init</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">sem_unlink</span><span class="p">(</span><span class="n">SEM_NAME</span><span class="p">);</span> <span class="c1">// Remove semaphore if exists</span>
    <span class="n">sem</span> <span class="o">=</span> <span class="n">sem_open</span><span class="p">(</span><span class="n">SEM_NAME</span><span class="p">,</span> <span class="n">O_CREAT</span> <span class="o">|</span> <span class="n">O_EXCL</span><span class="p">,</span> <span class="mo">0600</span><span class="p">,</span> <span class="n">sem_init</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">sem</span> <span class="o">==</span> <span class="n">SEM_FAILED</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">"Failed to create semaphore</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">"Mapping an anonymous share memory</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>

    <span class="c1">// 익명의 공유 메모리 </span>
    <span class="c1">// 프로그램 내에서 바로 해당 메모리 포인터를 받아 부모-자식 간 사용할 것이므로 이름 없이 사용 가능</span>
    <span class="n">ptr</span> <span class="o">=</span> <span class="n">mmap</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="n">ptr</span><span class="p">),</span> <span class="n">PROT_READ</span> <span class="o">|</span> <span class="n">PROT_WRITE</span><span class="p">,</span> <span class="n">MAP_SHARED</span> <span class="o">|</span> <span class="n">MAP_ANONYMOUS</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">fork</span><span class="p">();</span>

    <span class="c1">// 자식 프로세스는 1씩 10000 번 더하기 수행</span>
    <span class="k">if</span><span class="p">(</span><span class="n">pid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sem_wait</span><span class="p">(</span><span class="n">sem</span><span class="p">);</span>
            <span class="p">(</span><span class="o">*</span><span class="n">ptr</span><span class="p">)</span><span class="o">++</span><span class="p">;</span> 
            <span class="c1">// 해당 더하기 작업은 실제 기계어 수준에서는</span>
            <span class="c1">// load *ptr, reg</span>
            <span class="c1">// inc reg, 1</span>
            <span class="c1">// store reg, *ptr</span>
            <span class="c1">// 이렇게 세번의 명령어 수행으로 이루어진다.</span>

            <span class="n">sem_post</span><span class="p">(</span><span class="n">sem</span><span class="p">);</span>
            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">printf</span><span class="p">(</span><span class="s">"[Child] %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="o">*</span><span class="n">ptr</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span> 
    <span class="c1">// 부모 프로세스는 1씩 10000 번 빼기 수행</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sem_wait</span><span class="p">(</span><span class="n">sem</span><span class="p">);</span>
            
            <span class="p">(</span><span class="o">*</span><span class="n">ptr</span><span class="p">)</span><span class="o">--</span><span class="p">;</span> <span class="c1">// </span>
            <span class="c1">// 해당 빼기 작업은 실제 기계어 수준에서는</span>
            <span class="c1">// load *ptr, reg </span>
            <span class="c1">// dec reg, 1</span>
            <span class="c1">//store reg, *ptr</span>
            <span class="c1">// 이렇게 세번의 명령어 수행으로 이루어진다. </span>

            <span class="n">sem_post</span><span class="p">(</span><span class="n">sem</span><span class="p">);</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">{</span>
                <span class="n">printf</span><span class="p">(</span><span class="s">"[Parent] %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="o">*</span><span class="n">ptr</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
        
        <span class="n">wait</span><span class="p">(</span><span class="nb">NULL</span><span class="p">);</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">"Final value: %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="o">*</span><span class="n">ptr</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="4-실행">4. 실행</h3>

<p>위 코드 파일을 컴파일하고, 동시 접근 스레드를 1개만 허용하여 실행하면 그 최종 결과 값은 항상 0이 나오게 된다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user@DESKTOP:~<span class="nv">$ </span> gcc <span class="nt">-o</span> sema sema.c
user@DESKTOP:~<span class="nv">$ </span> ./sema
Mapping an anonymous share memory
<span class="o">[</span>Parent] <span class="nt">-1</span>
<span class="o">[</span>Parent] <span class="nt">-1001</span>
<span class="o">[</span>Parent] <span class="nt">-2001</span>
<span class="o">[</span>Parent] <span class="nt">-3000</span>
<span class="o">[</span>Child] <span class="nt">-2652</span>
<span class="o">[</span>Parent] <span class="nt">-3039</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Parent] <span class="nt">-3175</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Parent] <span class="nt">-3119</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Parent] <span class="nt">-3287</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Parent] <span class="nt">-3250</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Parent] <span class="nt">-3060</span>
<span class="o">[</span>Child] <span class="nt">-3000</span>
<span class="o">[</span>Child] <span class="nt">-2999</span>
<span class="o">[</span>Child] <span class="nt">-1999</span>
<span class="o">[</span>Child] <span class="nt">-999</span>
Final value: 0
</code></pre></div></div>

<p>그러나 ‘-no-sem’ 옵션을 써서 부모 자식 스레드 동시 접근이 가능하도록 설정하면, 그 최종 결과는 0일 수도 있고, 아래와 같이 0 이 아닌 다른 값이 나올 수 도 있게 된다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user@DESKTOP:~<span class="nv">$ </span>./sema <span class="nt">-no-sem</span>
Mapping an anonymous share memory
<span class="o">[</span>Parent] <span class="nt">-1</span>
<span class="o">[</span>Parent] <span class="nt">-992</span>
<span class="o">[</span>Child] <span class="nt">-177</span>
<span class="o">[</span>Parent] <span class="nt">-1005</span>
<span class="o">[</span>Child] <span class="nt">-999</span>
<span class="o">[</span>Child] <span class="nt">-626</span>
<span class="o">[</span>Parent] <span class="nt">-1001</span>
<span class="o">[</span>Parent] <span class="nt">-1006</span>
<span class="o">[</span>Child] <span class="nt">-1001</span>
<span class="o">[</span>Parent] <span class="nt">-1086</span>
<span class="o">[</span>Child] <span class="nt">-1007</span>
<span class="o">[</span>Parent] <span class="nt">-1034</span>
<span class="o">[</span>Child] <span class="nt">-1010</span>
<span class="o">[</span>Parent] <span class="nt">-1041</span>
<span class="o">[</span>Child] <span class="nt">-1021</span>
<span class="o">[</span>Parent] <span class="nt">-1078</span>
<span class="o">[</span>Child] <span class="nt">-1014</span>
<span class="o">[</span>Parent] <span class="nt">-1097</span>
<span class="o">[</span>Child] <span class="nt">-1013</span>
<span class="o">[</span>Child] <span class="nt">-1006</span>
Final value: <span class="nt">-7</span>
</code></pre></div></div>]]></content><author><name>Yong gon Yun</name></author><category term="linux" /><category term="IPC" /><category term="inter-process communication" /><category term="shared memory" /><category term="semaphore" /><summary type="html"><![CDATA[개발자를 위한 반도체 SW개발 기초 (디바이스 드라이버 개발) 관련 학습 19]]></summary></entry></feed>