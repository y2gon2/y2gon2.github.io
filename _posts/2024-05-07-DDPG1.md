---
layout: post
read_time: true
show_date: true
title:  DDPG(Deep Deterministic Policy Gradient) Algorithm 이해
date:   2024-05-07 09:32:20 +0900
description: DDPG(Deep Deterministic Policy Gradient) Algorithm 이해

img: posts/general/post_general15.jpg
tags: [DDPG, Deep Deterministic Policy Gradient, DQN]
author: Yong gon Yun
---

<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

* 해당 내용은 다음의 강의 내용을 개인적으로 재학습 하기 위해 작성됨. <br>
  [인프런 - 유니티 머신러닝 에이전트 완전정복 (기초편) | 민규식](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)


### 1. 기존 DQN 의 한계

선택하는 action 이 이산적인 행동(ex. 상, 하, 좌, 우)환경에만 적용 가능. 따라서 로봇 팔의 움직임, 로콧의 엔진 분출량 조절과 같은 연속적인 선택의 환경에서 적용 불가

### 2. Deep Deterministic Policy Gradient (DDPG) 

* Actor-Critic 기반 강화학습 알고리즘
* DPG(Deterministic Policy Gradient) 알고리즘을 신경망 network 에 적용
* 연속적인 값 중에서 한가지 행동 값을 출력
* 행동을 선택하는 actor 와 해당 행동을 실제로 수행했을 때 다음 상태에서의 q 값을 확인하는 critic 으로 강화학습을 구현

### 3. DDPG 알고리즘 기법

#### 3.1 경험 리플레이 (Experience Replay)

DQN 에서 사용한 바와 같이, 학습을 수행하면서 경험하는 정보를 일정랑 보관하고, 해당 데이터를 임의의 Batch 크기만큼씩 가져와서 훈련시 사용. 데이터간 상관관계 문제를 해결

#### 3.2 타겟 네트워크 (Target Network) - Soft Target Update 기법

기존 DQN 에서는 Target Network 의 경우, 매 step 마다 업데이트하는 경우, Target 애 매 학습마다 변화하는 문제를 발생시키기 때문에, Target Network 는 고정시킨 상태로 학습을 진행. 일정 주기마다 학습 Network 와 동기화 시켜 사용하였다. 

그러나 연속적인 선택에서 이러한 Target Network update 는 맞지 않아 Soft Target Update 기법을 사용하여 매 step update 를 진행함. 

* 지수이동평균(Exponential Moving Average, EMA) 과 같은 방법을 통한 업데이트

<center><img src="assets\img\posts\2024-05-07-DDPG1\1.png" width="420"></center>

* θ  : 학습을 통해 산출된 파라미터
* θ- : Target Network 파라미터

* 0 <=  τ <=  1 값을 통해 기존 Target Network Parameters 업데이트 수준을 조절. 
  - τ == 0    : 학습된 파라미터로 완전 업데이트 (기존 DQN 방식)
  - 0 < τ < 1 : 일정 비율로 비례해서 학습된 파라미터 값을 기존 타겟 네트워크 파라미터에 반영 

#### 3.3  OU Noise 를 사용한 탐험

* 역시 연속된 행동 환경에서 선택 가능한 행동의 수가 무한이므로 기존의 epsilon-greedy 기법을 사용할 수 없음.
* 실수 범위에서 행동을 선택하여 탐험할 수 있는 랜덤 평균 회귀 노이즈 생성

<center><img src="assets\img\posts\2024-05-07-DDPG1\2.png" width="530"></center>

* 1 번 식을 통해 현재 상태값 Xt 에서 다음 상태 Xt+1 로 변경시키는 그 변위 dx 는 2번 식과 같은 형태로 구성된다. 
* 2 번 식의 우변 값을 결정하는 요소들은 다음과 같다. 
    - θ   : 평균 회귀 속도, 즉 변수가 평균 값 𝜇로 돌아가려는 속도
    - μ   : 평균 값으로, 시스템이 장기적으로 안정되려는 목표 상태
    - σ   : 노이즈의 크기 또는 강도, 시스템의 변동성을 결정
    - dt  : 시간 증분, 일반적으로 미분 방정식을 시뮬레이션할 때 사용되는 시간의 단위
    - dWt : 위너 과정(Wiener process) 또는 브라운 운동으로부터 파생된 임의의 충격. dt 시간 동안의 무작위 움직임을 표현
  
  즉,  μ − Xt 는 Xt 가 𝜇로 회귀하려는 정도 또는 현재 위치에서 목표 위치까지의 차이를 의미한다. 그리고 그 회귀가 얼마나 빠른가는 θ 에 비례한다. 다만 σdWt (노이즈 * dt 시간 동안의 무작위 움직임) 가 항상 더해지고 있으므로, 목표값에 도달하더라도 일정 수준의 연속적인 변동이 발생한다. 

​<center><img src="assets\img\posts\2024-05-07-DDPG1\3.png" width="650"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

#### 3.4 Critic Network Update

DQN 과 동일하게 벨만 방정식을 사용하여 Q(x) 가 최대인 값을 타겟값으로 업데이트 한다. 
​<center><img src="assets\img\posts\2024-05-07-DDPG1\4.png" width="580"></center>

<br>
손실함수의 경우도 DQN 과 동일한, 차이 제곱 평균 (MSE) 적용
​<center><img src="assets\img\posts\2024-05-07-DDPG1\5.png" width="260"></center>
<br>
actor network update 는 목표 함수값를 최대하하는 방향으로 정책을 업데이트 

​<center><img src="assets\img\posts\2024-05-07-DDPG1\6.png" width="650"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)
<br>
목표 함수를 최대화 하는 방향 계산을 위한 gradient (아래 유도 과정은 아직 이해 X ;;)

​<center><img src="assets\img\posts\2024-05-07-DDPG1\7.png" width="650"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)
<br>


### 4. DDPG 알고리즘을 사용한 network 학습 프로세스

​<center><img src="assets\img\posts\2024-05-07-DDPG1\8.png" width="650"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

1. Agent 가 환경과 상호작용 (상태 전이)
2. 상호작용시 Agent 의 행동을 결정하는 것은 Actor Network 
   (input: 현재 상태 s -> output : 행동 a)
   다만 이때, 행동은 연속적며, 이런 연속적 행동 선택에서 OU noise 를 추가하여 탐험을 수행 
3. 2번으로 선택된 행동을 환경에 적용, 다음 단계의 상태 (경험) 을 생성.
4. 경험 데이터를 replay memory 에 저장
5. (경험 데이터가 일정량 이상 쌓이 이후), mini batch data 를 sampling 하여 학습을 수행
6. critic network 학습 (input : 상태, 행동 -> output : Q(s, a) 값)
   - 6.1 (일반) Critic Network (input : s, a -> output: q)
   - 6.2 Traget Critic Network (input : s', a' -> output: q')
   - 6.3 6.1 6.2 결과값의 차이를 통해 손실값을 계산
   - 6.4 손실값이 최소화 되도록 critic network 를 update
   - 6.5 매 step 마다 soft target update 로 critic network 를 통해 target critic network 를 update
7. Critic Network 의 q 값을 최대화하는 방향으로 Policy Gradient 를 통해 Actor Network 을 학습시킴.
8. 6.5 와 동일하게 매 step 마다 (일반) soft target update 로 actor network 를 통해 target actor network 를 update