---
layout: post
read_time: true
show_date: true
title:  PPO(Proximal Policy Optimization) Code
date:   2024-05-14 09:32:20 +0900
description: PPO(Proximal Policy Optimization) Code

img: posts/general/post_general18.jpg
tags: [ppo, proximal policy optimization, surrogate object, clipping]
author: Yong gon Yun
---

<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

* 해당 내용은 다음의 강의 및 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br>
  [인프런 - 유니티 머신러닝 에이전트 완전정복 (응용편) ](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)<br>


### 1. 라이브러리 & 파라미터 설정

```python
import numpy as np
import datetime
import platform
import torch
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from mlagents_envs.environment import UnityEnvironment, ActionTuple
from mlagents_envs.side_channel.engine_configuration_channel\
                             import EngineConfigurationChannel
from mlagents_envs.side_channel.environment_parameters_channel\
                             import EnvironmentParametersChannel
# 파라미터 값 세팅 
state_size = 122
# ray 당 정보 수집: 40 rays * 감지된 물체의 (거리 + x축 속도 + z축 속도) = 120
# agent 좌표 (x, z) = 2

action_size = 5 # 상, 하, 좌, 우, 정지

load_model = False
train_mode = True

discount_factor = 0.99  # 미래 보상의 감가율
learning_rate = 3e-4    # 네트워크 학습률
n_step = 128            # 모델 학습 주기
batch_size = 128        # 한번 network 를 업데이트할 때 사용되는 데이터 수
n_epoch = 3             # 한번 모델을 학습할 때 시행하는 epoch 수
_lambda = 0.95          # GAE 기법에 사용할 설정값
epsilon = 0.2           # clipped surrogate objective 에 사용할 설정 값

run_step = 2000000 if train_mode else 0 # 학습모드에서 진행할 스텝 수
test_step = 100000     # 평가모드에서 사용할 스텝수

print_interval = 10
save_interval = 100

# 닷지 환경 설정
env_static_config = {"ballSpeed": 4, "ballRandom": 0.2, "agentSpeed": 3}
# 정적 리셋 파라미터들을 가진 변수 (환경 리셋시 항상 동일하게 사용되는 파라미트들)

env_dynamic_config = {"boardRadius": {"min":6, "max": 8, "seed": 77},
                      "ballNums": {"min": 10, "max": 15, "seed": 77}}
# 동적 리셋 파라미터들을 가진 변수 (환경 리셋시 min ~ max 값 사이의 임의 값을 매번 다르게 생성)

# 유니티 환경 경로 
game = "Dodge"
os_name = platform.system()
if os_name == 'Windows':
    env_name = f"../Env/{game}/{game}"
elif os_name == 'Darwin':
    env_name = f"../envs/{game}_{os_name}"

# 모델 저장 및 불러오기 경로
date_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
save_path = f"./saved_models/{game}/PPO/{date_time}"
load_path = f"./saved_models/{game}/PPO/20230728125435"

# 연산 장치
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

### 2. Model class

```python
class ActorCritic(torch.nn.Module):
    def __init__(self, **kwargs):
        super(ActorCritic, self).__init__(**kwargs)
        self.d1 = torch.nn.Linear(state_size, 128)
        self.d2 = torch.nn.Linear(128, 128)
        self.pi = torch.nn.Linear(128, action_size)
        self.v = torch.nn.Linear(128, 1)
        
    def forward(self, x):
        x = F.relu(self.d1(x))
        x = F.relu(self.d2(x))
        return F.softmax(self.pi(x), dim=-1), self.v(x)
```

Actor-Critic 통합 모델
​<center><img src="assets\img\posts\2024-05-14-PPO2_code\1.png" width="480"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)


### 3. Agent class

#### 3.1 PPOAgent class 와 초기화

```python
class PPOAgent:
    def __init__(self):
        self.network = ActorCritic().to(device)
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)
        self.memory = list() # n_step 동안 진행한 여러 worker(학습 agent)들을 저장
        self.writer = SummaryWriter(save_path)

        if load_model == True: # 저장된 모델을 사용할 경우
            print(f"... Load Model from {load_path}/ckpt ...")
            checkpoint = torch.load(load_path+'/ckpt', map_location=device)  # cpu or gpu 메모링 모델 로드
            self.network.load_state_dict(checkpoint["network"])
            self.optimizer.load_state_dict(checkpoint["optimizer"])
```

#### 3.2 정책을 통해 행동 결정

```python
    def get_action(self, state, training=True):
        # 네트워크 모드 설정 (ex. Dropout layer 존재 시 train mode 에서 활성화, test mode 에서 비활성화)
        self.network.train(training)

        # 네트워크 연산에 따라 행동 결정
        pi, _ = self.network(torch.FloatTensor(state).to(device))
        action = torch.multinomial(pi, num_samples=1).cpu().numpy()
        # torch.multinomial() : pi ( 1 * 5 텐서이며, 모든 요소의 합이 1인 확률 분보 값) 를 사용
        # 해당 확률에 근거하여 하나의 index 를 선택

        return action
```

get_action 메서드를 사용하여 network 를 통해 action 을 선택하는 과정

​<center><img src="assets\img\posts\2024-05-14-PPO2_code\2.png" width="480"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

#### 3.3 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)

```python
    def append_sample(self, state, aciton, reward, next_state, doen):
        self.memory.append((state, action, reward, next_state, done))
```

#### 3.4 학습 수행

```python
    def train_model(self):
        self.network.train()

        # 롤아웃 데이터 추출 및 텐서 변환
        state      = np.stack([m[0] for m in self.memory], axis=0)
        action     = np.stack([m[1] for m in self.memory], axis=0)
        reward     = np.stack([m[2] for m in self.memory], axis=0)
        next_state = np.stack([m[3] for m in self.memory], axis=0)
        done       = np.stack([m[4] for m in self.memory], axis=0)
        self.memory.clear()

        # 실수형 텐서로 변환
        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),
                                                        [state, action, reward, next_state, done])
        # prob_old, adv, ret 계산
        # pi_old    : state 에 대한 업데이트 전 network 의 pi  예측값
        # value     : state 에 대한 업데이트 전 network 의 value 예측값
        # prob_old  : pi_old 의 action index 에 대한 value (업데이트 전 action 의 확률값)
        # adv       : 정책 신경망 업데이트에 사용할 어드벤티지 값
        # ret       : 가치 신경망 업데이트에 사용할 티켓 값
        # delta     : TD error (GAE 에서 1 ~ T step 까지 TD error 의 합은 Advantage (At) 임)
        with torch.no_grad():
            pi_old, value = self.network(state)         # (1)
            prob_old = pi_old.gather(1, action.long())  # (2)
            # gather(1, action) : 1 차원 (행) 의 상태를 유지하면서, 열의 값에서 action 에 해당하는 index 값을 추출

            _, next_value = self.network(next_state)    # (3)
            delta = reward + (1 - done) * discount_factor * next_value - value
            adv = delta.clone()                         # (4)

            adv, done = map(                            # (5)
                lambda x: x.view(n_step, -1).transpose(0,1).contiguous(), 
                [adv, done]
            ) 
            # view(n_step, -1): 만약 n_step 이 32, adv 와 done 텐서 크기가 128 이였다면, 
            #      adv, done 을 각각 4, 32 tensor 차원으로 변환 
            #      즉, 메모리에서 추출한 data 를 num_worker * n_step 차원을 변환
            # transpose(0, 1) : num_worker * n_step => n_step * num_worker 변환
            # countiguous()   : view나 transpose 같은 연산 후에 텐서의 물리적인 메모리 배열이 실제 데이터 배열과 일치하지 않을 수 있음
            #      contiguous()는 데이터를 메모리 상에서 연속적으로 재배치하여 텐서가 예상대로 작동하도록 함.

            # GAE 연산 수행
            for t in reversed(range(n_step-1)):         # (6)
                adv[:, t] += (1 - done[:, t]) * discount_factor * _lambda * adv[:, t+1]
            
            # GAE 작업을 완료된 advantage 값들은 원래 차원으로 되돌리는 변환을 진행한다.
            adv = adv.transpose(0,1).contiguous().view(-1, 1)
            
            # 현재 정책 조건에서의  advantage + 현재 상태 value 
            # => 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용
            ret = adv + value                           # (7)

        # 학습 이터레이션 시작
        actor_losses, critic_losses = [], []
        idxs = np.arange(len(reward))   # 사용되는 전체 data 의 크기의 index
        for _ in range(n_epoch):
            np.random.shuffle(idxs)     # idxs 간 순서를 통한 연관성을 배제시키기 위해
            for offset in range(0, len(reward), batch_size):
                idx = idxs[offset : offset + batch_size] # batch 크기 만큼씩 slicing

                _state, _action, _ret, _adv, _prob_old =\
                    map(
                        lambda x: x[idx], 
                        [state, action, ret, adv, prob_old]
                    ) # slicing 크기 만큼씩 추출
                
                pi, value = self.network(_state)
                prob = pi.gather(1, _action.long()) # 해당 action 의 정책 확률 추출

                # 정책신경망 손실함수 계산
                ratio = prob / (_prob_old + 1e-7)   # probability ratio
                surr1 = ratio * _adv                # surrogate object
                surr2 = torch.clamp(ratio, min=1-epsilon, max=1+epsilon) * _adv # clipped surrogate object
                actor_loss = -torch.min(surr1, surr2).mean()

                # 가치신경망 손실함수 계산
                critic_loss = F.mse_loss(value, _ret).mean()    # ret - V(s) 의 제곱 평균

                total_loss = actor_loss + critic_loss           # action entropy 반영되지 않음

                self.optimizer.zero_grad()
                total_loss.backward()
                self.optimizer.step()

                actor_losses.append(actor_loss.item())
                critic_losses.append(critic_loss.item())

        return np.mean(actor_losses), np.mean(critic_losses)
```

​<center><img src="assets\img\posts\2024-05-14-PPO2_code\3.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)


1. pi_old, value = self.network(state)   
   
   update 되기 전 network 에 상태를 넣어 update 되기 전 정책과 value 구함. 

2. prob_old = pi_old.gather(1, action.long())
   
   pi_old tensor 에서 열(column) 이 action 인 곳의 값을 취함. 

3. _, next_value = self.network(next_state)<br>
   delta = reward + (1 - done) * discount_factor * next_value - value

   next_state 를 update 전 network 에 넣어 next_value 를 구하고, 이를 앞에서 계산된 요소들로 Temporal Difference Error 를 구한다. 


4. adv = delta.clone()
   
   TD error 복제 하여 advantage 로 사용

5. adv, done = map(lambda x: x.view(n_step, -1).transpose(0, 1).contiguous(), [adv, done])
   
   GAE 진행 전, 학습 가능한 차원으로 adv를 전처리. (n_step * num_worker) 차원으로 변환.

6. for t in reversed(range(n_step - 1)):<br>
   adv[:, t] += (1 - done[:, t]) * discount_factor * _lambda * adv[:, t + 1]
    
    GAE 연산 수행

7. ret = adv + value<br>

   현재 정책 조건에서의  advantage + 현재 상태 value => 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용


#### 3.5 네트워크 모델 저장 & 학습기록

```python
    # 네트워크 모델 저장
    def save_model(self):
        print(f"... Save Model to {save_path}/ckpt ...")
        torch.save({
            "network" : self.network.state_dict(),
            "optimizer" : self.optimizer.state_dict(),
        }, save_path+'/ckpt')

    # 학습기록
    def write_summary(self, score, actor_loss, critic_loss, step):
        self.writer.add_scalar("run/score", score, step)
        self.wirter.add_scalar("model/actor_loss", actor_loss, step)
        self.writer.add_scalar("model/critic_loss", critic_loss, step)
```

### 4. Main 함수

```python
if __name__ == '__main__':
    # 유니티 환경 경로 설정 (file_name)
    engine_configuration_channel = EngineConfigurationChannel()
    environment_parameters_channel = EnvironmentParametersChannel()
    env = UnityEnvironment(file_name=env_name,
                           side_channels=[engine_configuration_channel,
                                          environment_parameters_channel])
    env.reset()

    # 유니티 behavior 설정 
    behavior_name = list(env.behavior_specs.keys())[0]
    spec = env.behavior_specs[behavior_name]
    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)
    
    # 환경 정적 파라미터 값 설정
    for key, value in env_static_config.items():
        environment_parameters_channel.set_float_parameter(key, value)

    # 환경 동적 파라미터 분포 설정 (환경이 reset 될 때마다 설정된 범위 내에서 임의 값을 sampling 하여 사용)
    for key, value in env_dynamic_config.items():
        environment_parameters_channel.set_uniform_sampler_parameters(
                              key, value["min"], value["max"], value["seed"])
    
    # decision step, termination step
    # episode 진행 중 -> dec
    # episode 종료 : 종료 step -> term / 다음 episode 의 첫 step -> dec
    dec, term = env.get_steps(behavior_name)
    num_worker = len(dec)

    # PPO 클래스를 agent로 정의 
    agent = PPOAgent()
    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0
    for step in range(run_step + test_step):
        if step == run_step:
            if train_mode:
                agent.save_model()
            print("TEST START")
            train_mode = False
            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)
        
        state = dec.obs[0]
        # dec.obs : 지정한 behavior_name 가진 모든 agent 에 대한 모든 관측을 포함한 튜플
        # 현재 관측 정보는 단 1개 종류이므로 obs[0] 값만 가져와서 state 정보로 넣어줌.

        action = agent.get_action(state, train_mode)
        action_tuple = ActionTuple()
        action_tuple.add_discrete(action)           # 이산적 action 할당
        env.set_actions(behavior_name, action_tuple)# unity 에 동작 정보 전달
        env.step()                                  # 동작 진행

        # 환경으로부터 얻는 정보 (분산 학습을 위한 worker data 저장)
        dec, term = env.get_steps(behavior_name)
        done = [False] * num_worker # 모든 worker 들 done 을 일괄적으로 초기화
        next_state = dec.obs[0]     # 모든 worker 들의 관측 정보 반영
        reward = dec.reward         # 모든 worker 들의 보상 정보 반영
        
        # 종료 worker 정보를 업데이트 
        for id in term.agent_id:
            _id = list(term.agent_id).index(id)
            done[id] = True
            next_state[id] = term.obs[0][_id]
            reward[id] = term.reward[_id]
        score += reward[0]

        if train_mode:
            # rollout memory 에 woker 들의 정보 저장
            for id in range(num_worker):
                agent.append_sample(state[id], action[id], [reward[id]], next_state[id], [done[id]])
            
            # n_step 마다 학습수행 모델 업데이트 및 loss 값 들을 리스트에 추가
            if (step+1) % n_step == 0:
                actor_loss, critic_loss = agent.train_model()
                actor_losses.append(actor_loss)
                critic_losses.append(critic_loss)

        if done[0]:
            episode +=1
            scores.append(score)
            score = 0

            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 
            if episode % print_interval == 0:
                mean_score = np.mean(scores)
                mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0
                mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0
                agent.write_summary(mean_score, mean_actor_loss, mean_critic_loss, step)
                actor_losses, critic_losses, scores = [], [], []

                print(f"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / " +\
                      f"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}" )

            # 네트워크 모델 저장 
            if train_mode and episode % save_interval == 0:
                agent.save_model()
    env.close()
```


### 5. 전체 코드 

```python
# 라이브러리 불러오기
import numpy as np
import datetime
import platform
import torch
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from mlagents_envs.environment import UnityEnvironment, ActionTuple
from mlagents_envs.side_channel.engine_configuration_channel\
                             import EngineConfigurationChannel
from mlagents_envs.side_channel.environment_parameters_channel\
                             import EnvironmentParametersChannel
# 파라미터 값 세팅 
state_size = 122
# ray 당 정보 수집: 40 rays * 감지된 물체의 (거리 + x축 속도 + z축 속도) = 120
# agent 좌표 (x, z) = 2

action_size = 5 # 상, 하, 좌, 우, 정지

load_model = False
train_mode = True

discount_factor = 0.99  # 미래 보상의 감가율
learning_rate = 3e-4    # 네트워크 학습률
n_step = 128            # 모델 학습 주기
batch_size = 128        # 한번 network 를 업데이트할 때 사용되는 데이터 수
n_epoch = 3             # 한번 모델을 학습할 때 시행하는 epoch 수
_lambda = 0.95          # GAE 기법에 사용할 설정값
epsilon = 0.2           # clipped surrogate objective 에 사용할 설정 값

run_step = 2000000 if train_mode else 0 # 학습모드에서 진행할 스텝 수
test_step = 100000     # 평가모드에서 사용할 스텝수

print_interval = 10
save_interval = 100

# 닷지 환경 설정
env_static_config = {"ballSpeed": 4, "ballRandom": 0.2, "agentSpeed": 3}
# 정적 리셋 파라미터들을 가진 변수 (환경 리셋시 항상 동일하게 사용되는 파라미트들)

env_dynamic_config = {"boardRadius": {"min":6, "max": 8, "seed": 77},
                      "ballNums": {"min": 10, "max": 15, "seed": 77}}
# 동적 리셋 파라미터들을 가진 변수 (환경 리셋시 min ~ max 값 사이의 임의 값을 매번 다르게 생성)

# 유니티 환경 경로 
game = "Dodge"
os_name = platform.system()
if os_name == 'Windows':
    env_name = f"../Env/{game}/{game}"
elif os_name == 'Darwin':
    env_name = f"../envs/{game}_{os_name}"

# 모델 저장 및 불러오기 경로
date_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
save_path = f"./saved_models/{game}/PPO/{date_time}"
load_path = f"./saved_models/{game}/PPO/20230728125435"

# 연산 장치
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ActorCritic 클래스 -> Actor Network, Critic Network 정의 
class ActorCritic(torch.nn.Module):
    def __init__(self, **kwargs):
        super(ActorCritic, self).__init__(**kwargs)
        self.d1 = torch.nn.Linear(state_size, 128)
        self.d2 = torch.nn.Linear(128, 128)
        self.pi = torch.nn.Linear(128, action_size)
        self.v = torch.nn.Linear(128, 1)
        
    def forward(self, x):
        x = F.relu(self.d1(x))
        x = F.relu(self.d2(x))
        return F.softmax(self.pi(x), dim=-1), self.v(x)

# PPOAgent 클래스 -> PPO 알고리즘을 위한 다양한 함수 정의 
class PPOAgent:
    def __init__(self):
        self.network = ActorCritic().to(device)
        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)
        self.memory = list() # n_step 동안 진행한 여러 worker(학습 agent)들을 저장
        self.writer = SummaryWriter(save_path)

        if load_model == True: # 저장된 모델을 사용할 경우
            print(f"... Load Model from {load_path}/ckpt ...")
            checkpoint = torch.load(load_path+'/ckpt', map_location=device)  # cpu or gpu 메모링 모델 로드
            self.network.load_state_dict(checkpoint["network"])
            self.optimizer.load_state_dict(checkpoint["optimizer"])

    # 정책을 통해 행동 결정 
    def get_action(self, state, training=True):
        # 네트워크 모드 설정 (ex. Dropout layer 존재 시 train mode 에서 활성화, test mode 에서 비활성화)
        self.network.train(training)

        # 네트워크 연산에 따라 행동 결정
        pi, _ = self.network(torch.FloatTensor(state).to(device))
        action = torch.multinomial(pi, num_samples=1).cpu().numpy()
        # torch.multinomial() : pi ( 1 * 5 텐서이며, 모든 요소의 합이 1인 확률 분보 값) 를 사용
        # 해당 확률에 근거하여 하나의 index 를 선택

        return action

    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
    def append_sample(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    # 학습 수행
    def train_model(self):
        self.network.train()

        # 롤아웃 데이터 추출 및 텐서 변환
        state      = np.stack([m[0] for m in self.memory], axis=0)
        action     = np.stack([m[1] for m in self.memory], axis=0)
        reward     = np.stack([m[2] for m in self.memory], axis=0)
        next_state = np.stack([m[3] for m in self.memory], axis=0)
        done       = np.stack([m[4] for m in self.memory], axis=0)
        self.memory.clear()

        # 실수형 텐서로 변환
        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),
                                                        [state, action, reward, next_state, done])
        # prob_old, adv, ret 계산
        # pi_old    : state 에 대한 업데이트 전 network 의 pi  예측값
        # value     : state 에 대한 업데이트 전 network 의 value 예측값
        # prob_old  : pi_old 의 action index 에 대한 value (업데이트 전 action 의 확률값)
        # adv       : 정책 신경망 업데이트에 사용할 어드벤티지 값
        # ret       : 가치 신경망 업데이트에 사용할 티켓 값
        # delta     : TD error (GAE 에서 1 ~ T step 까지 TD error 의 합은 Advantage (At) 임)
        with torch.no_grad():
            pi_old, value = self.network(state)         # (1)
            prob_old = pi_old.gather(1, action.long())  # (2)
            # gather(1, action) : 1 차원 (행) 의 상태를 유지하면서, 열의 값에서 action 에 해당하는 index 값을 추출

            _, next_value = self.network(next_state)    # (3)
            delta = reward + (1 - done) * discount_factor * next_value - value
            adv = delta.clone()                         # (4)

            adv, done = map(                            # (5)
                lambda x: x.view(n_step, -1).transpose(0,1).contiguous(), 
                [adv, done]
            ) 
            # view(n_step, -1): 만약 n_step 이 32, adv 와 done 텐서 크기가 128 이였다면, 
            #      adv, done 을 각각 4, 32 tensor 차원으로 변환 
            #      즉, 메모리에서 추출한 data 를 num_worker * n_step 차원을 변환
            # transpose(0, 1) : num_worker * n_step => n_step * num_worker 변환
            # countiguous()   : view나 transpose 같은 연산 후에 텐서의 물리적인 메모리 배열이 실제 데이터 배열과 일치하지 않을 수 있음
            #      contiguous()는 데이터를 메모리 상에서 연속적으로 재배치하여 텐서가 예상대로 작동하도록 함.

            # GAE 연산 수행
            for t in reversed(range(n_step-1)):         # (6)
                adv[:, t] += (1 - done[:, t]) * discount_factor * _lambda * adv[:, t+1]
            
            # GAE 작업을 완료된 advantage 값들은 원래 차원으로 되돌리는 변환을 진행한다.
            adv = adv.transpose(0,1).contiguous().view(-1, 1)
            
            # 현재 정책 조건에서의  advantage + 현재 상태 value 
            # => 특정 행동 a 를 최했을 때의 전체적인 가치 Q(s, a) 추정치 로 사용
            ret = adv + value                           # (7)

        # 학습 이터레이션 시작
        actor_losses, critic_losses = [], []
        idxs = np.arange(len(reward))   # 사용되는 전체 data 의 크기의 index
        for _ in range(n_epoch):
            np.random.shuffle(idxs)     # idxs 간 순서를 통한 연관성을 배제시키기 위해
            for offset in range(0, len(reward), batch_size):
                idx = idxs[offset : offset + batch_size] # batch 크기 만큼씩 slicing

                _state, _action, _ret, _adv, _prob_old =\
                    map(
                        lambda x: x[idx], 
                        [state, action, ret, adv, prob_old]
                    ) # slicing 크기 만큼씩 추출
                
                pi, value = self.network(_state)
                prob = pi.gather(1, _action.long()) # 해당 action 의 정책 확률 추출

                # 정책신경망 손실함수 계산
                ratio = prob / (_prob_old + 1e-7)   # probability ratio
                surr1 = ratio * _adv                # surrogate object
                surr2 = torch.clamp(ratio, min=1-epsilon, max=1+epsilon) * _adv # clipped surrogate object
                actor_loss = -torch.min(surr1, surr2).mean()

                # 가치신경망 손실함수 계산
                critic_loss = F.mse_loss(value, _ret).mean()    # ret - V(s) 의 제곱 평균

                total_loss = actor_loss + critic_loss           # action entropy 반영되지 않음

                self.optimizer.zero_grad()
                total_loss.backward()
                self.optimizer.step()

                actor_losses.append(actor_loss.item())
                critic_losses.append(critic_loss.item())

        return np.mean(actor_losses), np.mean(critic_losses)

    # 네트워크 모델 저장
    def save_model(self):
        print(f"... Save Model to {save_path}/ckpt ...")
        torch.save({
            "network" : self.network.state_dict(),
            "optimizer" : self.optimizer.state_dict(),
        }, save_path+'/ckpt')

    # 학습 기록 
    def write_summary(self, score, actor_loss, critic_loss, step):
        self.writer.add_scalar("run/score", score, step)
        self.writer.add_scalar("model/actor_loss", actor_loss, step)
        self.writer.add_scalar("model/critic_loss", critic_loss, step)

# Main 함수 -> 전체적으로 PPO 알고리즘을 진행 
if __name__ == '__main__':
    # 유니티 환경 경로 설정 (file_name)
    engine_configuration_channel = EngineConfigurationChannel()
    environment_parameters_channel = EnvironmentParametersChannel()
    env = UnityEnvironment(file_name=env_name,
                           side_channels=[engine_configuration_channel,
                                          environment_parameters_channel])
    env.reset()

    # 유니티 behavior 설정 
    behavior_name = list(env.behavior_specs.keys())[0]
    spec = env.behavior_specs[behavior_name]
    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)
    
    # 환경 정적 파라미터 값 설정
    for key, value in env_static_config.items():
        environment_parameters_channel.set_float_parameter(key, value)

    # 환경 동적 파라미터 분포 설정 (환경이 reset 될 때마다 설정된 범위 내에서 임의 값을 sampling 하여 사용)
    for key, value in env_dynamic_config.items():
        environment_parameters_channel.set_uniform_sampler_parameters(
                              key, value["min"], value["max"], value["seed"])
    
    # decision step, termination step
    # episode 진행 중 -> dec
    # episode 종료 : 종료 step -> term / 다음 episode 의 첫 step -> dec
    dec, term = env.get_steps(behavior_name)
    num_worker = len(dec)

    # PPO 클래스를 agent로 정의 
    agent = PPOAgent()
    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0
    for step in range(run_step + test_step):
        if step == run_step:
            if train_mode:
                agent.save_model()
            print("TEST START")
            train_mode = False
            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)
        
        state = dec.obs[0]
        # dec.obs : 지정한 behavior_name 가진 모든 agent 에 대한 모든 관측을 포함한 튜플
        # 현재 관측 정보는 단 1개 종류이므로 obs[0] 값만 가져와서 state 정보로 넣어줌.

        action = agent.get_action(state, train_mode)
        action_tuple = ActionTuple()
        action_tuple.add_discrete(action)           # 이산적 action 할당
        env.set_actions(behavior_name, action_tuple)# unity 에 동작 정보 전달
        env.step()                                  # 동작 진행

        # 환경으로부터 얻는 정보 (분산 학습을 위한 worker data 저장)
        dec, term = env.get_steps(behavior_name)
        done = [False] * num_worker # 모든 worker 들 done 을 일괄적으로 초기화
        next_state = dec.obs[0]     # 모든 worker 들의 관측 정보 반영
        reward = dec.reward         # 모든 worker 들의 보상 정보 반영
        
        # 종료 worker 정보를 업데이트 
        for id in term.agent_id:
            _id = list(term.agent_id).index(id)
            done[id] = True
            next_state[id] = term.obs[0][_id]
            reward[id] = term.reward[_id]
        score += reward[0]

        if train_mode:
            # rollout memory 에 woker 들의 정보 저장
            for id in range(num_worker):
                agent.append_sample(state[id], action[id], [reward[id]], next_state[id], [done[id]])
            
            # n_step 마다 학습수행 모델 업데이트 및 loss 값 들을 리스트에 추가
            if (step+1) % n_step == 0:
                actor_loss, critic_loss = agent.train_model()
                actor_losses.append(actor_loss)
                critic_losses.append(critic_loss)

        if done[0]:
            episode +=1
            scores.append(score)
            score = 0

            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 
            if episode % print_interval == 0:
                mean_score = np.mean(scores)
                mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0
                mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0
                agent.write_summary(mean_score, mean_actor_loss, mean_critic_loss, step)
                actor_losses, critic_losses, scores = [], [], []

                print(f"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / " +\
                      f"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}" )

            # 네트워크 모델 저장 
            if train_mode and episode % save_interval == 0:
                agent.save_model()
    env.close()
```
