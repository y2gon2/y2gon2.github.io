---
layout: post
read_time: true
show_date: true
title:  DDPG(Deep Deterministic Policy Gradient) 구현
date:   2024-05-07 12:32:20 +0900
description: DDPG(Deep Deterministic Policy Gradient) 구현

img: posts/general/post_general16.jpg
tags: [DDPG, Deep Deterministic Policy Gradient, DQN, pytorch]
author: Yong gon Yun
---

<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

* 해당 내용은 다음의 강의 내용을 개인적으로 재학습 하기 위해 작성됨. <br>
  [인프런 - 유니티 머신러닝 에이전트 완전정복 (기초편) | 민규식](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

### 0. 코드 요약

해당 code 는 DDPG 를 적용한 신경망 학습 코드이며, 그 내용은 아래와 같다.

1. 라이브러리 & 파라미터 설정
2. OU Noise class : 연속적인 행동을 선택할 때 탐험을 위한 noise 클래스
3. Actor class : state 입력을 통해 action 을 선택하는 신경망 클래스
4. Critic class: state, action 입력을 통해 q 값을 반환하는 신경망 클래스
5. Agent class
6. Main 함수

해당 포스트에서는 코드의 알고리즘 진행 프로세스와 ML-agents 와 연동하여 어떻게 프로그램 로직이 연결되어 있는지 위주로 설명할 예정임. 해당 코드에서의 PyTroch , ML-agents 등 라이브러리 사용 및 환경 설정에 대한 설명은 이전 포스트 (Deep Q-Network + ML-agents 구현)[https://y2gon2.github.io/DQN_ml_agents.html]  와 유사함으로 해당 포스트를 참조.

### 1. 라이브러리 & 파라미터 설정

```python
import numpy as np
import random
import copy
import datetime
import platform
import torch
import torch.nn.functional as F

from torch.unils.tensorboard import SummaryWriter
from collections import deque
from mlagents_envs.environment import UnityEnvironment, ActionTuple
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel

# DDPG 파라미터 
state_size = 9 # (1)
action_size = 3 # 각 축방향 값

load_model = False
train_mode = True

batch_size = 128
mem_maxlen = 50000
discount_factor = 0.9
actor_lr = 1e-4     # actor network 학습률
critic_lr = 5e-4    # critic network 학습률
tau  = 1e-3         # soft target update parameter

# OU noise 파라미터
mu = 0          # 회귀할 평균값
theta = 1e-3    # 회귀 속도
sigma = 2e-3    # 랜덤 프로세스의 변동성

run_step = 50000 if train_mode else 0
test_step = 10000
train_start_step = 5000

print_interval = 10
save_interval = 100

# 유니티 환경 경로
game = "Drone"
os_name = platform.system()
if os_name == 'Windows':
    env_name = f"../envs/{game}_{os_name}/{game}"
elif os_name == 'Darwin':
    env_name = f"../envs/{game}_{os_name}"

# 모델 저장 및 불러오기 경로
date_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
save_path = f"./saved_models/{game}/DDPG/{date_time}"
load_path = f"./saved_models/{game}/DDPG/202405071234"

# 연산장치
device = torch.device("cuda" if torch.cudoa.is_available() else "cpu")
```
(1) state_size = 9

현재 드론의 위치 - 골인 지점의 위치 (x, y, z) 
현재 드론의 속도 (x, y, z)                    
현재 드론의 각속도 (x, y, z)
=> 총 9개의 상태값 사용

### 2. OU Noise class

```python
class OU_noise:
    def __init__(self):
        self.reset()

    def reset(self): # noise reset
        self.X = np.ones((1, action_size), dtype=np.float32) * mu # (1)

    def sample(self): # noise sampling
        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X)) # (2)
        self.X += dx
        return self.x
```
(1) self.X = np.ones((1, action_size), dtype=np.floate32) * mu 

* 모두 1.0 값을 가지는 (1 * 3) 크기의 np array * 0 =  [[0.0, 0.0, 0.0]]<br>

(2) dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))

* len(self.X) = 1 (1 * 3 size np array 이므로)
* np.random.randn(1) : 평균 0, 표준편자 1 정규분포 내 임의의 값 1개

### 3. Actor class

```python
class Actor(torch.nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.fc1 = torch.nn.Linear(state_size, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.mu = torch.nn.Linear(128, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        return torch.tanh(self.mu(x)) # (1)
```

(1) torch.tanh(self.mu(x))

비화성화 함수 torch.tanh() 는 해당 결과를  -1 ~ 1 사이 값을 갖는 결과를 출력한다. 그런데 일반적으로 사용하는 relu() 가 아닌 tanh() 사용한 이유는 무엇일까? (by ChatGPT)

* tanh()
​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\1.png" width="200"></center>

1. 출력 범위 제한:
액션 공간(action space)이 연속적이고 특정 범위 내에서 정의될 때 (예: 로봇 팔의 관절 각도나 차량의 조향 각도), 액션의 크기를 적절히 제한하는 것이 중요합니다. tanh 함수는 자연스럽게 출력을 -1에서 1 사이로 스케일링하여, 액션의 범위를 효과적으로 제어할 수 있게 합니다.

1. 미분 가능성과 비선형성:
tanh 함수는 연속적이며 미분 가능한 비선형 함수로서, 신경망의 학습 과정에서 그라디언트 기반 최적화 방법을 사용할 때 중요한 역할을 합니다. 비선형 함수를 사용함으로써, 네트워크는 더 복잡한 액션 선택 전략을 학습할 수 있습니다.

1. Zero-Centered 출력:
tanh는 출력이 중심(0)을 기준으로 대칭이라는 장점이 있습니다. 이는 학습 과정에서 네트워크가 편향되지 않게 하는 데 도움이 될 수 있습니다. 예를 들어, 로봇이 앞으로 가거나 뒤로 가는 액션을 선택할 때, tanh는 양의 방향과 음의 방향을 모두 동일한 범위로 제공하여 균형 잡힌 학습을 촉진합니다.

1. 자연스러운 전환:
tanh 함수는 -1과 1 근처에서 포화(saturation)되는 성질을 가지고 있습니다. 이는 매우 높거나 낮은 입력 값에 대해 출력의 변화가 적어짐을 의미합니다. 이 포화 특성은 때때로 네트워크가 극단적인 액션을 취하는 것을 자연스럽게 제한할 수 있습니다.

<center>Actor Network</center>
​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\2.png" width="400"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

### 4. Critic class

```python
class Critic(torch.nn.Module):
    def __init__(self):
        super(Critic, self).__init__()

        self.fc1 = torch.nn.Linear(state_size, 128)
        self.fc2 = torch.nn.Linear(128 + action_size, 128)
        self.q = torch.nn.Linear(128, 1) # q 값 반환

    def forward(self, state, action):
        x = torch.relu(self.fc1(state))
        x = torch.cat((x, action), dim=-1) # (1)
        x = torch.relu(self.fc2(x))
        return self.q(x)
```

(1) x = torch.cat((x, action), dim=-1)

critic network 에 input parameter 는 state 와 action 두 개 이다. 두 요소가 서로 관련은 있지만, 수식적으로 동등한 요소로 취급해도 되는 것일까? 두 요소를 다르게 취급하려면 어떻게 해야 할까? (by ChatGPT)

상태(state)와 행동(action) 정보를 같은 첫 번째 층에 동시에 넣는 것 대신 두 번째 층에서 결합하는 방식을 선택하는 이유는 신경망의 학습 동역학과 관련된 몇 가지 중요한 고려사항 때문입니다. 여기서 고려하는 접근 방식은 알고리즘의 성능과 일반화 능력을 최적화하는 데 도움이 됩니다.

1. 정보 처리의 단계적 접근:
상태(state) 정보는 환경에 대한 전반적인 맥락을 제공하고, 행동(action) 정보는 특정 상황에서 취할 구체적인 조치를 나타냅니다. 첫 번째 층에서 상태 정보만 처리함으로써 네트워크는 먼저 상태에 대한 특징을 추출하고 이해하는 데 집중할 수 있습니다. 그런 다음, 두 번째 층에서 이 상태 특징을 행동 정보와 결합함으로써, 특정 상태에서 특정 행동을 취했을 때의 결과(예: Q-값)를 학습하는 데 필요한 관계를 더 효과적으로 모델링할 수 있습니다.

2. 학습의 효율성:
상태와 행동을 각각 다른 층에서 처리함으로써 학습 과정에서 더 효율적으로 정보를 조합할 수 있습니다. 상태 정보에 기반한 특징은 행동에 독립적으로 유용할 수 있으며, 이를 통해 상태의 중요한 특성을 더 넓은 맥락에서 추출할 수 있습니다. 이러한 접근은 특히 행동 공간이 클 때 유용하며, 행동과 상태 사이의 복잡한 상호작용을 학습하는 데 도움을 줍니다.

3. 모듈성과 유연성:
상태와 행동을 별도의 층에서 처리하는 것은 신경망의 모듈성을 증가시키고, 각각의 입력 유형에 특화된 처리 방식을 적용할 수 있게 합니다. 이는 신경망이 더 다양한 종류의 문제에 적응하고, 상태 또는 행동의 표현이 변경되었을 때 쉽게 확장하거나 수정할 수 있는 구조를 제공합니다.

4. 노이즈와 불확실성 관리:
상태 데이터는 종종 노이즈가 많고 불확실할 수 있습니다. 상태 정보를 먼저 처리하고 그 결과를 행동과 결합함으로써, 네트워크는 노이즈가 결과에 미치는 영향을 최소화할 수 있으며, 더 정확한 Q-값 예측을 위한 정보 처리가 가능해집니다.

이러한 이유로, 상태와 행동을 다른 층에서 처리하는 것은 일반적으로 더 나은 결과를 제공하며, 강화 학습 알고리즘에서 널리 사용되는 접근 방식입니다.

위와 같은 이유로 첫번째 layer 에 state 를 넣고, 두번째 layer 에 action 을 추가하기 위해, layer 의 units 을 확장(concatenate) 시킬 수 있는 torch.cat((x, action), dim=-1) 를 사용하였다. 여기서 `dim=-1`
은 x, 와 action 텐서의 마지막 차원(열) 갯수의 합으로 units 확장하여 결합시킨다는 의미이다. 

따라서 해당 네트워크를 도식화 하면 아래와 같다. 

​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\3.png" width="400"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

### 5. Agent class

```python
class DDPGAgent():
    def __init__(self):
        self.actor = Actor().to(device)
        self.target_actor = copy.deepcopy(self.actor)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic = Critic().to(device)
        self.target_critic = copy.deepcopy(self.cirtic)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        self.OU = OU_noise()
        self.memory = deque(maxlen=mem_maxlen)
        self.writer = SummaryWriter(save_path)

        if load_model == True:
            checkpoint = torch.load(load_path+'/ckpt', map_location=device)
            self.actor.load_state_dict(checkpoint["actor"])
            self.target_actor.load_state_dict(checkpoint["actor"])
            self.actor_mitimizer.load_state_dict(checkpoint["actor_optimizer"])
            self.critic.load_state_dict(checkpoint["critic"])
            self.critic_optimizer.load_state_dic(checkpoint["critic_optimizer"])

    # OU noise 기법에 따라 행동결정
    def get_action(self, state, training=True):
        # 네트워크 모드 설정
        self.actor.train(training) # 훈련 상태로 준비 (ex. Dropout layer 일부 노드 무작위 비활성화, 입력 정규화 (BatchNorm layer))

        action = self.actor(torch.FloatTensor(state).to(device)).cpu().detach().numpy()

        # train_mode -> action + noise 반환
        # test_mode  -> action 반환
        return action + self.OU.sample() if training else action
    
    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
    def append_sample(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_model(self):
        batch = random.sample(self.memory, batch_size)
        state = np.stack([b[0] for b in batch], axis=0)
        action = np.stack([b[1] for b in batch], axis=0)
        reward = np.stack([b[2] for b in batch], axis=0)
        next_state = np.stack([b[3] for b in batch], axis=0)
        done = np.stack([b[4] for b in batch], axis=0)

        state, action, reward, next_state, done = map(
            lambda x: torch.FloatTensor(x).to(device),
            [state, action, reward, next_state, done]
        )

        # Critic update
        next_actions = self.target_action(next_state) # (1)
        next_q = self.target_critic(next_state, next_actions) # (2)
        target_q = reward + (1 - done) * discount_factor * next_q # (3)
        q = self.critic(state, action) # (4)
        critic_loss = F.mse_loss(target_q, q) # (5)

        # (6)
        self.critic_optimizer.zero_grad() 
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        action_pred = self.actor(state) # (7)
        actor_loss = -self.critic(state, action_pred).mean() # (8)

        # (9)
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        return actor_loss.item(), critic_loss.item()
    
    # soft target update
    def soft_update_target(self):
        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)

        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(tau * local_param.data + ( 1.0 - tau) * target_param.data)

    # 네트워크 모델 저장
    def save_model(self):
        print(f"... Save Model to {save_path}/ckpt ...")
        torch.save({
            "actor" : self.actor.state_dict(),
            "actor_optimizer" : self.actor_optimizer.state_dict(),
            "critic" : self.critic.state_dict(),
            "critic_optimizer" : self.critic_optimizer.state_dict(),
        }, save_path+'/ckpt')

    # 학습 기록
    def write_summary(self, score, actor_loss, critic_loss, step):
        self.writer.add_scalar("run/score", score, step)
        self.writer.add_scalar("model/actor_loss", actor_loss, step)
        self.writer.add_scalar("model/critic_loss", critic_loss, step)
```

train_model 메서드 process 진행 과정 도식도

​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\4.png" width="700"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)


### 6. Main 함수

```python
if __name__ == '__main__':
    # 유니티 환경 경로 설정 (file name)
    engine_configuration_channel = EngineConfigurationChannel()
    env = UnityEnvironment(
        file_name=env_name,
        side_channels=[engine_configuration_channel]
    )

    env.reset()

    # 유니티 브레인 설정
    behavior_name = list(env.behavior_specs.keys())[0]
    spec = env.behavior_specs[behavior_name]
    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)
    dec, term = env.get_steps(behavior_name)

    # DDPGAgent 클래스를 agent 로 정의
    agent = DDPGAgent()
    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0
    for step in range(run_step + test_step):
        if step == run_step:
            if train_mode:
                agent.save_model()
            print("TEST START")
            train_mode = False
            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)

        state = dec.obs[0] # (1)
        action = agent.get_action(state, train_mode) # (2)
        action_tuple = ActionTuple()
        action_tuple.add_continuous(action)
        env.set_actions(behavior_name, action_tuple) # (3)
        env.step() # (4)

        dec, term = env.get_steps(behavior_name) # (5)
        done = len(term.agent_id) > 0
        reward = term.reward if done else dec.reward #(5.1)
        next_state = term.obs[0] if done else dec.obs[0] # (5.2)
        score += reward[0]

        if train_mode: # (6)
            agent.append_sample(state[0], action[0], reward, next_state[0], [done])

        if train_mode and step > max(batch_size, train_start_step):
            # 학습수행
            actor_loss, critic_loss = agent.train_model() # (7)
            actor_losses.append(actor_loss)
            critic_losses.append(critic_loss)

            # 타겟 네트워크 소프스 업데이트
            agent.soft_update_target() # (8)

        if done:
            episode += 1
            scores.append(score)
            score = 0

            # 게임 진행 상황 출력 및 텐서보드에 보상과 손실함수 값 기록
            if episode % print_interval == 0:
                mean_score = np.mean(scores)
                mean_actor_loss = np.mean(actor_losses)
                mean_critic_loss = np.mean(critic_losses)
                agent.write_summary(mean_score, mean_actor_loss, mean_critic_loss, step)
                actor_losses, critic_losses, scores = [], [], []

                print(f"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / "+\
                      f"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}")
                
            # 네트워크 모델 저장
            if train_mode and episode % save_interval == 0:
                agent.save_model()

    env.close()
```

프로그램 실행 도식도

​<center><img src="assets\img\posts\2024-05-08-DDPG2_code\5.png" width="700"></center>
[이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EA%B8%B0%EC%B4%88)

### 7. 전체 코드

```python
import numpy as np
import random
import copy
import datetime
import platform
import torch
import torch.nn.functional as F

from torch.unils.tensorboard import SummaryWriter
from collections import deque
from mlagents_envs.environment import UnityEnvironment, ActionTuple
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel

# DDPG 파라미터 
state_size = 9
# 현재 드론의 위치 - 골인 지점의 위치 (x, y, z)
# 현재 드론의 속도 (x, y, z)
# 현재 드론의 각속도 (x, y, z)
#  => 총 9개 
action_size = 3 # 각 축방향 값

load_model = False
train_mode = True

batch_size = 128
mem_maxlen = 50000
discount_factor = 0.9
actor_lr = 1e-4     # actor network 학습률
critic_lr = 5e-4    # critic network 학습률
tau  = 1e-3         # soft target update parameter

# OU noise 파라미터
mu = 0          # 회귀할 평균값
theta = 1e-3    # 회귀 속도
sigma = 2e-3    # 랜덤 프로세스의 변동성

run_step = 50000 if train_mode else 0
test_step = 10000
train_start_step = 5000

print_interval = 10
save_interval = 100

# 유니티 환경 경로
game = "Drone"
os_name = platform.system()
if os_name == 'Windows':
    env_name = f"../envs/{game}_{os_name}/{game}"
elif os_name == 'Darwin':
    env_name = f"../envs/{game}_{os_name}"

# 모델 저장 및 불러오기 경로
date_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
save_path = f"./saved_models/{game}/DDPG/{date_time}"
load_path = f"./saved_models/{game}/DDPG/202405071234"

# 연산장치
device = torch.device("cuda" if torch.cudoa.is_available() else "cpu")

# OU noise 클래스 -> ou noise 정의 및 파라미터 결정
class OU_noise:
    def __init__(self):
        self.reset()

    def reset(self): # noise reset
        self.X = np.ones((1, action_size), dtype=np.float32) * mu # [[0.0, 0.0, 0.0]]

    def sample(self): # noise sampling
        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))
        # len(self.X) = 1 (1 * 3 size np array 이므로)
        # np.random.randn(1) : 평균 0, 표준편자 1 정규분포 내 임의의 값 1개
        self.X += dx
        return self.x
    
# Actor 클래스
class Actor(torch.nn.Module):
    def __init__(self):
        super(Actor, self).__init__()
        self.fc1 = torch.nn.Linear(state_size, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.mu = torch.nn.Linear(128, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        return torch.tanh(self.mu(x)) # -1 ~ 1

# Critic class
class Critic(torch.nn.Module):
    def __init__(self):
        super(Critic, self).__init__()

        self.fc1 = torch.nn.Linear(state_size, 128)
        self.fc2 = torch.nn.Linear(128 + action_size, 128)
        self.q = torch.nn.Linear(128, 1) # q 값 반환

    def forward(self, state, action):
        x = torch.relu(self.fc1(state))
        x = torch.cat((x, action), dim=-1)
        x = torch.relu(self.fc2(x))
        return self.q(x)
    
# DDPGAgent 
class DDPGAgent():
    def __init__(self):
        self.actor = Actor().to(device)
        self.target_actor = copy.deepcopy(self.actor)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic = Critic().to(device)
        self.target_critic = copy.deepcopy(self.cirtic)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        self.OU = OU_noise()
        self.memory = deque(maxlen=mem_maxlen)
        self.writer = SummaryWriter(save_path)

        if load_model == True:
            checkpoint = torch.load(load_path+'/ckpt', map_location=device)
            self.actor.load_state_dict(checkpoint["actor"])
            self.target_actor.load_state_dict(checkpoint["actor"])
            self.actor_mitimizer.load_state_dict(checkpoint["actor_optimizer"])
            self.critic.load_state_dict(checkpoint["critic"])
            self.critic_optimizer.load_state_dic(checkpoint["critic_optimizer"])

    # OU noise 기법에 따라 행동결정
    def get_action(self, state, training=True):
        # 네트워크 모드 설정
        self.actor.train(training) # 훈련 상태로 준비 (ex. Dropout layer 일부 노드 무작위 비활성화, 입력 정규화 (BatchNorm layer))

        action = self.actor(torch.FloatTensor(state).to(device)).cpu().detach().numpy()

        # train_mode -> action + noise 반환
        # test_mode  -> action 반환
        return action + self.OU.sample() if training else action
    
    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)
    def append_sample(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_model(self):
        batch = random.sample(self.memory, batch_size)
        state = np.stack([b[0] for b in batch], axis=0)
        action = np.stack([b[1] for b in batch], axis=0)
        reward = np.stack([b[2] for b in batch], axis=0)
        next_state = np.stack([b[3] for b in batch], axis=0)
        done = np.stack([b[4] for b in batch], axis=0)

        state, action, reward, next_state, done = map(
            lambda x: torch.FloatTensor(x).to(device),
            [state, action, reward, next_state, done]
        )

        # Critic update
        next_actions = self.target_action(next_state) # (1)
        next_q = self.target_critic(next_state, next_actions) # (2)
        target_q = reward + (1 - done) * discount_factor * next_q # (3)
        q = self.critic(state, action) # (4)
        critic_loss = F.mse_loss(target_q, q) # (5)

        # (6)
        self.critic_optimizer.zero_grad() 
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        action_pred = self.actor(state) # (7)
        actor_loss = -self.critic(state, action_pred).mean() # (8)

        # (9)
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        return actor_loss.item(), critic_loss.item()
    
    # soft target update
    def soft_update_target(self):
        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)

        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(tau * local_param.data + ( 1.0 - tau) * target_param.data)

    # 네트워크 모델 저장
    def save_model(self):
        print(f"... Save Model to {save_path}/ckpt ...")
        torch.save({
            "actor" : self.actor.state_dict(),
            "actor_optimizer" : self.actor_optimizer.state_dict(),
            "critic" : self.critic.state_dict(),
            "critic_optimizer" : self.critic_optimizer.state_dict(),
        }, save_path+'/ckpt')

    # 학습 기록
    def write_summary(self, score, actor_loss, critic_loss, step):
        self.writer.add_scalar("run/score", score, step)
        self.writer.add_scalar("model/actor_loss", actor_loss, step)
        self.writer.add_scalar("model/critic_loss", critic_loss, step)


# Main
if __name__ == '__main__':
    # 유니티 환경 경로 설정 (file name)
    engine_configuration_channel = EngineConfigurationChannel()
    env = UnityEnvironment(
        file_name=env_name,
        side_channels=[engine_configuration_channel]
    )

    env.reset()

    # 유니티 브레인 설정
    behavior_name = list(env.behavior_specs.keys())[0]
    spec = env.behavior_specs[behavior_name]
    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)
    dec, term = env.get_steps(behavior_name)

    # DDPGAgent 클래스를 agent 로 정의
    agent = DDPGAgent()
    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0
    for step in range(run_step + test_step):
        if step == run_step:
            if train_mode:
                agent.save_model()
            print("TEST START")
            train_mode = False
            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)

        state = dec.obs[0] # (1)
        action = agent.get_action(state, train_mode) # (2)
        action_tuple = ActionTuple()
        action_tuple.add_continuous(action)
        env.set_actions(behavior_name, action_tuple) # (3)
        env.step() # (4)

        dec, term = env.get_steps(behavior_name) # (5)
        done = len(term.agent_id) > 0
        reward = term.reward if done else dec.reward #(5.1)
        next_state = term.obs[0] if done else dec.obs[0] # (5.2)
        score += reward[0]

        if train_mode: # (6)
            agent.append_sample(state[0], action[0], reward, next_state[0], [done])

        if train_mode and step > max(batch_size, train_start_step):
            # 학습수행
            actor_loss, critic_loss = agent.train_model() # (7)
            actor_losses.append(actor_loss)
            critic_losses.append(critic_loss)

            # 타겟 네트워크 소프스 업데이트
            agent.soft_update_target() # (8)

        if done:
            episode += 1
            scores.append(score)
            score = 0

            # 게임 진행 상황 출력 및 텐서보드에 보상과 손실함수 값 기록
            if episode % print_interval == 0:
                mean_score = np.mean(scores)
                mean_actor_loss = np.mean(actor_losses)
                mean_critic_loss = np.mean(critic_losses)
                agent.write_summary(mean_score, mean_actor_loss, mean_critic_loss, step)
                actor_losses, critic_losses, scores = [], [], []

                print(f"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / "+\
                      f"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}")
                
            # 네트워크 모델 저장
            if train_mode and episode % save_interval == 0:
                agent.save_model()

    env.close()


```