---
layout: post
read_time: true
show_date: true
title:  PPO(Proximal Policy Optimization) 이론 
date:   2024-05-12 09:32:20 +0900
description: PPO(Proximal Policy Optimization) 이론 

img: posts/general/post_general17.jpg
tags: [ppo, proximal policy optimization, surrogate object, clipping]
author: Yong gon Yun
---

<style>
    summary::-webkit-details-marker {
        display: none;
    }
    summary {
        list-style: none;
    }
</style>

<details><summary></summary>
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
에러방지  에러방지 에러방지  에러방지 에러방지  에러방지 에러방지  에러방지
</details>

* 해당 내용은 다음의 강의 및 책 내용을 개인적으로 재학습 하기 위해 작성됨. <br>
  [인프런 - 유니티 머신러닝 에이전트 완전정복 (응용편) ](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)<br>
  [단단한 심층 강화 학습](https://product.kyobobook.co.kr/detail/S000001952238)

### 1. PPO 의 개념 및 특징

PPO 의 주요 아이더는 단조 정책 향상 (monotonic policy imporvement) 를 보장함으로써 성능 붕괴를 피하게 해주는 대리 목적(surrogate objective) 을 도입하는 것. 

구조적 특징
 
 * 정책 + 가치 기반 강화학습 알고리즘
 * 확률적 경사 상승법 (Stochastic Gradient Ascent) 사용 -> "Surrogate" 목적 함수 최대화
 * 다수의 epoch 동안 미니 배치 업데이트 수행
 * On-policy RL 알고리즘
  

기능적 특징

 * Trust Region Policy Optimization (TRPO) 의 장점 + TRPO 대비 비교적 단순한 구현, 다양한 환경에서의 평균적으로 좋은 성능, 더 낮은 샘플 복잡도의 장점을 지님.


### 2. PPO 알고리즘 배경 

#### 2.1 성능 붕괴

REIMFORCE 알고리즘과 같이, 정책 πθ 에 대한 최대 목적 max J(π<sub>θ</sub>) 찾기 위해 gradient 를 사용한다. 다시 말해서, 정책 경사 알고리즘을 사용하는 강화 학습의 경우, ∇<sub>θ</sub>J(θ) 를 이용하여 파라미터 θ 를 조정하는 방식으로 최적화 된다. 

그러나, 우리가 학습시키는 파라미터 θ 로 표현된 공간 (parameter space) 'Θ'  과 실제로 수행하고자 하는 정책들 π 로 표현된 공간 (policy space) 'Π' 은 별개의 공간이다. 그리고 각각의 공간상의 값들이 어떤 연관 관계가 있는지 알 수 없다. 그 배치가 일정한지, 또는 서로 비례된 간격으로 배치되어 있는지 모른다. 이를 수식으로 표현하면 아래와 같다. 
​<center><img src="assets\img\posts\2024-05-12-PPO1\1.png" width="400"></center>

이 문제는 파라미미터 업데이트를 위한 이상적인 학습률 α 를 결정하기 어렵다는 점에서 문제가 될 수 있다. 파라미터 공간에 대응되는 정책 공간 Π 에 속하는 정책들 사이의 간격이 얼마인지를 사전에 알 수 없다. 

만약 α 가 너무 작으면,

* 훈련 횟수 증가에 따른 학습 시간 지연
* 지역 최대값 (local maxima) 빠져 전체 최대값을 찾지 못함 (성능 향상 중단)

의 문제가 발생할 수 있고, α 가 너무 크면, 

* 업데이트 간경이 너무 커서 좋은 정책을 건너뛰게 되는 성능 붕괴 발생

이 나타날 수 있다. 

#### 2.2 상대적 정책 성능 식별자 (relative policy preformance identity)

앞에서 언급된 문제를 해결하기 위해, 학습의 방향을 각각의 정책을 평가하는 것이 아닌 현재의 정책 새로운 정책이 얼마나 향상되었는가를 평가한다. 이를 아래와 같이 표기하며,  상대적 정책 성능 식별자 (relative policy performance identity) 라고 한다. (식 유도는 생략)
​<center><img src="assets\img\posts\2024-05-12-PPO1\2.png" width="300"></center>

 π          : 기존정책 <br>
 π'         : 다음정책<br>
 A<sup>π</sup>(s , a)   : 이전 정책으로부터의 이득 A<sup>π</sup><sub>t</sub>=Q(s<sub>t</sub>,a<sub>t</sub>)−V(s<sub>t</sub>)<br>

 상대적 정책 성능 식별자  J(π') – J(π) 는 정책 향상을 측정하는 지표 역할 을 하며, J(π') 를 최대화 하는 것은, 곧 상대적 정책 성능 식별가 최대가 되게 하는 것이다. 
 ​<center><img src="assets\img\posts\2024-05-12-PPO1\3.png" width="300"></center>

 목적함수를 이렇나 방식으로 구조화하는 것은  모든 정책 반복이 음이 아닌(단조로운) 향상, 즉  J(π') – J(π)≥ 0 을 보장할 수 있어야 한다는 뜻이다. 최악의 경우에도 π' = π 이기 때문에, 이렇게 할 수 있다면, 훈련 과정에서 성능 붕괴는 일어나지 않을 것이다. 

#### 2.3 대리 목적 (surrogate objective)

상대적 정책 성능 식별자  J(π') – J(π) 에 대한 최대의 기대값을 찾을 수 있다면, 성능붕괴를 막을 수 있었다. 그러나 문제는 업데이트된 정책에 대한 목적값 J(π') 을 현재 상태에서 알 수 없다. 이러한 역설을 해결하기 위해 상대적 정책 성능 식별자에 근사된 대리 목적(surrogate objective)  J<sub>π</sub><sup>CPI</sup>(π') 을 구하는 함수 를 사용한다. (유도과정은 생략)
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\4.png" width="400"></center>

* CPI 의미 : 보수적 정책 반복 (Conservative Policy Iteration)

#### 2.4 Probability ratio & TRPO

surrogate objective 함수에서 '새로운 정책 / 기존 정책' 을 probability ratio,
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\5.png" width="330"></center>

라고 하며, TRPO (Trust Region Policy Optimization) 에서는 L<sup>CPI</sup> 을 최대화 하는 것을 목표로 한다. 
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\6.png" width="340"></center>

그러나 probablity ratio 를 그대로 사용할 경우, 해당 값이 과도하게 큰 경우, 학습이 실패하거나 성능이 정하되는 문제가 발생한다. 이를 해결하기 위해 TRPO 의 경우, KL-Divergence 를 사용하여 penalty 를 주는 방식으로 문제를 해결하지만 이 방법은 이해가 어렵고 구현이 난해하다는 단점이 있다. PPO 의 경우 이 문제를  Clipping 기법을 사용하여, 비교적 간단하게 해결 하였다. 

### 3. PPO (Proximal Policy Optimization)

#### 3.1 Clipped Surrogate Objective

계산적으로 효율적인 Penalty 를 적용하고 과도한 Policy 업데이트를 방지 <br>
​<img src="assets\img\posts\2024-05-12-PPO1\7.png" width="420">
​<center><img src="assets\img\posts\2024-05-12-PPO1\8.png" width="500"></center>

 A<sub>t</sub> > 0 일때, <br>
 1. r = 1 : 새 정책과 이전 정책이 같은 확률로 동일 행동을 선택. 이 경우 새 정책에서의 어드밴티지 A<sub>t</sub>의 기대값은 변하지 않음.
 2. r < 1 : 새 정책이 이전 정책보다 해당 행동을 선택할 확률이 낮아졌음을 의미. 즉, 새 정책이 이전 정책보다 더 적은 이득을 기대할 때 발생.
 3. r > 1 : 정책이 이전 정책보다 해당 행동을 선택할 확률이 높아졌음을 의미. 새 정책에서 더 많은 이득을 기대
 4. Clipping 조건
    * <strong>r > 1 + ϵ</strong> :  r의 값이 너무 높게 나타나면 학습 과정에서 성능 붕괴가 발생할 위험이 있다. 예를 들어, 에이전트가 한 행동에 지나치게 의존하게 되면, 다른 잠재적으로 유리한 행동들을 탐색하지 못하고 환경에 대한 이해가 제한될 수 있다. <strong>클리핑은 L<sup>CLIP</sup> = (1 + ϵ) * A<sub>t</sub> 로 제한</strong>함으로써, 너무 큰 정책 변동을 방지하고 보다 안정적인 학습을 촉진. 
    * <strong>r < 1 - ϵ</strong> : <strong>r<sub>t</sub>(θ)의 값을 1 - ϵ 으로 고정.</strong> 이렇게 하면, 에이전트가 불리한 행동을 취하는 것으로 평가되어 이전보다 훨씬 적게 선택될 경우, 이러한 선택의 영향을 완화하여 학습의 안정성을 높이는 것이다.

A<sub>t</sub> < 0 일때도 clipping 조건은 동일 하다. 

#### 3.2 Generalized Advantage Estimate (GAE)
​<center><img src="assets\img\posts\2024-05-12-PPO1\8_1.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)


#### 3.3 분산 강화 학습 (Distributed RL)

다수의 환경을 통해 얻은 데이터 사용
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\9.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

#### 3.4 Network Architecture

A2C 에서는 가치를 평가하는 critic network 와 행동을 결정하는 Actor network 가 각각 존재 하였으나, PPO 에서는 통합 네트워크로 운영됨.

  ​<center><img src="assets\img\posts\2024-05-12-PPO1\10.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

네트워크 업데이트
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\11.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

1. Actor Network Update 
   * Surrogate Functon L<sub>t</sub><sup>CLIP</sup> 을 최대화 하도록 업데이트 
   * Action Entropy S\[π<sub>θ</sub>](s<sub>t</sub>) 행동 선택의 불확실성을 나타내는 척도. 최대화하여 탐색 가능성을 높임

2. Critic Network Update
   L<sub>t</sub><sup>VF</sup> 는 예측 상태 가치 V<sub>θ</sub>(S<sub>t</sub>) 와 예측 상태  가치 V<sub>t</sub><sup>targ</sup> 의 차이 (오차)를 최소화 하도록 업데이트 (제곱의 의미 : 오차의 절대값을 고려 및 큰 오차에 대한 큰 패널티 부과)

3. Actor-Critic Network in PPO
   Actor 와 Critic Network 를 통합하여 구성. c<sub>1</sub> 은 가치 함수 오차의 영향 조절 계수. c<sub>2</sub> 는 탐색을 장려하는 정도를 조절하는 계수

#### 3.5 PPO 알고리즘 의사 코드

  ​<center><img src="assets\img\posts\2024-05-12-PPO1\12.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

1. 분산학습을 통해 N 갯수의 actor 를
2. T timesteps 까지 실행하여 
3. GAE 기법을 사용한 각 actor 의 T-step TD error A<sub>t</sub><sup>(T)</sup> 를 추정한다.
4. 이렇게 모은 advantage set NT 에서 mini batch M 을 추출하여
5. (목표) 대리함수 L 에 대해 확률적 경사 상승 (gardient) 를 적용하여 
6. 파라미터 θ 를 업데이트 
7. K epochs : 모든 데이터를 통해 mini batch 학습한 횟수 

#### 3.6 PPO 알고리즘 logic process

PPO 알고리즘은 Actor-Critic 통합 network 를 통해 행동 결정 및 q 값 산출을 통한 업데이트를 진행된다.

  ​<center><img src="assets\img\posts\2024-05-12-PPO1\13.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)

1. (I) T-steps 만큼 환경을 실행하여, 
2. (I) Trajectory Memory 에 정보를 저장한다. 
3. (L) 메모리에서  M (mini batch) 를 K (epochs) 만큼 추출
4. (L) Network 학습 정보를 통해,
5. (L) 현재 상태 가치와 타겟 상태 가치를 사용하여 Loss VF 값을 계산
6. (L) Action Entropy 계산
7. (L) probability ratio - r 과 GAE 로 계산된 advantage A<sub>t</sub> 를 clipping  하여 clipped objective L<sup>CLIP</sup>(θ)를 도출
8. (L) clipped objective + 상태 가치 loss + action entropy 를 통해 목적식을 산출
9. (L) stochastic gradient decent 를 적용하여 actor-critic 모델 업데이트

#### 3.7 PPO 결과

아래와 같이 타 알고리즘 대비 빠른 학습 결과를 얻을 수 있음을 확인
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\14.png" width="650"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)


Atari 환경의 다양한 게임에서 A2C, ACER 와 성능비교

해당 결과에서 모든 훈련 episode 평균 보상 값이  PPO 가 가장 높은 것을 확인. (빠르게 학습되므로)<br>
다만, 학습 후반에 대한 보상 평균 점수는 ACER 가 더 높은 것으로 확인됨.
  ​<center><img src="assets\img\posts\2024-05-12-PPO1\15.png" width="500"></center>
  [이미지 출처](https://www.inflearn.com/course/%EC%9C%A0%EB%8B%88%ED%8B%B0-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8-%EC%9D%91%EC%9A%A9/dashboard)